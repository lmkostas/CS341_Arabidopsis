{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging phenotypes: learning and labeling function iteration\n",
    "\n",
    "## Introduction\n",
    "In this notebook, we build a phenotype tagger from scratch.\n",
    "\n",
    "Here's the pipeline we'll follow:\n",
    "\n",
    "1. Load extracted candidates for tagging\n",
    "2. Write labeling functions\n",
    "6. Learn the tagging model\n",
    "7. Iterate on labeling functions\n",
    "3. Generate features\n",
    "4. Learn two discriminative models - LogReg and LSTM\n",
    "\n",
    "\n",
    "This notebook requires candidates extracted from `Complex_Pheno_Extraction.ipynb` and gold labels extracted from `Complex_Pheno_BRAT_Import.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# print(os.environ['SNORKELDB'])\n",
    "# Use production DB\n",
    "from set_env import set_env\n",
    "set_env() \n",
    "sys.path.insert(1, '../snorkel')\n",
    "\n",
    "# Must set SNORKELDB before importing SnorkelSession\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import TextDocPreprocessor\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence, candidate_subclass\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "session = SnorkelSession()\n",
    "\n",
    "#np.random.seed(seed=1701)\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (18,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading candidate extractions\n",
    "First, we'll load in the candidates that we created in the last notebook. We can construct an docs object with the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PhenoPair = candidate_subclass('ComplexPhenotypes', ['descriptor', 'entity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should ultimately edit splits to be 0,1,and,2.\n",
    "train = session.query(PhenoPair).filter(PhenoPair.split == 3).all()  \n",
    "dev = session.query(PhenoPair).filter(PhenoPair.split == 4).all()\n",
    "test = session.query(PhenoPair).filter(PhenoPair.split == 5).all()\n",
    "\n",
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Sentences:\", session.query(Sentence).count()\n",
    "\n",
    "print 'Train Document Set:\\t{0} candidates'.format(len(train))\n",
    "print 'Dev Document Set:\\t{0} candidates'.format(len(dev))\n",
    "print 'Test Document Set:\\t{0} candidates'.format(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models.context import TemporaryContext\n",
    "import re\n",
    "import os\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens,\n",
    "    get_between_tokens,\n",
    "    get_right_tokens,\n",
    "    contains_token,\n",
    "    get_text_between,\n",
    "    get_text_splits,\n",
    "    get_tagged_text,\n",
    "    is_inverted,\n",
    "    get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,    \n",
    ")\n",
    "\n",
    "#DICTIONARIES\n",
    "cause_words = set(['affect', 'lead', 'led', 'show', 'display', 'exhibit', 'cause', 'result in'])\n",
    "mutant_words = set(['mutant', 'mutation', 'plant', 'line', 'phenotype', 'seedling', 'variant'])\n",
    "helper_vbs = set(['is', 'was', 'are', 'were', 'become', 'became', 'has', 'had'])\n",
    "tester_words = set(['sequence', 'published', 'diagram', 'hypothesis', 'hypothesize', 'aim', 'goal', 'understand', 'examine', 'we', 'our', 'experiment', 'test', 'study', 'design', 'analyze', 'analysis', 'research'])\n",
    "neg_words = set(['strategy', 'public', 'examine', 'measure', 'subject', 'statistic', 'instance'])\n",
    "adj_words = set(['increase', 'low', 'reduce', 'high', 'less', 'more', 'elevate', 'decrease', 'insensitive', 'absence', 'inhibit', 'double'])   \n",
    "stats_words = set(['statistically', 'quantitative', 'qualitative', 'real-time', 'generate', 'expose', 'stratify'])\n",
    "cc_words = set(['while', 'but', 'however', 'whereas'])\n",
    "comp_words = set(['compare', 'relative', 'than', 'same', 'different', 'relatively', 'contrast', 'similar'])\n",
    "\n",
    "#HELPERS\n",
    "def inverted(c):\n",
    "    return 1 if is_inverted(c) else 0\n",
    "\n",
    "def distance_btwn(c):\n",
    "    span0 = c[0]\n",
    "    span1 = c[1]\n",
    "    indices0 = set(np.arange(span0.get_word_start(), span0.get_word_end() + 1))\n",
    "    indices1 = set(np.arange(span1.get_word_start(), span1.get_word_end() + 1))\n",
    "    if len(indices0.intersection(indices1)) > 0: return 0\n",
    "    if span0.get_word_start() < span1.get_word_start():\n",
    "        return span1.get_word_start() - span0.get_word_end() - 1\n",
    "    else:\n",
    "        left_span = span1\n",
    "        return span0.get_word_start() - span1.get_word_end() - 1\n",
    "    \n",
    "def overlap(c):\n",
    "    span0 = c[0]\n",
    "    span1 = c[1]\n",
    "    indices0 = set(np.arange(span0.get_word_start(), span0.get_word_end() + 1))\n",
    "    indices1 = set(np.arange(span1.get_word_start(), span1.get_word_end() + 1))\n",
    "    if len(indices0.intersection(indices1)) > 0: return 1\n",
    "    return 0\n",
    "\n",
    "def ends_in(ci, val, attrib):\n",
    "    return val == ci.get_attrib_tokens(attrib)[-1]\n",
    "    \n",
    "def starts_with(ci, val, attrib):\n",
    "    return val == ci.get_attrib_tokens(attrib)[0]\n",
    "\n",
    "\n",
    "#DISTANCE RULES\n",
    "def lfdistBtw0(c):\n",
    "    return 1 if distance_btwn(c) == 0 else 0\n",
    "def lfdistBtwMax1(c):\n",
    "    return 1 if distance_btwn(c) < 2 else 0\n",
    "def lfdistBtwMax2(c):\n",
    "    return 1 if distance_btwn(c) < 3 else 0\n",
    "def lfdistBtwnOverlap(c):\n",
    "    return overlap(c)\n",
    "\n",
    "def lfdistBtwMin5(c):\n",
    "    return -1 if distance_btwn(c) > 4 else 0\n",
    "def lfdistBtwMin8(c):\n",
    "    return -1 if distance_btwn(c) > 8 else 0\n",
    "def lfdistBtwMin12(c):\n",
    "    return -1 if distance_btwn(c) > 11 else 0\n",
    "def lfdistBtwMin14(c):\n",
    "    return -1 if distance_btwn(c) > 14 else 0\n",
    "\n",
    "#LENGTH RULES\n",
    "def lfLenCand(c):\n",
    "    return -1 if len(c[0].get_attrib_tokens('words')) == 1 else 0\n",
    "\n",
    "#PAIRWISE RULES\n",
    "def lfend_prep(c):\n",
    "    if not overlap(c):\n",
    "        left = c[0] if c[0].get_word_start() < c[1].get_word_start() else c[1]\n",
    "        right = c[1] if c[0].get_word_start() < c[1].get_word_start() else c[0]\n",
    "        if left.get_attrib_tokens('pos_tags')[-1] == 'IN' and len(set(right.get_attrib_tokens('pos_tags')[:2]).intersection(set(['NN', 'NNS', 'NNP', 'NNPS', 'DT'])))==0:\n",
    "            return -1 \n",
    "    return 0\n",
    "    \n",
    "def lfend_det(c):\n",
    "    if not overlap(c):\n",
    "        left = c[0] if c[0].get_word_start() < c[1].get_word_start() else c[1]\n",
    "        right = c[1] if c[0].get_word_start() < c[1].get_word_start() else c[0]\n",
    "        if left.get_attrib_tokens('pos_tags')[-1] == 'DT' and right.get_attrib_tokens('pos_tags')[0] not in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            return -1\n",
    "    return 0\n",
    "\n",
    "  \n",
    "def lfend_adj(c):\n",
    "    left = c[0] if c[0].get_word_start() < c[1].get_word_start() else c[1]\n",
    "    right = c[1] if c[0].get_word_start() < c[1].get_word_start() else c[0]\n",
    "    if left.get_attrib_tokens('pos_tags')[-1] in ['JJ', 'JJR', 'VBN'] and len(set(get_right_tokens(left, attrib='pos_tags', n_max=2)).intersection(set(['NN', 'NNS', 'NNP', 'NNPS'])))==0:\n",
    "        return -1\n",
    "    return 0\n",
    "        \n",
    "#CONTAINS RULES\n",
    "def lfin_fig(c):\n",
    "    return -1 if contains_token(c[0], 'Fig', attrib='words') \\\n",
    "    or contains_token(c[0], 'FIG', attrib='words') \\\n",
    "    or contains_token(c[0], 'fig', attrib='words') \\\n",
    "    or contains_token(c[0], 'Fig', attrib='words') \\\n",
    "    or contains_token(c[0], 'FIG.', attrib='words') \\\n",
    "    or contains_token(c[0], 'fig.', attrib='words') \\\n",
    "    or contains_token(c[0], 'Fig.', attrib='words') else 0\n",
    "\n",
    "def lfin_num(c):\n",
    "    return 1 if contains_token(c[0], 'CD', attrib='pos_tags') or contains_token(c[1], 'CD', attrib='pos_tags') else 0\n",
    "\n",
    "def lfin_equals(c):\n",
    "    return -1 if contains_token(c[0], '=', attrib='words') or contains_token(c[1], '=', attrib='words') else 0\n",
    "\n",
    "def lfin_comp_adj(c):\n",
    "    return 1 if contains_token(c[0], 'JJR', attrib='pos_tags') or contains_token(c[1], 'JJR', attrib='pos_tags') else 0\n",
    "    \n",
    "def lfin_comp_advb(c):\n",
    "    return 1 if contains_token(c[0], 'RBR', attrib='pos_tags') or contains_token(c[1], 'RBR', attrib='pos_tags') else 0\n",
    "\n",
    "#BETWEEN RULES\n",
    "def lfbtwn_is(c):\n",
    "    return 1 if len(helper_vbs.intersection(set(get_between_tokens(c, attrib='lemmas', n_max=4)))) > 0 else 0\n",
    "    return 1 if 1 == distance_btwn(c) and len(helper_vbs.intersection(set(get_between_tokens(c, attrib='lemmas')))) > 0 else 0\n",
    "                                             \n",
    "def lfbtwm_comma(c):\n",
    "    return -1 if -1 == rule_regex_search_btw_BA(c, '*[,;]*', -1) or -1 == rule_regex_search_btw_AB(c, '*[,;]*', -1) else 0\n",
    "                                             \n",
    "def lfbtwn_parenthesis(c):\n",
    "    return -1 if re.search(r'\\([^\\)]*{{A}}.*\\).*{{B}}', get_tagged_text(c), flags=re.I) \\\n",
    "    or re.search(r'\\([^\\)]*{{B}}.*\\).*{{A}}', get_tagged_text(c), flags=re.I) \\\n",
    "    or re.search(r'\\{{A}}.*\\([^\\)]*{{B}}.*\\)', get_tagged_text(c), flags=re.I) \\\n",
    "    or re.search(r'\\{{B}}.*\\([^\\)]*{{A}}.*\\)', get_tagged_text(c), flags=re.I) else 0\n",
    "    \n",
    "                                             \n",
    "#WORD BASED RULES\n",
    "\n",
    "\n",
    "#WORDS IN CAND RULES\n",
    "def LF_dna(c):\n",
    "    return -1 if contains_token(c, 'DNA', attrib='words') else 0\n",
    "def LF_rna(c):\n",
    "    return -1 if contains_token(c, 'RNA', attrib='words') else 0\n",
    "def LF_snp(c):\n",
    "    return -1 if contains_token(c, 'SNP', attrib='words') else 0\n",
    "\n",
    "def lfwordis_result(c):\n",
    "    return -1 if (len(c[0].get_attrib_tokens('words')) == 1 and contains_token(c[0], 'result', attrib='lemmas')) or (len(c[1].get_attrib_tokens('lemmas')) == 1 and contains_token(c[1], 'result', attrib='lemmas')) else 0\n",
    "\n",
    "def lfwordsin_percent(c):\n",
    "    return 1 if contains_token(c, r'fold') or contains_token(c, r'\\d+(\\.\\d+)?%') or contains_token(c, 'percent') else 0\n",
    "\n",
    "def lfwordsin_phenotype(c):\n",
    "    return 1 if contains_token(c, 'phenotype', attrib='lemmas') else 0\n",
    "\n",
    "def lfwordsin_testerwords(c):\n",
    "    #return -1 if len(tester_words.intersection(set(get_tagged_text(c).split()))) > 0 else 0\n",
    "    return -1 if len(tester_words.intersection(set(c.get_parent()._asdict()['text'].split()))) > 0 else 0\n",
    "\n",
    "#def lfwordsin_statistically(c):\n",
    "#    return 1 if 'statistically' in c.get_parent()._asdict()['text'].split() else 0\n",
    "\n",
    "def lfwordsin_compwords(c):\n",
    "    for word in comp_words:\n",
    "        if contains_token(c, word, attrib='lemmas'): return 1\n",
    "    return 0 \n",
    "\n",
    "def lfwordsin_negwords(c):\n",
    "    for word in neg_words:\n",
    "        if contains_token(c, word, attrib='lemmas'): return -1\n",
    "    return 0 \n",
    "def lfwordsin_causewords(c):\n",
    "    for aw in cause_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return 1\n",
    "    return 0\n",
    "def lfwordsin_adjwords(c):\n",
    "    for aw in adj_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return 1\n",
    "    return 0\n",
    "def lfwordsin_statswords(c):\n",
    "    for aw in stats_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return -1\n",
    "    return 0\n",
    "\n",
    "#WORDS IN CONTEXT\n",
    "def lfwordscontext_mutant(c):\n",
    "    return 1 if len(mutant_words.intersection(set(get_left_tokens(c[0], attrib='lemmas')))) > 0 or len(mutant_words.intersection(set(get_right_tokens(c[0], attrib='lemmas')))) > 0 or len(mutant_words.intersection(set(get_left_tokens(c[1], attrib='lemmas')))) > 0 or len(mutant_words.intersection(set(get_right_tokens(c[1], attrib='lemmas')))) > 0 else 0\n",
    "def lfwordsbtwn_mutant(c):\n",
    "    return 1 if len(mutant_words.intersection(set(get_between_tokens(c, attrib='lemmas', n_max=4)))) > 0 else 0\n",
    "\n",
    "def LF_variant(c):\n",
    "    return 1 if ('variant' in get_right_tokens(c, attrib='lemmas')) or ('variant' in get_left_tokens(c, attrib='lemmas')) else 0\n",
    "def LF_express(c):\n",
    "    return 1 if ('express' in get_right_tokens(c, attrib='lemmas')) or ('express' in get_left_tokens(c, attrib='lemmas')) else 0  \n",
    "#def lfLenCand(c):\n",
    "#    return -1 if len(c[0].get_attrib_tokens('words')) == 1 or len(c[1].get_attrib_tokens('words')) == 1 else 0\n",
    "\n",
    "\n",
    "def lfwordscontext_protein_desc(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[0], attrib='lemmas') or 'protein' in get_right_tokens(c[0], attrib='lemmas') else 0\n",
    "def lfwordscontext_protein_ent(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[1], window=2, attrib='lemmas') or 'protein' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "def lfwordsin_protein(c):\n",
    "    return -1 if contains_token(c[1], 'protein', attrib='lemmas') or contains_token(c[0], 'protein', attrib='lemmas') else 0\n",
    "\n",
    "\n",
    "#def lf1(c):\n",
    "    #return 1 if 'in' in get_between_tokens(c, attrib='words') else 0\n",
    "#def lf21(c):\n",
    "    #return rule_regex_search_btw_BA(c, '.* in .*', 1)\n",
    "\n",
    "def lf2(c):\n",
    "    return 1 if len(cause_words.intersection(set(get_between_tokens(c, attrib='lemmas')))) > 0 else 0\n",
    "\n",
    "\n",
    "def lf6(c):\n",
    "    return 1 if len(helper_vbs.intersection(set(get_between_tokens(c, attrib='lemmas', n_max=3)))) > 0 else 0\n",
    "\n",
    "#def lf7(c):\n",
    "#    return -1 if 'not' in get_between_tokens(c) else 0\n",
    "\n",
    "#def lf8(c):\n",
    "#    return -1 if 'not' in get_left_tokens(c[0]) or 'not' in get_left_tokens(c[1]) else 0\n",
    "\n",
    "#def lf9(c):\n",
    "#    return -1 if 'level' in get_left_tokens(c[0], attrib='lemmas', n_max=2) or 'level' in get_right_tokens(c[0], attrib='lemmas', n_max=2) else 0\n",
    "\n",
    "#def lf10(c):\n",
    "#    return -1 if 'transcript' in get_left_tokens(c[0], attrib='lemmas', n_max=3) or 'transcript' in get_right_tokens(c[0], attrib='lemmas', n_max=2) else 0\n",
    "\n",
    "#def lf12(c):\n",
    "#    return 1 if inverted(c) and lf1(c) else 0\n",
    "\n",
    "\n",
    "\n",
    "#def lf16(c):\n",
    "#    return -1 if 'activity' in get_left_tokens(c[0], attrib='lemmas', n_max=2) or 'level' in get_right_tokens(c[0], attrib='lemmas', n_max=1) else 0\n",
    "\n",
    "\n",
    "def LF_phenotype_dp(c):\n",
    "    return 1 if 'phenotype' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "\n",
    "#def LF_dev_dp(c):\n",
    "#    return -1 if 'development' in get_right_tokens(c[1], window=2, attrib='lemmas')  else 0\n",
    "#def LF_network_dp(c):\n",
    "#    return -1 if 'network' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "\n",
    "def lf_helpers(c):\n",
    "    return 1 if any(word in get_left_tokens(c, window=2, attrib='words') for word in ['had', 'has', 'was', 'have', 'showed', 'were', 'is', 'are', 'results']) else 0\n",
    "\n",
    "#def lf22(c):\n",
    "#    return -1 if 'expression' in get_right_tokens(c[0], attrib='lemmas', window=2) or 'expression' in get_left_tokens(c[0], attrib='lemmas', window=2) else 0\n",
    "    \n",
    "def lf23(c):\n",
    "    return -1 if not inverted(c) and len(helper_vbs.intersection(set(get_right_tokens(c[0], window = 1, attrib='lemmas')))) > 0 and ('VBN' == c[0].get_attrib_tokens('pos_tags')[0] or ('VBN' == c[0].get_attrib_tokens('pos_tags')[1] and 'RB' == c[0].get_attrib_tokens('pos_tags')[0])) else 0\n",
    "\n",
    "def lf32(c):\n",
    "     return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['is', 'are']]) else 0\n",
    "        \n",
    "def lf33(c):\n",
    "    return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['results', 'affected']]) else 0\n",
    "\n",
    "def lf35(c):\n",
    "    return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['showed', 'were', 'was']]) else 0\n",
    "\n",
    "#POS\n",
    "\n",
    "def LF_LRB_Context(c):\n",
    "    return -1 if '-RRB-' in get_right_tokens(c[0], window=1, attrib='pos_tags') or '-RRB-' in get_right_tokens(c[1], window=1, attrib='pos_tags')else 0\n",
    "def LF_LRB_Contains(c):\n",
    "    return -1 if '-LRB-' == c[0].get_attrib_tokens('pos_tags')[0] or '-LRB-' == c[1].get_attrib_tokens('pos_tags')[0] else 0\n",
    "def LF_RRB(c):\n",
    "    return -1 if '-LRB-' in get_right_tokens(c[0], window=1, attrib='pos_tags') or '-LRB-' in get_right_tokens(c[1], window=1, attrib='pos_tags') else 0\n",
    "def LF_JJR(c):\n",
    "    return 1 if contains_token(c, 'JJR', attrib='pos_tags') else 0\n",
    "\n",
    "def LF_ModPhrase(c):\n",
    "    if is_inverted(c):\n",
    "        if c[1].get_attrib_tokens('lemmas')[0] in helper_vbs and c[1].get_attrib_tokens('pos_tags')[1] in ['JJR', 'VBN', 'JJ', 'RBR', 'RB']:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def LF_JJ(c):\n",
    "    return 1 if 'JJ' in get_right_tokens(c, attrib='pos_tags') else 0\n",
    "def LF_IN(c):\n",
    "    return 1 if 'IN' in get_right_tokens(c, window=1, attrib='pos_tags') else 0\n",
    "   \n",
    "def LF_NNP(c):\n",
    "    return -1 if contains_token(c, 'NNP', attrib='pos_tags') else 0\n",
    "\n",
    "\n",
    "def lf13(c):\n",
    "    return 1 if inverted(c) and 'IN' in get_between_tokens(c, attrib='pos_tags', n_max=4) else 0\n",
    "\n",
    "def LF_JJ_dp(c):\n",
    "    return -1 if 'JJ' in get_right_tokens(c[1], window=2, attrib='pos_tags') else 0\n",
    "\n",
    "# def lf20(c):\n",
    "#     lemmas = c[0].get_attrib_tokens('lemmas')\n",
    "#     poses = c[0].get_attrib_tokens('pos_tags')\n",
    "#     result = 0\n",
    "#     for i, w in enumerate(lemmas):\n",
    "#         if w in ['NN', 'NNS', 'NNP', 'NNPS'] and not re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "#             result = 0\n",
    "#         elif re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "#             result = -1\n",
    "#     return result\n",
    "\n",
    "def lf20(c):\n",
    "    lemmas = c[0].get_attrib_tokens('lemmas')\n",
    "    poses = c[0].get_attrib_tokens('pos_tags')\n",
    "    result = 0\n",
    "    for i, w in enumerate(lemmas):\n",
    "        if re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "            return 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def lf24(c):\n",
    "    return -1 if not contains_token(c[0], 'VB', attrib='pos_tags') and not contains_token(c[0], 'VBZ', attrib='pos_tags') and not contains_token(c[0], 'VBD', attrib='pos_tags') else 0\n",
    "\n",
    "def lf25(c):\n",
    "    return -1 if 'IN' == c[0].get_attrib_tokens('pos_tags')[0] or 'TO' == c[0].get_attrib_tokens('pos_tags')[0] else 0\n",
    "\n",
    "def lf26(c):\n",
    "    if len(c[0].get_attrib_tokens('pos_tags')) < 2:\n",
    "        return 0\n",
    "    return -1 if 'JJR' == c[0].get_attrib_tokens('pos_tags')[0] and len(set(['NN', 'NNS', 'NNP', 'NNPS']).intersection(set(c[0].get_attrib_tokens('pos_tags')[1]))) == 0 else 0\n",
    "\n",
    "def lfnonoun(c):\n",
    "    return -1 if len(set(['NN', 'NNS', 'NNP', 'NNPS']).intersection(set(c[0].get_attrib_tokens('pos_tags')+c[1].get_attrib_tokens('pos_tags')))) == 0 else 0\n",
    "    return -1 if (len(c[0]) < 3 and hasNoNoun) else 0\n",
    "                  \n",
    "def lf28(c):\n",
    "    if len(c[0].get_attrib_tokens('pos_tags')) == 0:\n",
    "        return 0\n",
    "    if len(c[0].get_attrib_tokens('lemmas')) < 2:\n",
    "        return 0\n",
    "    lastWordAdj = True if c[0].get_attrib_tokens('pos_tags')[-1] in set(['JJ', 'JJR']) else False\n",
    "    nextLastVrb = True if c[0].get_attrib_tokens('lemmas')[-2] in helper_vbs else False\n",
    "    return -1 if not nextLastVrb and lastWordAdj else 0              \n",
    "    \n",
    "def lf29(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[0].get_attrib_tokens('pos_tags')[-1] == 'VBG' else 0\n",
    "\n",
    "def lf30(c):                #if ends in prep, its bad\n",
    "    return -1 if c[0].get_attrib_tokens('pos_tags')[-1] == 'IN' else 0\n",
    "\n",
    "def lf29b(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] == 'VBG' else 0\n",
    "\n",
    "def lf30b(c):                #if ends in prep, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] == 'IN' else 0\n",
    "\n",
    "def lf30c(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] in ['JJ', 'JJR', 'JJS'] else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFs = [\n",
    "    lfdistBtw0,\n",
    "    lfdistBtwMax1,\n",
    "    lfdistBtwMax2,\n",
    "    lfdistBtwnOverlap,\n",
    "    lfdistBtwMin5,\n",
    "    lfdistBtwMin8,\n",
    "    lfdistBtwMin12,\n",
    "    lfdistBtwMin14,\n",
    "    lfLenCand,\n",
    "    lfend_prep,\n",
    "    lfend_det,\n",
    "    lfend_adj,\n",
    "    lfin_fig,\n",
    "    lfin_num,\n",
    "    lfin_equals,\n",
    "    lfin_comp_adj,\n",
    "    lfbtwn_is,\n",
    "    lfbtwm_comma,\n",
    "    lfbtwn_parenthesis,\n",
    "    LF_dna,\n",
    "    LF_rna,\n",
    "    LF_snp,\n",
    "    lfwordis_result,\n",
    "    lfwordsin_percent,\n",
    "    lfwordsin_phenotype,\n",
    "    lfwordsin_testerwords,\n",
    "    lfwordsin_compwords,\n",
    "    lfwordsin_negwords,\n",
    "    lfwordsin_causewords,\n",
    "    lfwordsin_adjwords,\n",
    "    lfwordsin_statswords,\n",
    "    lfnonoun,\n",
    "#     lfwordscontext_mutant,\n",
    "#     lfwordsbtwn_mutant,\n",
    "#     LF_variant,\n",
    "#     LF_express,\n",
    "#     lfLenCand,\n",
    "#     lfwordscontext_protein_desc,\n",
    "#     lfwordscontext_protein_ent,\n",
    "#     lfwordsin_protein,\n",
    "     lf2,\n",
    "     lf6,\n",
    "     LF_phenotype_dp,\n",
    "     lf_helpers,\n",
    "#     lf23,\n",
    "     lf32,\n",
    "     lf33,\n",
    "     lf35,\n",
    "# #     LF_LRB_Context,\n",
    "# #     LF_LRB_Contains,\n",
    "# #     LF_RRB,\n",
    "#     LF_JJR,\n",
    "#     LF_ModPhrase,\n",
    "#     LF_JJ,\n",
    "#     LF_IN,\n",
    "#     LF_NNP,\n",
    "# #    lf13,\n",
    "# #    LF_JJ_dp,\n",
    "     lf20,\n",
    "     lf24,\n",
    "#     lf25,\n",
    "     lf26\n",
    "# #    lf27,\n",
    "#     lf28,\n",
    "#     lf29,\n",
    "#     lf30\n",
    "# #     lf29b,\n",
    "# #     lf30b,\n",
    "# #     lf30c\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing how to query candidates - no need to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models.context import TemporaryContext\n",
    "import re\n",
    "\n",
    "print docs[15]\n",
    "sent = docs[15].get_parent()\n",
    "print sent\n",
    "text = sent._asdict()['text']\n",
    "splt = text.split()\n",
    "print splt\n",
    "print splt[4:5]\n",
    "print \"\\n\"\n",
    "print \"REGEX VERSION: \"\n",
    "\n",
    "resplit = re.split(' ',text)\n",
    "print resplit\n",
    "print resplit[4:5]\n",
    "print \"\\n\"\n",
    "\n",
    "print docs[15].get_contexts()\n",
    "print (docs[15][0]).get_attrib_tokens('words')\n",
    "print len((docs[15][0]).get_attrib_tokens('words'))\n",
    "# print (docs[15][0]).get_attrib_tokens('dep_parents')\n",
    "\n",
    "#print LF_DP(docs[0])\n",
    "print get_text_splits(docs[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LFs on the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "import multiprocessing\n",
    "from snorkel.annotations import load_gold_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeler = LabelAnnotator(f=LFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time L_train = labeler.apply(split=3, parallelism=multiprocessing.cpu_count())\n",
    "L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>Coverage</b> is the fraction of candidates that the labeling function emits a non-zero label for.\n",
    "* <b>Overlap</b> is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a non-zero label for.\n",
    "* <b>Conflict</b> is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a conflicting non-zero label for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_gold_dev = load_gold_labels(session, annotator_name='gold_complex', split=4)\n",
    "\n",
    "L_dev = labeler.apply_existing(split=4, parallelism=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single LF Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function returns +1 if candidates separated by at most 1 token and -1 otherwise\n",
    "def LF1(c):\n",
    "    return 1 if distance_btwn(c)<2 or overlap(c) else -1\n",
    "\n",
    "single_LF = [LF1]\n",
    "single_labeler = LabelAnnotator(f=LFs)\n",
    "%time single_L_train = single_labeler.apply(split=3, parallelism=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "single_L_dev = single_labeler.apply_existing(split=4, parallelism=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "tlabs = (single_L_dev+L_gold_dev)/2\n",
    "tlabs = csr_matrix.toarray(tlabs).reshape(tlabs.shape[0])\n",
    "flabs = (single_L_dev-L_gold_dev)/2\n",
    "flabs = csr_matrix.toarray(flabs).reshape(flabs.shape[0])\n",
    "\n",
    "tp = tlabs[tlabs==1].shape[0]\n",
    "tn = tlabs[tlabs==-1].shape[0]\n",
    "fp = flabs[flabs==1].shape[0]\n",
    "fn = flabs[flabs==-1].shape[0]\n",
    "\n",
    "prec = float(tp) / (tp+fp)\n",
    "rec = float(tp) / (tp+fn)\n",
    "f1 = (2.0*prec*rec)/(prec+rec)\n",
    "print 'Precision:', prec\n",
    "print 'Recall:', rec\n",
    "print 'F1:', f1\n",
    "print '========================='\n",
    "\n",
    "print 'True Positives:', tp\n",
    "print 'False Positives:', fp\n",
    "print 'True Negatives:', tn\n",
    "print 'False Negatives:', fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Vote Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed = np.clip(np.sum(L_dev, axis=1), -1, 1)\n",
    "eqs = (L_gold_dev == summed)\n",
    "eqs_1 = (eqs == summed)\n",
    "print eqs_1.shape\n",
    "# print (summed[summed == 0].shape)\n",
    "# print (summed[summed == 1].shape)\n",
    "print L_gold_dev[L_gold_dev == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L_gold_dev.shape)\n",
    "print(summed.shape)\n",
    "\n",
    "print eqs[eqs == True].shape\n",
    "from snorkel.learning.utils import MentionScorer\n",
    "dev_candidates = [L_dev.get_candidate(session, i) for i in xrange(L_dev.shape[0])]\n",
    "s = MentionScorer(dev_candidates, L_gold_dev)\n",
    "s.score(summed, b=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning.structure import DependencySelector\n",
    "ds = DependencySelector()\n",
    "deps = ds.select(L_train, threshold=0.1)\n",
    "len(deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps # (lf, lf, relationship_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = GenerativeModel(lf_propensity=True)\n",
    "gen_model.train(\n",
    "    L_train, epochs=2, decay=0.975, step_size=0.0/L_train.shape[0],\n",
    "    init_acc=2.0, reg_param=0.0, burn_in = 10,\n",
    "    verbose=True\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the generative model to the training candidates to get the noise-aware training label set. We'll refer to these as the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.weights.lf_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import save_marginals\n",
    "save_marginals(session, L_train, train_marginals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "train_marginals = load_marginals(session, split=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model to Iterate on Labeling Functions\n",
    "Now that we have learned the generative model, we can stop here and use this to potentially debug and/or improve our labeling function set. First, we apply the LFs to our development set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tp_gen, fp_gen, tn_gen, fn_gen = gen_model.score(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Doing Some Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### At this point, we might want to look at some examples in one of the error buckets. For example, one of the false negatives that we did not correctly label as true mentions. To do this, we can again just use the Viewer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View False Positives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fpsv = SentenceNgramViewer(fp, session, height=400)\n",
    "else:\n",
    "    fpsv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View False Negatives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fnsv = SentenceNgramViewer(fn, session, height=400)\n",
    "else:\n",
    "    fnsv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View True Positives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tpsv = SentenceNgramViewer(tp, session, height=400)\n",
    "else:\n",
    "    tpsv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View True Negatives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tnsv = SentenceNgramViewer(tn, session, height=400)\n",
    "else:\n",
    "    tnsv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically Creating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator\n",
    "import multiprocessing\n",
    "featurizer = FeatureAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time F_train = featurizer.apply(split=3, parallelism=multiprocessing.cpu_count())\n",
    "F_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we apply the feature set we just got from the training set to the dev and test sets by using apply_existing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "F_dev  = featurizer.apply_existing(split=4, parallelism=multiprocessing.cpu_count())\n",
    "F_test = featurizer.apply_existing(split=5, parallelism=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F_train = featurizer.load_matrix(session, split=3)\n",
    "F_dev   = featurizer.load_matrix(session, split=4)\n",
    "F_test  = featurizer.load_matrix(session, split=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Discriminative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the training marginals to train a discriminative model that classifies each Candidate as a true or false mention. We'll use a random hyperparameter search, evaluated on the development set labels, to find the best hyperparameters for our model. To run a hyperparameter search, we need labels for a development set. If they aren't already available, we can manually create labels using the Viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "disc_model = SparseLogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we set up and run the hyperparameter search, training our model with different hyperparamters and picking the best model configuration to keep. We'll set the random seed to maintain reproducibility.\n",
    "Note that we are fitting our model's parameters to the training set generated by our labeling functions, while we are picking hyperparamters with respect to score over the development set labels which we created by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning.utils import MentionScorer\n",
    "from snorkel.learning import ListParameter, RangeParameter\n",
    "\n",
    "# Searching over learning rate\n",
    "rate_param = RangeParameter('lr', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l1_param  = RangeParameter('l1_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l2_param  = RangeParameter('l2_penalty', 1e-6, 1e-2, step=1, log_base=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load in our dev set labels. We will pick the optimal result from the hyperparameter search by testing against these labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold_complex', split=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the hyperparameter search / train the end extraction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_model.train(F_train, train_marginals, n_epochs=50, lr=0.0001, batch_size=100, \\\n",
    "                 l1_penalty=0.0001, l2_penalty=0.01, print_freq=25,\\\n",
    "                 rebalance=0.5, seed=432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, _ = disc_model.get_weights()\n",
    "largest_idxs = reversed(np.argsort(np.abs(w))[-5:])\n",
    "for i in largest_idxs:\n",
    "    print 'Feature: {0: <70}Weight: {1:.6f}'.format(F_train.get_key(session, i).name, w[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this last section of the tutorial, we'll get the score we've been after: the performance of the extraction model on the blind test set (split 2). First, we load the test set labels and gold candidates we made in Part III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold_complex', split=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we score using the discriminative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_lr, fp_lr, tn_lr, fn_lr = disc_model.score(session, F_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_lr_test, fp_lr_test, tn_lr_test, fn_lr_test = disc_model.score(session, F_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Implement a version of an LSTM w/ a decayed learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_dev_labels = (np.ravel(L_gold_dev.todense()) + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.rnn import reRNN\n",
    "\n",
    "np.random.seed(432)\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':   0.01,\n",
    "    'dim':        50,\n",
    "    'n_epochs':   42,\n",
    "    'dropout':    0.5,\n",
    "    'rebalance':  0.5,\n",
    "    'print_freq': 1\n",
    "}\n",
    "\n",
    "lstm = reRNN(seed=1701, n_threads=None)\n",
    "lstm.train(train, train_marginals, dev_candidates=dev, dev_labels=lstm_dev_labels, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.rnn.utils import f1_score\n",
    "dev_data, _ = lstm._preprocess_data(dev, extend=False)\n",
    "labels = np.ravel(dev_labels)\n",
    "dev_p = lstm._marginals_preprocessed(dev_data)\n",
    "f1, p, r = f1_score(dev_p, labels)\n",
    "b=0.5\n",
    "tp_lstm = np.sum((dev_p > b) * (labels > b))\n",
    "fp_lstm = np.sum((dev_p > b) * (labels <= b))\n",
    "tn_lstm = np.sum((dev_p <= b) * (labels <= b))\n",
    "fn_lstm = np.sum((dev_p <= b) * (labels > b))\n",
    "print 'Precision:', p\n",
    "print 'Recall:', r\n",
    "print 'F1:', f1\n",
    "print '============================'\n",
    "print 'True Positives:', tp_lstm\n",
    "print 'False Positives:', fp_lstm\n",
    "print 'True Negatives:', tn_lstm\n",
    "print 'False Negatives:', fn_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewers for Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-dep Gen Model - f1=.605 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fpsv_gen = SentenceNgramViewer(fp_gen, session, height=400)\n",
    "else:\n",
    "    fpsv_gen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpsv_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fnsv_gen = SentenceNgramViewer(fn_gen, session, height=400)\n",
    "else:\n",
    "    fnsv_gen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnsv_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tpsv_gen = SentenceNgramViewer(tp_gen, session, height=400)\n",
    "else:\n",
    "    tpsv_gen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpsv_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tnsv_gen = SentenceNgramViewer(tn_gen, session, height=400)\n",
    "else:\n",
    "    tnsv_gen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnsv_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-dep log-reg dev set -f1=.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fpsv_lr = SentenceNgramViewer(fp_lr, session, height=400)\n",
    "else:\n",
    "    fpsv_lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpsv_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fnsv_lr = SentenceNgramViewer(fn_lr, session, height=400)\n",
    "else:\n",
    "    fnsv_lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnsv_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tpsv_lr = SentenceNgramViewer(tp_lr, session, height=400)\n",
    "else:\n",
    "    tpsv_lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpsv_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tnsv_lr = SentenceNgramViewer(tn_lr, session, height=400)\n",
    "else:\n",
    "    tnsv_lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnsv_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-dep log-reg test - f1=.64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fpsv_lr_test = SentenceNgramViewer(fp_lr_test, session, height=400)\n",
    "else:\n",
    "    fpsv_lr_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpsv_lr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fnsv_lr_test = SentenceNgramViewer(fn_lr_test, session, height=400)\n",
    "else:\n",
    "    fnsv_lr_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnsv_lr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tpsv_lr_test = SentenceNgramViewer(tp_lr_test, session, height=400)\n",
    "else:\n",
    "    tpsv_lr_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpsv_lr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tnsv_lr_test = SentenceNgramViewer(tn_lr_test, session, height=400)\n",
    "else:\n",
    "    tnsv_lr_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnsv_lr_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non-dep lstm dev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fpsv_lstm = SentenceNgramViewer(fp_lstm, session, height=400)\n",
    "else:\n",
    "    fpsv_lstm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpsv_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fnsv_lstm = SentenceNgramViewer(fn_lstm, session, height=400)\n",
    "else:\n",
    "    fnsv_lstm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnsv_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tpsv_lstm = SentenceNgramViewer(tp_lstm, session, height=400)\n",
    "else:\n",
    "    tpsv_lstm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpsv_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tnsv_lstm = SentenceNgramViewer(tn_lstm, session, height=400)\n",
    "else:\n",
    "    tnsv_lstm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnsv_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
