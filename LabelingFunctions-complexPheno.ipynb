{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging genes with ddlite: learning and labeling function iteration\n",
    "\n",
    "## Introduction\n",
    "In this example **ddlite** app, we'll build a gene tagger from scratch. Domain-specific tagging systems take months or years to develop. They use hand-crafted model circuitry and accurate, hand-labeled training data. We'll start to build a pretty good one in a few minutes with none of those things. The generalized extraction and learning utilities provided by ddlite will allow us to turn a sampling of article abstracts and some basic domain knowledge into an automated tagging system. Specifically, we want an accurate tagger for genes in academic articles. We have comprehensive dictionaries of genes, but applying a simple matching rule might yield a lot of false positives. For example, \"p53\" might get tagged as a gene if it refers to a page number. Our goal is to use distant supervision to improve precision.\n",
    "\n",
    "Here's the pipeline we'll follow:\n",
    "\n",
    "1. Obtain and parse input data (relevant article abstracts from PubMed)\n",
    "2. Extract candidates for tagging\n",
    "3. Generate features\n",
    "4. Create a test set\n",
    "5. Write labeling functions\n",
    "6. Learn the tagging model\n",
    "7. Iterate on labeling functions\n",
    "\n",
    "Parts 3 through 7 are covered in this notebook. It requires candidates extracted from `GeneTaggerExample_Extraction.ipynb`, which covers parts 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using postgres 1\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# print(os.environ['SNORKELDB'])\n",
    "# Use production DB\n",
    "from set_env import set_env\n",
    "set_env() \n",
    "sys.path.insert(1, '../snorkel')\n",
    "\n",
    "# Must set SNORKELDB before importing SnorkelSession\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import TextDocPreprocessor\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence, candidate_subclass\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "session = SnorkelSession()\n",
    "\n",
    "#np.random.seed(seed=1701)\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (18,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading candidate extractions\n",
    "First, we'll load in the candidates that we created in the last notebook. We can construct an docs object with the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 400\n",
      "Sentences: 95656\n",
      "Document set:\t49042 candidates\n"
     ]
    }
   ],
   "source": [
    "PhenoPair = candidate_subclass('ComplexPhenotypes', ['descriptor', 'entity'])\n",
    "\n",
    "docs = session.query(PhenoPair).filter(PhenoPair.split == 3).all()  #should edit split to be 1. \n",
    "\n",
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Sentences:\", session.query(Sentence).count()\n",
    "\n",
    "##Once we get all the labels, for loop through all docs and split into train, dev, test. \n",
    "\n",
    "print 'Document set:\\t{0} candidates'.format(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models.context import TemporaryContext\n",
    "import re\n",
    "\n",
    "print docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/CS341_Arabidopsis/venv/local/lib/python2.7/site-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens,\n",
    "    get_between_tokens,\n",
    "    get_right_tokens,\n",
    "    contains_token,\n",
    "    get_text_between,\n",
    "    get_text_splits,\n",
    "    get_tagged_text,\n",
    "    is_inverted,\n",
    "    get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "def LF_mutant(c):\n",
    "    return 1 if ('mutant' in get_right_tokens(c, attrib='lemmas')) or ('mutant' in get_left_tokens(c, attrib='lemmas')) else 0\n",
    "def LF_variant(c):\n",
    "    return 1 if ('variant' in get_right_tokens(c, attrib='lemmas')) or ('variant' in get_left_tokens(c, attrib='lemmas')) else 0\n",
    "def LF_express(c):\n",
    "    return 1 if ('express' in get_right_tokens(c, attrib='lemmas')) or ('express' in get_left_tokens(c, attrib='lemmas')) else 0  \n",
    "def LF_JJ(c):\n",
    "    return 1 if 'JJ' in get_right_tokens(c, attrib='pos_tags') else 0\n",
    "def LF_IN(c):\n",
    "    return 1 if 'IN' in get_right_tokens(c, window=1, attrib='pos_tags') else 0\n",
    "def LF_dna(c):\n",
    "    return -1 if contains_token(c, 'DNA', attrib='words') else 0\n",
    "def LF_rna(c):\n",
    "    return -1 if contains_token(c, 'RNA', attrib='words') else 0\n",
    "def LF_snp(c):\n",
    "    return -1 if contains_token(c, 'SNP', attrib='words') else 0\n",
    "def LF_protein(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c, attrib='lemmas') else 0\n",
    "def LF_LRB(c):\n",
    "    return -1 if '-LRB-' in get_right_tokens(c, window=1, attrib='pos_tags') else 0\n",
    "def LF_RRB(c):\n",
    "    return -1 if '-RRB-' in get_right_tokens(c, window=1, attrib='pos_tags') else 0    \n",
    "def LF_NNP(c):\n",
    "    return -1 if contains_token(c, 'NNP', attrib='pos_tags') else 0\n",
    "def lfdistBtw0(c):\n",
    "    return 1 if len(get_between_tokens(c, attrib=\"words\")) == 0 else 0\n",
    "def lfdistBtw(c):\n",
    "    return 1 if len(get_between_tokens(c, attrib=\"words\")) < 3 else 0\n",
    "def lfdistBtwNeg(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib=\"words\")) > 4 else 0\n",
    "def lfdistBtwNeg2(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib=\"words\")) > 8 else 0\n",
    "\n",
    "action_link_words = set(['affect', 'lead', 'led', 'show', 'display', 'exhibit', 'cause', 'result in'])\n",
    "mutant_words = set(['mutant', 'mutation', 'plant', 'line', 'phenotype', 'seedlings', 'variant'])\n",
    "helper_vbs = set(['is', 'was', 'are', 'were', 'become', 'became'])\n",
    "tester_words = set(['sequence', 'published', 'diagram', 'hypothesis', 'hypothesize', 'aim', 'goal', 'understand', 'examine', 'we', 'our', 'experiment', 'test', 'study', 'design', 'analyze', 'analysis', 'results', 'research'])\n",
    "neg_words = set(['strategy', 'public', 'examine', 'measure', 'subject', 'statistic', 'instance'])\n",
    "\n",
    "def lfnegWords(c):\n",
    "    for word in neg_words:\n",
    "        if contains_token(c, word, attrib='lemmas'): return -1\n",
    "    return 0    \n",
    "\n",
    "def resultsAlone(c):\n",
    "    return -1 if (len(c[0].get_attrib_tokens('words')) == 1 and contains_token(c[0], 'result', attrib='lemmas')) or (len(c[1].get_attrib_tokens('lemmas')) == 1 and contains_token(c[1], 'result', attrib='lemmas')) else 0\n",
    "\n",
    "def lfLenCand(c):\n",
    "    return -1 if len(c[0].get_attrib_tokens('words')) == 1 or len(c[1].get_attrib_tokens('words')) == 1 else 0\n",
    "\n",
    "def lf1(c):\n",
    "    return 1 if 'in' in get_between_tokens(c, attrib='words') else 0\n",
    "\n",
    "def lf2(c):\n",
    "    return 1 if len(action_link_words.intersection(set(get_between_tokens(c, attrib='lemmas')))) > 0 else 0\n",
    "\n",
    "def lf2a(c):\n",
    "    for aw in action_link_words:\n",
    "        if contains_token(c[1], aw, attrib='lemmas'): return 1\n",
    "    return 0\n",
    "\n",
    "def lf3(c):\n",
    "    return 1 if contains_token(c[0], 'JJR', attrib='pos_tags') else 0\n",
    "\n",
    "def lf4(c):\n",
    "    return 1 if contains_token(c, r'fold') or contains_token(c, r'\\d+(\\.\\d+_)?%') or contains_token(c, 'percent') else 0\n",
    "\n",
    "def lf5(c):\n",
    "    return 1 if len(mutant_words.intersection(set(get_left_tokens(c, attrib='lemmas')))) > 0 or len(mutant_words.intersection(set(get_right_tokens(c, attrib='lemmas')))) > 0 else 0\n",
    "\n",
    "def lf6(c):\n",
    "    return 1 if len(helper_vbs.intersection(set(get_between_tokens(c, attrib='lemmas', n_max=3)))) > 0 else 0\n",
    "\n",
    "def lf7(c):\n",
    "    return -1 if 'not' in get_between_tokens(c) else 0\n",
    "\n",
    "def lf8(c):\n",
    "    return -1 if 'not' in get_left_tokens(c[0]) or 'not' in get_left_tokens(c[1]) else 0\n",
    "\n",
    "def lf9(c):\n",
    "    return -1 if 'level' in get_left_tokens(c[0], attrib='lemmas', n_max=2) or 'level' in get_right_tokens(c[0], attrib='lemmas', n_max=2) else 0\n",
    "\n",
    "def lf10(c):\n",
    "    return -1 if 'transcript' in get_left_tokens(c[0], attrib='lemmas', n_max=3) or 'transcript' in get_right_tokens(c[0], attrib='lemmas', n_max=2) else 0\n",
    "\n",
    "def lf11(c):\n",
    "    return -1 if not contains_token(c, 'JJR', attrib='pos_tags') and not contains_token(c, 'JJ', attrib='pos_tags') and not contains_token(c[1], 'VBN', attrib='pos_tags') else 0\n",
    "\n",
    "def inverted(c):\n",
    "    return 1 if is_inverted(c) else 0\n",
    "\n",
    "def lf12(c):\n",
    "    return 1 if inverted(c) and lf1(c) else 0\n",
    "\n",
    "def lf13(c):\n",
    "    return 1 if inverted(c) and 'IN' in get_between_tokens(c, attrib='pos_tags', n_max=4) else 0\n",
    "\n",
    "def lf14(c):\n",
    "    return 1 if contains_token(c, 'phenotype', attrib='lemmas') else 0\n",
    "\n",
    "def lf15(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[0], attrib='lemmas') or 'protein' in get_right_tokens(c[0], attrib='lemmas') else 0\n",
    "\n",
    "def lf16(c):\n",
    "    return -1 if 'activity' in get_left_tokens(c[0], attrib='lemmas', n_max=2) or 'level' in get_right_tokens(c[0], attrib='lemmas', n_max=1) else 0\n",
    "\n",
    "def lf17(c):\n",
    "    return -1 if len(tester_words.intersection(set(get_tagged_text(c).split()))) > 0 else 0\n",
    "\n",
    "# def LF_gene_dp(c):\n",
    "#     return 1 if 'gene' in get_left_tokens(c[0], window=2, attrib='lemmas') else 0\n",
    "# def LF_genotype_dp(c):\n",
    "#     return 1 if 'genotype' in get_left_tokens(c[0], window=2, attrib='lemmas') else 0\n",
    "def LF_phenotype_dp(c):\n",
    "    return 1 if 'phenotype' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "def LF_mutation(c):\n",
    "    return 1 if ('mutation' in get_right_tokens(c[0], window=2, attrib='lemmas') or 'mutant' in get_right_tokens(c[0], window=2, attrib='lemmas') or 'mutations' in get_right_tokens(c[0], window=2, attrib='lemmas')) else 0\n",
    "\n",
    "def LF_pheno(c):\n",
    "    return 1 if contains_token(c, 'phenotype', attrib='words') else 0\n",
    "\n",
    "def LF_dev_dp(c):\n",
    "    return -1 if 'development' in get_right_tokens(c[1], window=2, attrib='lemmas')  else 0\n",
    "def LF_protein_dp(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[1], window=2, attrib='lemmas') or 'protein' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "def LF_network_dp(c):\n",
    "    return -1 if 'network' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "def LF_JJ_dp(c):\n",
    "    return -1 if 'JJ' in get_right_tokens(c[1], window=2, attrib='pos_tags') else 0\n",
    "\n",
    "def lf_helpers(c):\n",
    "    return 1 if any(word in get_left_tokens(c, window=2, attrib='words') for word in ['had', 'has', 'was', 'have', 'showed', 'were', 'is', 'are', 'results']) else 0\n",
    "\n",
    "adj_words = set(['increase', 'lower', 'reduce', 'higher', 'less', 'more', 'elevate', 'decrease', 'insensitive', 'absence', 'inhibit', 'double',])\n",
    "def lf_adjwords(c):\n",
    "    for aw in adj_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return 1\n",
    "    return 0\n",
    "    \n",
    "stats_words = set(['statistically', 'significant', 'quantitative', 'real-time', 'generated', 'exposed', 'stratified'])\n",
    "def lf_statswords(c):\n",
    "    for aw in stats_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return -1\n",
    "    return 0\n",
    "   \n",
    "def lf_proteinIn(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[1], attrib='lemmas') or 'protein' in get_right_tokens(c[1], attrib='lemmas') or contains_token(c[1], 'protein', attrib='lemmas') else 0\n",
    "\n",
    "def lf18(c):\n",
    "    return 1 if 'mutant' in get_tagged_text(c).split() or 'mutation' in get_text_splits(c) else 0\n",
    "\n",
    "# def lf19(c):\n",
    "#     lemmas = c[0].get_attrib_tokens('lemmas')\n",
    "#     poses = c[0].get_attrib_tokens('pos_tags')\n",
    "#     for i, w in enumerate(poses):\n",
    "#         if w in ['NN', 'NNS', 'NNP', 'NNPS'] and not re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "#             return -1\n",
    "#     return 0\n",
    "\n",
    "def lf20(c):\n",
    "    lemmas = c[0].get_attrib_tokens('lemmas')\n",
    "    poses = c[0].get_attrib_tokens('pos_tags')\n",
    "    result = 0\n",
    "    for i, w in enumerate(lemmas):\n",
    "        if w in ['NN', 'NNS', 'NNP', 'NNPS'] and not re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "            result = 0\n",
    "        elif re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "            result = -1\n",
    "    return result\n",
    "\n",
    "def lf21(c):\n",
    "    return rule_regex_search_btw_BA(c, '.* in .*', 1)\n",
    "\n",
    "def lf22(c):\n",
    "    return -1 if 'expression' in get_right_tokens(c[0], attrib='lemmas', window=2) or 'expression' in get_left_tokens(c[0], attrib='lemmas', window=2) else 0\n",
    "    \n",
    "def lf23(c):\n",
    "    return -1 if not inverted(c) and len(helper_vbs.intersection(set(get_right_tokens(c[0], window = 1, attrib='lemmas')))) > 0 and ('VBN' == c[0].get_attrib_tokens('pos_tags')[0] or ('VBN' == c[0].get_attrib_tokens('pos_tags')[1] and 'RB' == c[0].get_attrib_tokens('pos_tags')[0])) else 0\n",
    "\n",
    "def lf24(c):\n",
    "    return -1 if not contains_token(c[0], 'VB', attrib='pos_tags') and not contains_token(c[0], 'VBZ', attrib='pos_tags') and not contains_token(c[0], 'VBD', attrib='pos_tags') else 0\n",
    "\n",
    "def lf25(c):\n",
    "    return -1 if 'IN' == c[0].get_attrib_tokens('pos_tags')[0] or 'TO' == c[0].get_attrib_tokens('pos_tags')[0] else 0\n",
    "\n",
    "def lf26(c):\n",
    "    if len(c[0].get_attrib_tokens('pos_tags')) < 2:\n",
    "        return 0\n",
    "    return -1 if 'JJR' == c[0].get_attrib_tokens('pos_tags')[0] and len(set(['NN', 'NNS', 'NNP', 'NNPS']).intersection(set(c[0].get_attrib_tokens('pos_tags')[1]))) == 0 else 0\n",
    "\n",
    "def lf27(c):\n",
    "    hasNoNoun = len(set(['NN', 'NNS', 'NNP', 'NNPS']).intersection(set(c[0].get_attrib_tokens('pos_tags')[0]))) == 0\n",
    "    return -1 if (len(c[0]) < 3 and hasNoNoun) else 0\n",
    "                  \n",
    "def lf28(c):\n",
    "    if len(c[0].get_attrib_tokens('pos_tags')) == 0:\n",
    "        return 0\n",
    "    if len(c[0].get_attrib_tokens('lemmas')) < 2:\n",
    "        return 0\n",
    "    lastWordAdj = True if c[0].get_attrib_tokens('pos_tags')[-1] in set(['JJ', 'JJR']) else False\n",
    "    nextLastVrb = True if c[0].get_attrib_tokens('lemmas')[-2] in helper_vbs else False\n",
    "    return -1 if not nextLastVrb and lastWordAdj else 0              \n",
    "    \n",
    "def lf29(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[0].get_attrib_tokens('pos_tags')[-1] == 'VBG' else 0\n",
    "\n",
    "def lf30(c):                #if ends in prep, its bad\n",
    "    return -1 if c[0].get_attrib_tokens('pos_tags')[-1] == 'IN' else 0\n",
    "\n",
    "def lf29b(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] == 'VBG' else 0\n",
    "\n",
    "def lf30b(c):                #if ends in prep, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] == 'IN' else 0\n",
    "\n",
    "def lf30c(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] in ['JJ', 'JJR', 'JJS'] else 0\n",
    "\n",
    "\n",
    "\n",
    "def lf31(c):\n",
    "    return 1 if len(get_between_tokens(c, attrib='words')) <= 2 else 0\n",
    "\n",
    "def lf32(c):\n",
    "     return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['is', 'are']]) else 0\n",
    "        \n",
    "def lf33(c):\n",
    "    return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['results', 'affected']]) else 0\n",
    "\n",
    "def lf34(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib='words')) >= 5 else 0\n",
    "\n",
    "def lf35(c):\n",
    "    return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['showed', 'were',]]) else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#HELPERS\n",
    "def inverted(c):\n",
    "    return 1 if is_inverted(c) else 0\n",
    "\n",
    "#DISTANCE RULES\n",
    "def lfdistBtw0(c):\n",
    "    return 1 if len(get_between_tokens(c, attrib=\"words\")) == 0 else 0\n",
    "def lfdistBtwMax2(c):\n",
    "    return 1 if len(get_between_tokens(c, attrib=\"words\")) < 3 else 0\n",
    "def lfdistBtwMin5(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib=\"words\")) > 4 else 0\n",
    "def lfdistBtwMin8(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib=\"words\")) > 8 else 0\n",
    "def lfdistBtwMax1(c):\n",
    "    return 1 if len(get_between_tokens(c, attrib='words')) < 2 else 0\n",
    "def lfdistBtwMin12(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib='words')) > 11 else 0\n",
    "def lfdistBtwMin14(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib='words')) > 14 else 0\n",
    "\n",
    "def lfLenCand(c):\n",
    "    return -1 if len(c[0].get_attrib_tokens('words')) == 1 else 0\n",
    "\n",
    "#WORD BASED RULES\n",
    "cause_words = set(['affect', 'lead', 'led', 'show', 'display', 'exhibit', 'cause', 'result in'])\n",
    "mutant_words = set(['mutant', 'mutation', 'plant', 'line', 'phenotype', 'seedling', 'variant'])\n",
    "helper_vbs = set(['is', 'was', 'are', 'were', 'become', 'became', 'has', 'had'])\n",
    "tester_words = set(['sequence', 'published', 'diagram', 'hypothesis', 'hypothesize', 'aim', 'goal', 'understand', 'examine', 'we', 'our', 'experiment', 'test', 'study', 'design', 'analyze', 'analysis', 'research'])\n",
    "neg_words = set(['strategy', 'public', 'examine', 'measure', 'subject', 'statistic', 'instance'])\n",
    "adj_words = set(['increase', 'low', 'reduce', 'high', 'less', 'more', 'elevate', 'decrease', 'insensitive', 'absence', 'inhibit', 'double'])   \n",
    "stats_words = set(['statistically', 'quantitative', 'qualitative', 'real-time', 'generate', 'expose', 'stratify'])\n",
    "\n",
    "#WORDS IN CAND RULES\n",
    "def LF_dna(c):\n",
    "    return -1 if contains_token(c, 'DNA', attrib='words') else 0\n",
    "def LF_rna(c):\n",
    "    return -1 if contains_token(c, 'RNA', attrib='words') else 0\n",
    "def LF_snp(c):\n",
    "    return -1 if contains_token(c, 'SNP', attrib='words') else 0\n",
    "\n",
    "def lfwordis_result(c):\n",
    "    return -1 if (len(c[0].get_attrib_tokens('words')) == 1 and contains_token(c[0], 'result', attrib='lemmas')) or (len(c[1].get_attrib_tokens('lemmas')) == 1 and contains_token(c[1], 'result', attrib='lemmas')) else 0\n",
    "\n",
    "def lfwordsin_percent(c):\n",
    "    return 1 if contains_token(c, r'fold') or contains_token(c, r'\\d+(\\.\\d+)?%') or contains_token(c, 'percent') else 0\n",
    "\n",
    "def lfwordsin_phenotype(c):\n",
    "    return 1 if contains_token(c, 'phenotype', attrib='lemmas') else 0\n",
    "\n",
    "def lfwordsin_testerwords(c):\n",
    "    #return -1 if len(tester_words.intersection(set(get_tagged_text(c).split()))) > 0 else 0\n",
    "    return -1 len(tester_words.intersection(set(c.get_parent()._asdict()['text'].split()))) > 0 else 0\n",
    "\n",
    "def lfwordsin_statistically(c):\n",
    "    return 1 if 'statistically' in c.get_parent()._asdict()['text'].split() else 0\n",
    "\n",
    "def lfwordsin_negwords(c):\n",
    "    for word in neg_words:\n",
    "        if contains_token(c, word, attrib='lemmas'): return -1\n",
    "    return 0 \n",
    "def lfwordsin_causewords(c):\n",
    "    for aw in cause_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return 1\n",
    "    return 0\n",
    "def lfwordsin_adjwords(c):\n",
    "    for aw in adj_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return 1\n",
    "    return 0\n",
    "def lfwordsin_statswords(c):\n",
    "    for aw in stats_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return -1\n",
    "    return 0\n",
    "\n",
    "#WORDS IN CONTEXT\n",
    "def lfwordscontext_mutant(c):\n",
    "    return 1 if len(mutant_words.intersection(set(get_left_tokens(c[0], attrib='lemmas')))) > 0 or len(mutant_words.intersection(set(get_right_tokens(c[0], attrib='lemmas')))) > 0 or len(mutant_words.intersection(set(get_left_tokens(c[1], attrib='lemmas')))) > 0 or len(mutant_words.intersection(set(get_right_tokens(c[1], attrib='lemmas')))) > 0 else 0\n",
    "def lfwordsbtwn_mutant(c):\n",
    "    return 1 if len(mutant_words.intersection(set(get_between_tokens(c, attrib='lemmas', n_max=4)))) > 0\n",
    "\n",
    "def LF_variant(c):\n",
    "    return 1 if ('variant' in get_right_tokens(c, attrib='lemmas')) or ('variant' in get_left_tokens(c, attrib='lemmas')) else 0\n",
    "def LF_express(c):\n",
    "    return 1 if ('express' in get_right_tokens(c, attrib='lemmas')) or ('express' in get_left_tokens(c, attrib='lemmas')) else 0  \n",
    "def lfLenCand(c):\n",
    "    return -1 if len(c[0].get_attrib_tokens('words')) == 1 or len(c[1].get_attrib_tokens('words')) == 1 else 0\n",
    "\n",
    "\n",
    "def lfwordscontext_protein_desc(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[0], attrib='lemmas') or 'protein' in get_right_tokens(c[0], attrib='lemmas') else 0\n",
    "def lfwordscontext_protein_ent(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[1], window=2, attrib='lemmas') or 'protein' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "def lfwordsin_protein(c):\n",
    "    return -1 if contains_token(c[1], 'protein', attrib='lemmas') or contains_token(c[0], 'protein', attrib='lemmas') else 0\n",
    "\n",
    "\n",
    "#def lf1(c):\n",
    "    return 1 if 'in' in get_between_tokens(c, attrib='words') else 0\n",
    "#def lf21(c):\n",
    "    return rule_regex_search_btw_BA(c, '.* in .*', 1)\n",
    "\n",
    "def lf2(c):\n",
    "    return 1 if len(action_link_words.intersection(set(get_between_tokens(c, attrib='lemmas')))) > 0 else 0\n",
    "\n",
    "\n",
    "def lf6(c):\n",
    "    return 1 if len(helper_vbs.intersection(set(get_between_tokens(c, attrib='lemmas', n_max=3)))) > 0 else 0\n",
    "\n",
    "#def lf7(c):\n",
    "#    return -1 if 'not' in get_between_tokens(c) else 0\n",
    "\n",
    "#def lf8(c):\n",
    "#    return -1 if 'not' in get_left_tokens(c[0]) or 'not' in get_left_tokens(c[1]) else 0\n",
    "\n",
    "#def lf9(c):\n",
    "#    return -1 if 'level' in get_left_tokens(c[0], attrib='lemmas', n_max=2) or 'level' in get_right_tokens(c[0], attrib='lemmas', n_max=2) else 0\n",
    "\n",
    "#def lf10(c):\n",
    "#    return -1 if 'transcript' in get_left_tokens(c[0], attrib='lemmas', n_max=3) or 'transcript' in get_right_tokens(c[0], attrib='lemmas', n_max=2) else 0\n",
    "\n",
    "def lf12(c):\n",
    "    return 1 if inverted(c) and lf1(c) else 0\n",
    "\n",
    "\n",
    "\n",
    "#def lf16(c):\n",
    "#    return -1 if 'activity' in get_left_tokens(c[0], attrib='lemmas', n_max=2) or 'level' in get_right_tokens(c[0], attrib='lemmas', n_max=1) else 0\n",
    "\n",
    "\n",
    "def LF_phenotype_dp(c):\n",
    "    return 1 if 'phenotype' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "\n",
    "#def LF_dev_dp(c):\n",
    "#    return -1 if 'development' in get_right_tokens(c[1], window=2, attrib='lemmas')  else 0\n",
    "def LF_network_dp(c):\n",
    "    return -1 if 'network' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "\n",
    "def lf_helpers(c):\n",
    "    return 1 if any(word in get_left_tokens(c, window=2, attrib='words') for word in ['had', 'has', 'was', 'have', 'showed', 'were', 'is', 'are', 'results']) else 0\n",
    "\n",
    "def lf22(c):\n",
    "    return -1 if 'expression' in get_right_tokens(c[0], attrib='lemmas', window=2) or 'expression' in get_left_tokens(c[0], attrib='lemmas', window=2) else 0\n",
    "    \n",
    "def lf23(c):\n",
    "    return -1 if not inverted(c) and len(helper_vbs.intersection(set(get_right_tokens(c[0], window = 1, attrib='lemmas')))) > 0 and ('VBN' == c[0].get_attrib_tokens('pos_tags')[0] or ('VBN' == c[0].get_attrib_tokens('pos_tags')[1] and 'RB' == c[0].get_attrib_tokens('pos_tags')[0])) else 0\n",
    "\n",
    "def lf32(c):\n",
    "     return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['is', 'are']]) else 0\n",
    "        \n",
    "def lf33(c):\n",
    "    return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['results', 'affected']]) else 0\n",
    "\n",
    "def lf35(c):\n",
    "    return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['showed', 'were', 'was']]) else 0\n",
    "\n",
    "#POS\n",
    "\n",
    "def LF_LRB_Context(c):\n",
    "    return -1 if '-RRB-' in get_right_tokens(c[0], window=1, attrib='pos_tags') or '-RRB-' in get_right_tokens(c[1], window=1, attrib='pos_tags')else 0\n",
    "def LF_LRB_Contains(c):\n",
    "    return -1 if '-LRB-' == c[0].get_attrib_tokens['pos_tags'][0] or '-LRB-' == c[1].get_attrib_tokens['pos_tags'][0] else 0\n",
    "def LF_RRB(c):\n",
    "    return -1 if '-LRB-' in get_right_tokens(c[0], window=1, attrib='pos_tags') or '-LRB-' in get_right_tokens(c[1], window=1, attrib='pos_tags') else 0\n",
    "def LF_JJR(c)\n",
    "    return 1 if contains_token(c, 'JJR', attrib='pos_tags') else 0\n",
    "\n",
    "def LF_ModPhrase(c):\n",
    "    if is_inverted(c):\n",
    "        if c[1].get_attrib_tokens['lemmas'][0] in helper_vbs and c[1].get_attrib_tokens['pos_tags'][1] in ['JJR', 'VBN', 'JJ', 'RBR', 'RB']:\n",
    "            return 1\n",
    "    else 0\n",
    "\n",
    "def LF_JJ(c):\n",
    "    return 1 if 'JJ' in get_right_tokens(c, attrib='pos_tags') else 0\n",
    "def LF_IN(c):\n",
    "    return 1 if 'IN' in get_right_tokens(c, window=1, attrib='pos_tags') else 0\n",
    "   \n",
    "def LF_NNP(c):\n",
    "    return -1 if contains_token(c, 'NNP', attrib='pos_tags') else 0\n",
    "\n",
    "\n",
    "def lf13(c):\n",
    "    return 1 if inverted(c) and 'IN' in get_between_tokens(c, attrib='pos_tags', n_max=4) else 0\n",
    "\n",
    "def LF_JJ_dp(c):\n",
    "    return -1 if 'JJ' in get_right_tokens(c[1], window=2, attrib='pos_tags') else 0\n",
    "\n",
    "def lf20(c):\n",
    "    lemmas = c[0].get_attrib_tokens('lemmas')\n",
    "    poses = c[0].get_attrib_tokens('pos_tags')\n",
    "    result = 0\n",
    "    for i, w in enumerate(lemmas):\n",
    "        if w in ['NN', 'NNS', 'NNP', 'NNPS'] and not re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "            result = 0\n",
    "        elif re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "            result = -1\n",
    "    return result\n",
    "\n",
    "\n",
    "def lf24(c):\n",
    "    return -1 if not contains_token(c[0], 'VB', attrib='pos_tags') and not contains_token(c[0], 'VBZ', attrib='pos_tags') and not contains_token(c[0], 'VBD', attrib='pos_tags') else 0\n",
    "\n",
    "def lf25(c):\n",
    "    return -1 if 'IN' == c[0].get_attrib_tokens('pos_tags')[0] or 'TO' == c[0].get_attrib_tokens('pos_tags')[0] else 0\n",
    "\n",
    "def lf26(c):\n",
    "    if len(c[0].get_attrib_tokens('pos_tags')) < 2:\n",
    "        return 0\n",
    "    return -1 if 'JJR' == c[0].get_attrib_tokens('pos_tags')[0] and len(set(['NN', 'NNS', 'NNP', 'NNPS']).intersection(set(c[0].get_attrib_tokens('pos_tags')[1]))) == 0 else 0\n",
    "\n",
    "def lf27(c):\n",
    "    hasNoNoun = len(set(['NN', 'NNS', 'NNP', 'NNPS']).intersection(set(c[0].get_attrib_tokens('pos_tags')[0]))) == 0\n",
    "    return -1 if (len(c[0]) < 3 and hasNoNoun) else 0\n",
    "                  \n",
    "def lf28(c):\n",
    "    if len(c[0].get_attrib_tokens('pos_tags')) == 0:\n",
    "        return 0\n",
    "    if len(c[0].get_attrib_tokens('lemmas')) < 2:\n",
    "        return 0\n",
    "    lastWordAdj = True if c[0].get_attrib_tokens('pos_tags')[-1] in set(['JJ', 'JJR']) else False\n",
    "    nextLastVrb = True if c[0].get_attrib_tokens('lemmas')[-2] in helper_vbs else False\n",
    "    return -1 if not nextLastVrb and lastWordAdj else 0              \n",
    "    \n",
    "def lf29(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[0].get_attrib_tokens('pos_tags')[-1] == 'VBG' else 0\n",
    "\n",
    "def lf30(c):                #if ends in prep, its bad\n",
    "    return -1 if c[0].get_attrib_tokens('pos_tags')[-1] == 'IN' else 0\n",
    "\n",
    "def lf29b(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] == 'VBG' else 0\n",
    "\n",
    "def lf30b(c):                #if ends in prep, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] == 'IN' else 0\n",
    "\n",
    "def lf30c(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] in ['JJ', 'JJR', 'JJS'] else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test how to query candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComplexPhenotypes(Span(\"C-terminal regions\", sentence=425979, chars=[337,354], words=[57,58]), Span(\"significant differences were found in particular regions of the proteins such as the N-terminal\", sentence=425979, chars=[236,330], words=[41,54]))\n",
      "Sentence(Document PMC3914372,148,Comparison between GTS1 and other WD40 repeat proteins, such as templates 2gnq and 2h9l did not exhibited large differences in the general topology as it was further confirmed by the RMSD value of 2.408 and 3.192, respectively, whereas significant differences were found in particular regions of the proteins such as the N-terminal, and C-terminal regions.)\n",
      "[u'Comparison', u'between', u'GTS1', u'and', u'other', u'WD40', u'repeat', u'proteins,', u'such', u'as', u'templates', u'2gnq', u'and', u'2h9l', u'did', u'not', u'exhibited', u'large', u'differences', u'in', u'the', u'general', u'topology', u'as', u'it', u'was', u'further', u'confirmed', u'by', u'the', u'RMSD', u'value', u'of', u'2.408', u'and', u'3.192,', u'respectively,', u'whereas', u'significant', u'differences', u'were', u'found', u'in', u'particular', u'regions', u'of', u'the', u'proteins', u'such', u'as', u'the', u'N-terminal,', u'and', u'C-terminal', u'regions.']\n",
      "[u'other']\n",
      "\n",
      "\n",
      "REGEX VERSION: \n",
      "[u'Comparison', u'between', u'GTS1', u'and', u'other', u'WD40', u'repeat', u'proteins,', u'such', u'as', u'templates', u'2gnq', u'and', u'2h9l', u'did', u'not', u'exhibited', u'large', u'differences', u'in', u'the', u'general', u'topology', u'as', u'it', u'was', u'further', u'confirmed', u'by', u'the', u'RMSD', u'value', u'of', u'2.408', u'and', u'3.192,', u'respectively,', u'whereas', u'significant', u'differences', u'were', u'found', u'in', u'particular', u'regions', u'of', u'the', u'proteins', u'such', u'as', u'the', u'N-terminal,', u'and', u'C-terminal', u'regions.']\n",
      "[u'other']\n",
      "\n",
      "\n",
      "(Span(\"C-terminal regions\", sentence=425979, chars=[337,354], words=[57,58]), Span(\"significant differences were found in particular regions of the proteins such as the N-terminal\", sentence=425979, chars=[236,330], words=[41,54]))\n",
      "[u'C-terminal', u'regions']\n",
      "2\n",
      "[u'Comparison between GTS1 and other WD40 repeat proteins, such as templates 2gnq and 2h9l did not exhibited large differences in the general topology as it was further confirmed by the RMSD value of 2.408 and 3.192, respectively, whereas ', '{{B}}', u', and ', '{{A}}', u'.']\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models.context import TemporaryContext\n",
    "import re\n",
    "\n",
    "print docs[15]\n",
    "sent = docs[15].get_parent()\n",
    "print sent\n",
    "text = sent._asdict()['text']\n",
    "splt = text.split()\n",
    "print splt\n",
    "print splt[4:5]\n",
    "print \"\\n\"\n",
    "print \"REGEX VERSION: \"\n",
    "\n",
    "resplit = re.split(' ',text)\n",
    "print resplit\n",
    "print resplit[4:5]\n",
    "print \"\\n\"\n",
    "\n",
    "print docs[15].get_contexts()\n",
    "print (docs[15][0]).get_attrib_tokens('words')\n",
    "print len((docs[15][0]).get_attrib_tokens('words'))\n",
    "# print (docs[15][0]).get_attrib_tokens('dep_parents')\n",
    "\n",
    "#print LF_DP(docs[0])\n",
    "print get_text_splits(docs[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LFs on the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFs = [\n",
    "    LF_mutant,\n",
    "    LF_variant,\n",
    "    LF_express,\n",
    "    LF_JJ,\n",
    "    LF_IN,\n",
    "    LF_dna,\n",
    "    LF_rna,\n",
    "    LF_snp,\n",
    "    LF_protein,\n",
    "    LF_LRB,\n",
    "    LF_RRB,\n",
    "    LF_NNP,\n",
    "    lfdistBtw0,\n",
    "    lfdistBtw,\n",
    "    lfdistBtwNeg,\n",
    "    lfdistBtwNeg2,\n",
    "    lf1,\n",
    "    lf2,\n",
    "    lf3,\n",
    "    lf4,\n",
    "    lf5,\n",
    "    lf6,\n",
    "    lf7,\n",
    "    lf8,\n",
    "    lf9,\n",
    "    lf10,\n",
    "    lf11,\n",
    "    lf2a,\n",
    "    lf12,\n",
    "    lf13,\n",
    "    lf14,\n",
    "    lf15,\n",
    "    lf16,\n",
    "    lf17,\n",
    "    lf18,\n",
    "    lf20,\n",
    "    lf21,\n",
    "    lf22,\n",
    "    lf23,\n",
    "    lf24,\n",
    "    lf25,\n",
    "    lf26,\n",
    "    lf27,\n",
    "    lf29,\n",
    "    lf30,\n",
    "#     lf29b,\n",
    "#     lf30b,\n",
    "#     lf30c,\n",
    "    lf31,\n",
    "    lf32,\n",
    "    lf33,\n",
    "    lf34,\n",
    "    lf35,\n",
    "    LF_phenotype_dp,\n",
    "    LF_mutation,\n",
    "    LF_pheno,\n",
    "    LF_dev_dp,\n",
    "    LF_protein_dp,\n",
    "    LF_network_dp,\n",
    "    LF_JJ_dp,\n",
    "    lf_helpers,\n",
    "    lf_adjwords,\n",
    "    lf_statswords,\n",
    "    lf_proteinIn,\n",
    "    lfnegWords,\n",
    "    resultsAlone,\n",
    "    lfLenCand,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "import multiprocessing\n",
    "labeler = LabelAnnotator(f=LFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 2min 56s, sys: 11.6 s, total: 3min 8s\n",
      "Wall time: 9min 38s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<49042x64 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 295318 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time L_train = labeler.apply(split=3, parallelism=multiprocessing.cpu_count())\n",
    "L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_mutant</th>\n",
       "      <td>0</td>\n",
       "      <td>0.027385</td>\n",
       "      <td>0.027385</td>\n",
       "      <td>0.024775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_variant</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_express</th>\n",
       "      <td>2</td>\n",
       "      <td>0.004710</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>0.004506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_JJ</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_IN</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_dna</th>\n",
       "      <td>5</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.003772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_rna</th>\n",
       "      <td>6</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.001774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_snp</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_protein</th>\n",
       "      <td>8</td>\n",
       "      <td>0.019983</td>\n",
       "      <td>0.019983</td>\n",
       "      <td>0.014498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_LRB</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_RRB</th>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_NNP</th>\n",
       "      <td>11</td>\n",
       "      <td>0.107357</td>\n",
       "      <td>0.107173</td>\n",
       "      <td>0.079503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lfdistBtw0</th>\n",
       "      <td>12</td>\n",
       "      <td>0.311345</td>\n",
       "      <td>0.311345</td>\n",
       "      <td>0.287305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lfdistBtw</th>\n",
       "      <td>13</td>\n",
       "      <td>0.412687</td>\n",
       "      <td>0.412687</td>\n",
       "      <td>0.383997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lfdistBtwNeg</th>\n",
       "      <td>14</td>\n",
       "      <td>0.504894</td>\n",
       "      <td>0.504894</td>\n",
       "      <td>0.282431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lfdistBtwNeg2</th>\n",
       "      <td>15</td>\n",
       "      <td>0.370213</td>\n",
       "      <td>0.370213</td>\n",
       "      <td>0.222585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.160617</td>\n",
       "      <td>0.160556</td>\n",
       "      <td>0.160454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf2</th>\n",
       "      <td>17</td>\n",
       "      <td>0.044819</td>\n",
       "      <td>0.044819</td>\n",
       "      <td>0.044778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.035276</td>\n",
       "      <td>0.035215</td>\n",
       "      <td>0.032462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf4</th>\n",
       "      <td>19</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>0.001917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.073774</td>\n",
       "      <td>0.073672</td>\n",
       "      <td>0.067228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf6</th>\n",
       "      <td>21</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.001631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf7</th>\n",
       "      <td>22</td>\n",
       "      <td>0.023327</td>\n",
       "      <td>0.023327</td>\n",
       "      <td>0.018025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf8</th>\n",
       "      <td>23</td>\n",
       "      <td>0.009278</td>\n",
       "      <td>0.009257</td>\n",
       "      <td>0.006729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf9</th>\n",
       "      <td>24</td>\n",
       "      <td>0.032136</td>\n",
       "      <td>0.032095</td>\n",
       "      <td>0.026284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf10</th>\n",
       "      <td>25</td>\n",
       "      <td>0.013009</td>\n",
       "      <td>0.012989</td>\n",
       "      <td>0.010909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf11</th>\n",
       "      <td>26</td>\n",
       "      <td>0.228049</td>\n",
       "      <td>0.227682</td>\n",
       "      <td>0.149708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf2a</th>\n",
       "      <td>27</td>\n",
       "      <td>0.041903</td>\n",
       "      <td>0.041821</td>\n",
       "      <td>0.039130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf12</th>\n",
       "      <td>28</td>\n",
       "      <td>0.079279</td>\n",
       "      <td>0.079279</td>\n",
       "      <td>0.079279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf13</th>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf18</th>\n",
       "      <td>34</td>\n",
       "      <td>0.074977</td>\n",
       "      <td>0.074875</td>\n",
       "      <td>0.072346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf20</th>\n",
       "      <td>35</td>\n",
       "      <td>0.428000</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.305921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf21</th>\n",
       "      <td>36</td>\n",
       "      <td>0.076261</td>\n",
       "      <td>0.076261</td>\n",
       "      <td>0.076261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf22</th>\n",
       "      <td>37</td>\n",
       "      <td>0.009808</td>\n",
       "      <td>0.009808</td>\n",
       "      <td>0.007728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf23</th>\n",
       "      <td>38</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf24</th>\n",
       "      <td>39</td>\n",
       "      <td>0.766833</td>\n",
       "      <td>0.763427</td>\n",
       "      <td>0.545451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf25</th>\n",
       "      <td>40</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf26</th>\n",
       "      <td>41</td>\n",
       "      <td>0.006933</td>\n",
       "      <td>0.006933</td>\n",
       "      <td>0.006933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf27</th>\n",
       "      <td>42</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf29</th>\n",
       "      <td>43</td>\n",
       "      <td>0.014539</td>\n",
       "      <td>0.014518</td>\n",
       "      <td>0.010705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf30</th>\n",
       "      <td>44</td>\n",
       "      <td>0.196954</td>\n",
       "      <td>0.196301</td>\n",
       "      <td>0.148465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf31</th>\n",
       "      <td>45</td>\n",
       "      <td>0.412687</td>\n",
       "      <td>0.412687</td>\n",
       "      <td>0.383997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf32</th>\n",
       "      <td>46</td>\n",
       "      <td>0.034888</td>\n",
       "      <td>0.034888</td>\n",
       "      <td>0.034685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf33</th>\n",
       "      <td>47</td>\n",
       "      <td>0.007341</td>\n",
       "      <td>0.007341</td>\n",
       "      <td>0.007259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf34</th>\n",
       "      <td>48</td>\n",
       "      <td>0.504894</td>\n",
       "      <td>0.504894</td>\n",
       "      <td>0.282431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf35</th>\n",
       "      <td>49</td>\n",
       "      <td>0.009706</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>0.009584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_phenotype_dp</th>\n",
       "      <td>50</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_mutation</th>\n",
       "      <td>51</td>\n",
       "      <td>0.017658</td>\n",
       "      <td>0.017638</td>\n",
       "      <td>0.016863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_pheno</th>\n",
       "      <td>52</td>\n",
       "      <td>0.010848</td>\n",
       "      <td>0.010848</td>\n",
       "      <td>0.010277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_dev_dp</th>\n",
       "      <td>53</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_protein_dp</th>\n",
       "      <td>54</td>\n",
       "      <td>0.009849</td>\n",
       "      <td>0.009849</td>\n",
       "      <td>0.007341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_network_dp</th>\n",
       "      <td>55</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_JJ_dp</th>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_helpers</th>\n",
       "      <td>57</td>\n",
       "      <td>0.040272</td>\n",
       "      <td>0.040272</td>\n",
       "      <td>0.039945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_adjwords</th>\n",
       "      <td>58</td>\n",
       "      <td>0.138147</td>\n",
       "      <td>0.137433</td>\n",
       "      <td>0.129562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_statswords</th>\n",
       "      <td>59</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>0.016537</td>\n",
       "      <td>0.012153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lf_proteinIn</th>\n",
       "      <td>60</td>\n",
       "      <td>0.067534</td>\n",
       "      <td>0.067452</td>\n",
       "      <td>0.047796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lfnegWords</th>\n",
       "      <td>61</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.013539</td>\n",
       "      <td>0.009278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resultsAlone</th>\n",
       "      <td>62</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>0.004445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lfLenCand</th>\n",
       "      <td>63</td>\n",
       "      <td>0.377248</td>\n",
       "      <td>0.376861</td>\n",
       "      <td>0.243159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  j  Coverage  Overlaps  Conflicts\n",
       "LF_mutant         0  0.027385  0.027385   0.024775\n",
       "LF_variant        1  0.000061  0.000061   0.000061\n",
       "LF_express        2  0.004710  0.004690   0.004506\n",
       "LF_JJ             3  0.000000  0.000000   0.000000\n",
       "LF_IN             4  0.000000  0.000000   0.000000\n",
       "LF_dna            5  0.004547  0.004547   0.003772\n",
       "LF_rna            6  0.002202  0.002202   0.001774\n",
       "LF_snp            7  0.000326  0.000326   0.000224\n",
       "LF_protein        8  0.019983  0.019983   0.014498\n",
       "LF_LRB            9  0.000000  0.000000   0.000000\n",
       "LF_RRB           10  0.000000  0.000000   0.000000\n",
       "LF_NNP           11  0.107357  0.107173   0.079503\n",
       "lfdistBtw0       12  0.311345  0.311345   0.287305\n",
       "lfdistBtw        13  0.412687  0.412687   0.383997\n",
       "lfdistBtwNeg     14  0.504894  0.504894   0.282431\n",
       "lfdistBtwNeg2    15  0.370213  0.370213   0.222585\n",
       "lf1              16  0.160617  0.160556   0.160454\n",
       "lf2              17  0.044819  0.044819   0.044778\n",
       "lf3              18  0.035276  0.035215   0.032462\n",
       "lf4              19  0.001958  0.001937   0.001917\n",
       "lf5              20  0.073774  0.073672   0.067228\n",
       "lf6              21  0.001631  0.001631   0.001631\n",
       "lf7              22  0.023327  0.023327   0.018025\n",
       "lf8              23  0.009278  0.009257   0.006729\n",
       "lf9              24  0.032136  0.032095   0.026284\n",
       "lf10             25  0.013009  0.012989   0.010909\n",
       "lf11             26  0.228049  0.227682   0.149708\n",
       "lf2a             27  0.041903  0.041821   0.039130\n",
       "lf12             28  0.079279  0.079279   0.079279\n",
       "lf13             29  0.000000  0.000000   0.000000\n",
       "...              ..       ...       ...        ...\n",
       "lf18             34  0.074977  0.074875   0.072346\n",
       "lf20             35  0.428000  0.426471   0.305921\n",
       "lf21             36  0.076261  0.076261   0.076261\n",
       "lf22             37  0.009808  0.009808   0.007728\n",
       "lf23             38  0.000000  0.000000   0.000000\n",
       "lf24             39  0.766833  0.763427   0.545451\n",
       "lf25             40  0.000775  0.000775   0.000653\n",
       "lf26             41  0.006933  0.006933   0.006933\n",
       "lf27             42  0.000326  0.000326   0.000245\n",
       "lf29             43  0.014539  0.014518   0.010705\n",
       "lf30             44  0.196954  0.196301   0.148465\n",
       "lf31             45  0.412687  0.412687   0.383997\n",
       "lf32             46  0.034888  0.034888   0.034685\n",
       "lf33             47  0.007341  0.007341   0.007259\n",
       "lf34             48  0.504894  0.504894   0.282431\n",
       "lf35             49  0.009706  0.009686   0.009584\n",
       "LF_phenotype_dp  50  0.000693  0.000693   0.000612\n",
       "LF_mutation      51  0.017658  0.017638   0.016863\n",
       "LF_pheno         52  0.010848  0.010848   0.010277\n",
       "LF_dev_dp        53  0.001142  0.001142   0.000999\n",
       "LF_protein_dp    54  0.009849  0.009849   0.007341\n",
       "LF_network_dp    55  0.000714  0.000714   0.000591\n",
       "LF_JJ_dp         56  0.000000  0.000000   0.000000\n",
       "lf_helpers       57  0.040272  0.040272   0.039945\n",
       "lf_adjwords      58  0.138147  0.137433   0.129562\n",
       "lf_statswords    59  0.016680  0.016537   0.012153\n",
       "lf_proteinIn     60  0.067534  0.067452   0.047796\n",
       "lfnegWords       61  0.013601  0.013539   0.009278\n",
       "resultsAlone     62  0.006403  0.006403   0.004445\n",
       "lfLenCand        63  0.377248  0.376861   0.243159\n",
       "\n",
       "[64 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>Coverage</b> is the fraction of candidates that the labeling function emits a non-zero label for.\n",
    "* <b>Overlap</b> is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a non-zero label for.\n",
    "* <b>Conflict</b> is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a conflicting non-zero label for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.learning.structure import DependencySelector\n",
    "ds = DependencySelector()\n",
    "deps = ds.select(L_train, threshold=0.1)\n",
    "len(deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 20, 0),\n",
       " (0, 34, 0),\n",
       " (8, 31, 0),\n",
       " (8, 60, 0),\n",
       " (12, 13, 0),\n",
       " (12, 45, 0),\n",
       " (13, 45, 0),\n",
       " (14, 15, 0),\n",
       " (14, 48, 0),\n",
       " (15, 22, 0),\n",
       " (15, 48, 0),\n",
       " (16, 17, 0),\n",
       " (16, 28, 0),\n",
       " (16, 36, 0),\n",
       " (18, 41, 0),\n",
       " (18, 58, 0),\n",
       " (20, 34, 0),\n",
       " (22, 23, 0),\n",
       " (24, 32, 0),\n",
       " (26, 35, 0),\n",
       " (26, 44, 0),\n",
       " (26, 63, 0),\n",
       " (28, 36, 0),\n",
       " (30, 52, 0),\n",
       " (31, 54, 0),\n",
       " (31, 60, 0),\n",
       " (34, 51, 0),\n",
       " (35, 44, 0),\n",
       " (39, 63, 0),\n",
       " (46, 57, 0),\n",
       " (47, 57, 0),\n",
       " (51, 58, 0),\n",
       " (54, 60, 0)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deps # (lf, lf, relationship_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FACTOR 0: STARTED BURN-IN...\n",
      "FACTOR 0: DONE WITH BURN-IN\n",
      "FACTOR 0: STARTED LEARNING\n",
      "FACTOR 0: EPOCH #0\n",
      "Current stepsize = 2.03906855348e-06\n",
      "Learning epoch took 0.000 sec.\n",
      "Weights:\n",
      "    weightId: 0\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 1\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 2\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 3\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 4\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 5\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 6\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 7\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 8\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 9\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 10\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 11\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 12\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 13\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 14\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 15\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 16\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 17\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 18\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 19\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 20\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 21\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 22\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 23\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 24\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 25\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 26\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 27\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 28\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 29\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 30\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 31\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 32\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 33\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 34\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 35\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 36\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 37\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 38\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 39\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 40\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 41\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 42\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 43\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 44\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 45\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 46\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 47\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 48\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 49\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 50\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 51\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 52\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 53\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 54\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 55\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 56\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 57\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 58\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 59\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 60\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 61\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 62\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 63\n",
      "        isFixed: False\n",
      "        weight:  2.0\n",
      "\n",
      "    weightId: 64\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 65\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 66\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 67\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 68\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 69\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 70\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 71\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 72\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 73\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 74\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 75\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 76\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 77\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 78\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 79\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 80\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 81\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 82\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 83\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 84\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 85\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 86\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 87\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 88\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 89\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 90\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 91\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 92\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 93\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 94\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 95\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 96\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 97\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 98\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 99\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 100\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 101\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 102\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 103\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 104\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 105\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 106\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 107\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 108\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 109\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 110\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 111\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 112\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 113\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 114\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 115\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 116\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 117\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 118\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 119\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 120\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 121\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 122\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 123\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 124\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 125\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 126\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 127\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 128\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 129\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 130\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 131\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 132\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 133\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 134\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 135\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 136\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 137\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 138\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 139\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 140\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 141\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 142\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 143\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 144\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 145\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 146\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 147\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 148\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 149\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 150\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 151\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 152\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 153\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 154\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 155\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 156\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 157\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 158\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 159\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n",
      "    weightId: 160\n",
      "        isFixed: False\n",
      "        weight:  1.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FACTOR 0: EPOCH #1\n",
      "Current stepsize = 1.93711512581e-06\n",
      "Learning epoch took 11.016 sec.\n",
      "Weights:\n",
      "    weightId: 0\n",
      "        isFixed: False\n",
      "        weight:  1.80442477875\n",
      "\n",
      "    weightId: 1\n",
      "        isFixed: False\n",
      "        weight:  1.81771542759\n",
      "\n",
      "    weightId: 2\n",
      "        isFixed: False\n",
      "        weight:  1.81755434117\n",
      "\n",
      "    weightId: 3\n",
      "        isFixed: False\n",
      "        weight:  1.81807634272\n",
      "\n",
      "    weightId: 4\n",
      "        isFixed: False\n",
      "        weight:  1.81795399861\n",
      "\n",
      "    weightId: 5\n",
      "        isFixed: False\n",
      "        weight:  1.8187002977\n",
      "\n",
      "    weightId: 6\n",
      "        isFixed: False\n",
      "        weight:  1.81828228864\n",
      "\n",
      "    weightId: 7\n",
      "        isFixed: False\n",
      "        weight:  1.81807226458\n",
      "\n",
      "    weightId: 8\n",
      "        isFixed: False\n",
      "        weight:  1.80622731536\n",
      "\n",
      "    weightId: 9\n",
      "        isFixed: False\n",
      "        weight:  1.81795195954\n",
      "\n",
      "    weightId: 10\n",
      "        isFixed: False\n",
      "        weight:  1.81811304595\n",
      "\n",
      "    weightId: 11\n",
      "        isFixed: False\n",
      "        weight:  1.83014966763\n",
      "\n",
      "    weightId: 12\n",
      "        isFixed: False\n",
      "        weight:  1.8321744627\n",
      "\n",
      "    weightId: 13\n",
      "        isFixed: False\n",
      "        weight:  1.8317013988\n",
      "\n",
      "    weightId: 14\n",
      "        isFixed: False\n",
      "        weight:  1.8976591493\n",
      "\n",
      "    weightId: 15\n",
      "        isFixed: False\n",
      "        weight:  1.87116553158\n",
      "\n",
      "    weightId: 16\n",
      "        isFixed: False\n",
      "        weight:  1.77517026222\n",
      "\n",
      "    weightId: 17\n",
      "        isFixed: False\n",
      "        weight:  1.79980832755\n",
      "\n",
      "    weightId: 18\n",
      "        isFixed: False\n",
      "        weight:  1.8031768688\n",
      "\n",
      "    weightId: 19\n",
      "        isFixed: False\n",
      "        weight:  1.81762978671\n",
      "\n",
      "    weightId: 20\n",
      "        isFixed: False\n",
      "        weight:  1.80462052934\n",
      "\n",
      "    weightId: 21\n",
      "        isFixed: False\n",
      "        weight:  1.81826801516\n",
      "\n",
      "    weightId: 22\n",
      "        isFixed: False\n",
      "        weight:  1.80745891276\n",
      "\n",
      "    weightId: 23\n",
      "        isFixed: False\n",
      "        weight:  1.80870478365\n",
      "\n",
      "    weightId: 24\n",
      "        isFixed: False\n",
      "        weight:  1.81175726927\n",
      "\n",
      "    weightId: 25\n",
      "        isFixed: False\n",
      "        weight:  1.81903470494\n",
      "\n",
      "    weightId: 26\n",
      "        isFixed: False\n",
      "        weight:  1.83479262672\n",
      "\n",
      "    weightId: 27\n",
      "        isFixed: False\n",
      "        weight:  1.81953223767\n",
      "\n",
      "    weightId: 28\n",
      "        isFixed: False\n",
      "        weight:  1.79205171077\n",
      "\n",
      "    weightId: 29\n",
      "        isFixed: False\n",
      "        weight:  1.81804983483\n",
      "\n",
      "    weightId: 30\n",
      "        isFixed: False\n",
      "        weight:  1.80805635985\n",
      "\n",
      "    weightId: 31\n",
      "        isFixed: False\n",
      "        weight:  1.80828677459\n",
      "\n",
      "    weightId: 32\n",
      "        isFixed: False\n",
      "        weight:  1.81117001753\n",
      "\n",
      "    weightId: 33\n",
      "        isFixed: False\n",
      "        weight:  1.84115859875\n",
      "\n",
      "    weightId: 34\n",
      "        isFixed: False\n",
      "        weight:  1.79810162717\n",
      "\n",
      "    weightId: 35\n",
      "        isFixed: False\n",
      "        weight:  1.84980628848\n",
      "\n",
      "    weightId: 36\n",
      "        isFixed: False\n",
      "        weight:  1.7924391338\n",
      "\n",
      "    weightId: 37\n",
      "        isFixed: False\n",
      "        weight:  1.81911422861\n",
      "\n",
      "    weightId: 38\n",
      "        isFixed: False\n",
      "        weight:  1.8177337792\n",
      "\n",
      "    weightId: 39\n",
      "        isFixed: False\n",
      "        weight:  1.88369152971\n",
      "\n",
      "    weightId: 40\n",
      "        isFixed: False\n",
      "        weight:  1.81838832021\n",
      "\n",
      "    weightId: 41\n",
      "        isFixed: False\n",
      "        weight:  1.80751396761\n",
      "\n",
      "    weightId: 42\n",
      "        isFixed: False\n",
      "        weight:  1.8183047184\n",
      "\n",
      "    weightId: 43\n",
      "        isFixed: False\n",
      "        weight:  1.81970759756\n",
      "\n",
      "    weightId: 44\n",
      "        isFixed: False\n",
      "        weight:  1.82174462705\n",
      "\n",
      "    weightId: 45\n",
      "        isFixed: False\n",
      "        weight:  1.83181762571\n",
      "\n",
      "    weightId: 46\n",
      "        isFixed: False\n",
      "        weight:  1.80641490966\n",
      "\n",
      "    weightId: 47\n",
      "        isFixed: False\n",
      "        weight:  1.80712450552\n",
      "\n",
      "    weightId: 48\n",
      "        isFixed: False\n",
      "        weight:  1.89757350842\n",
      "\n",
      "    weightId: 49\n",
      "        isFixed: False\n",
      "        weight:  1.81775824803\n",
      "\n",
      "    weightId: 50\n",
      "        isFixed: False\n",
      "        weight:  1.81794584233\n",
      "\n",
      "    weightId: 51\n",
      "        isFixed: False\n",
      "        weight:  1.80253864034\n",
      "\n",
      "    weightId: 52\n",
      "        isFixed: False\n",
      "        weight:  1.80810325842\n",
      "\n",
      "    weightId: 53\n",
      "        isFixed: False\n",
      "        weight:  1.81830879653\n",
      "\n",
      "    weightId: 54\n",
      "        isFixed: False\n",
      "        weight:  1.80442681782\n",
      "\n",
      "    weightId: 55\n",
      "        isFixed: False\n",
      "        weight:  1.81814974919\n",
      "\n",
      "    weightId: 56\n",
      "        isFixed: False\n",
      "        weight:  1.81833530443\n",
      "\n",
      "    weightId: 57\n",
      "        isFixed: False\n",
      "        weight:  1.80262835936\n",
      "\n",
      "    weightId: 58\n",
      "        isFixed: False\n",
      "        weight:  1.80165164552\n",
      "\n",
      "    weightId: 59\n",
      "        isFixed: False\n",
      "        weight:  1.82009502059\n",
      "\n",
      "    weightId: 60\n",
      "        isFixed: False\n",
      "        weight:  1.81198768402\n",
      "\n",
      "    weightId: 61\n",
      "        isFixed: False\n",
      "        weight:  1.81974022266\n",
      "\n",
      "    weightId: 62\n",
      "        isFixed: False\n",
      "        weight:  1.8190245096\n",
      "\n",
      "    weightId: 63\n",
      "        isFixed: False\n",
      "        weight:  1.85575425145\n",
      "\n",
      "    weightId: 64\n",
      "        isFixed: False\n",
      "        weight:  0.903627502958\n",
      "\n",
      "    weightId: 65\n",
      "        isFixed: False\n",
      "        weight:  0.905635985483\n",
      "\n",
      "    weightId: 66\n",
      "        isFixed: False\n",
      "        weight:  0.906143713553\n",
      "\n",
      "    weightId: 67\n",
      "        isFixed: False\n",
      "        weight:  0.905501406959\n",
      "\n",
      "    weightId: 68\n",
      "        isFixed: False\n",
      "        weight:  0.905548305536\n",
      "\n",
      "    weightId: 69\n",
      "        isFixed: False\n",
      "        weight:  0.906153908896\n",
      "\n",
      "    weightId: 70\n",
      "        isFixed: False\n",
      "        weight:  0.905813384448\n",
      "\n",
      "    weightId: 71\n",
      "        isFixed: False\n",
      "        weight:  0.905625790141\n",
      "\n",
      "    weightId: 72\n",
      "        isFixed: False\n",
      "        weight:  0.9028077974\n",
      "\n",
      "    weightId: 73\n",
      "        isFixed: False\n",
      "        weight:  0.905633946415\n",
      "\n",
      "    weightId: 74\n",
      "        isFixed: False\n",
      "        weight:  0.905531992987\n",
      "\n",
      "    weightId: 75\n",
      "        isFixed: False\n",
      "        weight:  0.915955711432\n",
      "\n",
      "    weightId: 76\n",
      "        isFixed: False\n",
      "        weight:  0.932023571634\n",
      "\n",
      "    weightId: 77\n",
      "        isFixed: False\n",
      "        weight:  0.942141429796\n",
      "\n",
      "    weightId: 78\n",
      "        isFixed: False\n",
      "        weight:  0.951309082012\n",
      "\n",
      "    weightId: 79\n",
      "        isFixed: False\n",
      "        weight:  0.937394478203\n",
      "\n",
      "    weightId: 80\n",
      "        isFixed: False\n",
      "        weight:  0.91643897068\n",
      "\n",
      "    weightId: 81\n",
      "        isFixed: False\n",
      "        weight:  0.906643285349\n",
      "\n",
      "    weightId: 82\n",
      "        isFixed: False\n",
      "        weight:  0.904416622488\n",
      "\n",
      "    weightId: 83\n",
      "        isFixed: False\n",
      "        weight:  0.905640063621\n",
      "\n",
      "    weightId: 84\n",
      "        isFixed: False\n",
      "        weight:  0.908280657397\n",
      "\n",
      "    weightId: 85\n",
      "        isFixed: False\n",
      "        weight:  0.905746095185\n",
      "\n",
      "    weightId: 86\n",
      "        isFixed: False\n",
      "        weight:  0.903276783167\n",
      "\n",
      "    weightId: 87\n",
      "        isFixed: False\n",
      "        weight:  0.903248236207\n",
      "\n",
      "    weightId: 88\n",
      "        isFixed: False\n",
      "        weight:  0.905503446027\n",
      "\n",
      "    weightId: 89\n",
      "        isFixed: False\n",
      "        weight:  0.906679988583\n",
      "\n",
      "    weightId: 90\n",
      "        isFixed: False\n",
      "        weight:  0.923243342442\n",
      "\n",
      "    weightId: 91\n",
      "        isFixed: False\n",
      "        weight:  0.909809958812\n",
      "\n",
      "    weightId: 92\n",
      "        isFixed: False\n",
      "        weight:  0.90879858081\n",
      "\n",
      "    weightId: 93\n",
      "        isFixed: False\n",
      "        weight:  0.905540149261\n",
      "\n",
      "    weightId: 94\n",
      "        isFixed: False\n",
      "        weight:  0.904269809553\n",
      "\n",
      "    weightId: 95\n",
      "        isFixed: False\n",
      "        weight:  0.904644998166\n",
      "\n",
      "    weightId: 96\n",
      "        isFixed: False\n",
      "        weight:  0.904848905022\n",
      "\n",
      "    weightId: 97\n",
      "        isFixed: False\n",
      "        weight:  0.923557359\n",
      "\n",
      "    weightId: 98\n",
      "        isFixed: False\n",
      "        weight:  0.907815749767\n",
      "\n",
      "    weightId: 99\n",
      "        isFixed: False\n",
      "        weight:  0.943709473513\n",
      "\n",
      "    weightId: 100\n",
      "        isFixed: False\n",
      "        weight:  0.908509033075\n",
      "\n",
      "    weightId: 101\n",
      "        isFixed: False\n",
      "        weight:  0.906418987808\n",
      "\n",
      "    weightId: 102\n",
      "        isFixed: False\n",
      "        weight:  0.905423922354\n",
      "\n",
      "    weightId: 103\n",
      "        isFixed: False\n",
      "        weight:  0.978901757677\n",
      "\n",
      "    weightId: 104\n",
      "        isFixed: False\n",
      "        weight:  0.905640063621\n",
      "\n",
      "    weightId: 105\n",
      "        isFixed: False\n",
      "        weight:  0.90296276661\n",
      "\n",
      "    weightId: 106\n",
      "        isFixed: False\n",
      "        weight:  0.905721626363\n",
      "\n",
      "    weightId: 107\n",
      "        isFixed: False\n",
      "        weight:  0.906983809797\n",
      "\n",
      "    weightId: 108\n",
      "        isFixed: False\n",
      "        weight:  0.920645569105\n",
      "\n",
      "    weightId: 109\n",
      "        isFixed: False\n",
      "        weight:  0.94220667999\n",
      "\n",
      "    weightId: 110\n",
      "        isFixed: False\n",
      "        weight:  0.905774642145\n",
      "\n",
      "    weightId: 111\n",
      "        isFixed: False\n",
      "        weight:  0.902879164799\n",
      "\n",
      "    weightId: 112\n",
      "        isFixed: False\n",
      "        weight:  0.951280535052\n",
      "\n",
      "    weightId: 113\n",
      "        isFixed: False\n",
      "        weight:  0.906476081727\n",
      "\n",
      "    weightId: 114\n",
      "        isFixed: False\n",
      "        weight:  0.905521797644\n",
      "\n",
      "    weightId: 115\n",
      "        isFixed: False\n",
      "        weight:  0.902714000246\n",
      "\n",
      "    weightId: 116\n",
      "        isFixed: False\n",
      "        weight:  0.903617307616\n",
      "\n",
      "    weightId: 117\n",
      "        isFixed: False\n",
      "        weight:  0.905827657927\n",
      "\n",
      "    weightId: 118\n",
      "        isFixed: False\n",
      "        weight:  0.901796419397\n",
      "\n",
      "    weightId: 119\n",
      "        isFixed: False\n",
      "        weight:  0.905746095185\n",
      "\n",
      "    weightId: 120\n",
      "        isFixed: False\n",
      "        weight:  0.905633946415\n",
      "\n",
      "    weightId: 121\n",
      "        isFixed: False\n",
      "        weight:  0.905056890014\n",
      "\n",
      "    weightId: 122\n",
      "        isFixed: False\n",
      "        weight:  0.91477509074\n",
      "\n",
      "    weightId: 123\n",
      "        isFixed: False\n",
      "        weight:  0.907371232822\n",
      "\n",
      "    weightId: 124\n",
      "        isFixed: False\n",
      "        weight:  0.907097997636\n",
      "\n",
      "    weightId: 125\n",
      "        isFixed: False\n",
      "        weight:  0.906885934507\n",
      "\n",
      "    weightId: 126\n",
      "        isFixed: False\n",
      "        weight:  0.906115166594\n",
      "\n",
      "    weightId: 127\n",
      "        isFixed: False\n",
      "        weight:  0.938691325803\n",
      "\n",
      "    weightId: 128\n",
      "        isFixed: False\n",
      "        weight:  0.995175563803\n",
      "\n",
      "    weightId: 129\n",
      "        isFixed: False\n",
      "        weight:  0.987025406794\n",
      "\n",
      "    weightId: 130\n",
      "        isFixed: False\n",
      "        weight:  0.998566534807\n",
      "\n",
      "    weightId: 131\n",
      "        isFixed: False\n",
      "        weight:  0.990683495779\n",
      "\n",
      "    weightId: 132\n",
      "        isFixed: False\n",
      "        weight:  0.984182945231\n",
      "\n",
      "    weightId: 133\n",
      "        isFixed: False\n",
      "        weight:  0.984329758167\n",
      "\n",
      "    weightId: 134\n",
      "        isFixed: False\n",
      "        weight:  1.00445128665\n",
      "\n",
      "    weightId: 135\n",
      "        isFixed: False\n",
      "        weight:  0.976155132336\n",
      "\n",
      "    weightId: 136\n",
      "        isFixed: False\n",
      "        weight:  1.0041821296\n",
      "\n",
      "    weightId: 137\n",
      "        isFixed: False\n",
      "        weight:  0.932676073571\n",
      "\n",
      "    weightId: 138\n",
      "        isFixed: False\n",
      "        weight:  0.976083764937\n",
      "\n",
      "    weightId: 139\n",
      "        isFixed: False\n",
      "        weight:  0.97191998695\n",
      "\n",
      "    weightId: 140\n",
      "        isFixed: False\n",
      "        weight:  0.986880632927\n",
      "\n",
      "    weightId: 141\n",
      "        isFixed: False\n",
      "        weight:  0.986240365401\n",
      "\n",
      "    weightId: 142\n",
      "        isFixed: False\n",
      "        weight:  1.00062191591\n",
      "\n",
      "    weightId: 143\n",
      "        isFixed: False\n",
      "        weight:  0.979301415114\n",
      "\n",
      "    weightId: 144\n",
      "        isFixed: False\n",
      "        weight:  0.979181110069\n",
      "\n",
      "    weightId: 145\n",
      "        isFixed: False\n",
      "        weight:  1.00265690633\n",
      "\n",
      "    weightId: 146\n",
      "        isFixed: False\n",
      "        weight:  1.00910851923\n",
      "\n",
      "    weightId: 147\n",
      "        isFixed: False\n",
      "        weight:  0.919334448026\n",
      "\n",
      "    weightId: 148\n",
      "        isFixed: False\n",
      "        weight:  0.940964887241\n",
      "\n",
      "    weightId: 149\n",
      "        isFixed: False\n",
      "        weight:  0.934399086498\n",
      "\n",
      "    weightId: 150\n",
      "        isFixed: False\n",
      "        weight:  1.00374372986\n",
      "\n",
      "    weightId: 151\n",
      "        isFixed: False\n",
      "        weight:  1.00957546593\n",
      "\n",
      "    weightId: 152\n",
      "        isFixed: False\n",
      "        weight:  0.994372170792\n",
      "\n",
      "    weightId: 153\n",
      "        isFixed: False\n",
      "        weight:  0.990754863179\n",
      "\n",
      "    weightId: 154\n",
      "        isFixed: False\n",
      "        weight:  0.986886750133\n",
      "\n",
      "    weightId: 155\n",
      "        isFixed: False\n",
      "        weight:  0.920252028875\n",
      "\n",
      "    weightId: 156\n",
      "        isFixed: False\n",
      "        weight:  0.916135149465\n",
      "\n",
      "    weightId: 157\n",
      "        isFixed: False\n",
      "        weight:  1.00422291097\n",
      "\n",
      "    weightId: 158\n",
      "        isFixed: False\n",
      "        weight:  0.999840952653\n",
      "\n",
      "    weightId: 159\n",
      "        isFixed: False\n",
      "        weight:  0.976146976062\n",
      "\n",
      "    weightId: 160\n",
      "        isFixed: False\n",
      "        weight:  0.991511357612\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FACTOR 0: DONE WITH LEARNING\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel(lf_propensity=True)\n",
    "gen_model.train(\n",
    "    L_train, deps=deps, epochs=2, decay=0.95, step_size=0.1/L_train.shape[0],\n",
    "    init_acc=2.0, reg_param=0.0,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the generative model to the training candidates to get the noise-aware training label set. We'll refer to these as the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCQAAAFpCAYAAABEaPkcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAIYTJAAAAIDhBAkAAABguP8HD7n1Fz7PvykAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa01c28b610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8347283 ,  0.83886696,  0.83882948,  0.83891417,  0.83883349,\n",
       "        0.83909422,  0.83896739,  0.83887068,  0.83522491,  0.83885966,\n",
       "        0.83892803,  0.84206445,  0.84198309,  0.84184897,  0.85835531,\n",
       "        0.85191113,  0.82655025,  0.83369978,  0.83441775,  0.83881191,\n",
       "        0.83475477,  0.83892882,  0.83559695,  0.83607171,  0.83687768,\n",
       "        0.83913858,  0.84270226,  0.83925904,  0.83128779,  0.83891033,\n",
       "        0.83590365,  0.83568404,  0.83670992,  0.84478527,  0.83292969,\n",
       "        0.84671374,  0.83142875,  0.83910303,  0.8388553 ,  0.85522018,\n",
       "        0.83898223,  0.83570534,  0.83897513,  0.8393548 ,  0.8393876 ,\n",
       "        0.84183427,  0.83542178,  0.83562259,  0.85834749,  0.83878895,\n",
       "        0.83885412,  0.83424867,  0.83601076,  0.83893406,  0.83473607,\n",
       "        0.83891833,  0.8389421 ,  0.83427072,  0.83388436,  0.8394214 ,\n",
       "        0.83668427,  0.83931506,  0.83913302,  0.84815494])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.weights.lf_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 49042 training marginals\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import save_marginals\n",
    "save_marginals(session, L_train, train_marginals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "train_marginals = load_marginals(session, split=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model to Iterate on Labeling Functions\n",
    "Now that we have learned the generative model, we can stop here and use this to potentially debug and/or improve our labeling function set. First, we apply the LFs to our development set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    }
   ],
   "source": [
    "L_dev = labeler.apply_existing(split=4, parallelism=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold_complex', split=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.728\n",
      "Neg. class accuracy: 0.712\n",
      "Precision            0.488\n",
      "Recall               0.728\n",
      "F1                   0.584\n",
      "----------------------------------------\n",
      "TP: 523 | FP: 549 | TN: 1354 | FN: 195\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tp, fp, tn, fn = gen_model.score(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require.undef('viewer');\n",
       "\n",
       "// NOTE: all elements should be selected using this.$el.find to avoid collisions with other Viewers\n",
       "\n",
       "define('viewer', [\"jupyter-js-widgets\"], function(widgets) {\n",
       "    var ViewerView = widgets.DOMWidgetView.extend({\n",
       "        render: function() {\n",
       "            this.cids   = this.model.get('cids');\n",
       "            this.nPages = this.cids.length;\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Insert the html payload\n",
       "            this.$el.append(this.model.get('html'));\n",
       "\n",
       "            // Initialize all labels from previous sessions\n",
       "            this.labels = this.deserializeDict(this.model.get('_labels_serialized'));\n",
       "            for (var i=0; i < this.nPages; i++) {\n",
       "                this.pid = i;\n",
       "                for (var j=0; j < this.cids[i].length; j++) {\n",
       "                    this.cxid = j;\n",
       "                    for (var k=0; k < this.cids[i][j].length; k++) {\n",
       "                        this.cid = k;\n",
       "                        if (this.cids[i][j][k] in this.labels) {\n",
       "                            this.markCurrentCandidate(false);\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Enable button functionality for navigation\n",
       "            var that = this;\n",
       "            this.$el.find(\"#next-cand\").click(function() {\n",
       "                that.switchCandidate(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-cand\").click(function() {\n",
       "                that.switchCandidate(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-context\").click(function() {\n",
       "                that.switchContext(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-context\").click(function() {\n",
       "                that.switchContext(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-page\").click(function() {\n",
       "                that.switchPage(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-page\").click(function() {\n",
       "                that.switchPage(-1);\n",
       "            });\n",
       "            this.$el.find(\"#label-true\").click(function() {\n",
       "                that.labelCandidate(true, true);\n",
       "            });\n",
       "            this.$el.find(\"#label-false\").click(function() {\n",
       "                that.labelCandidate(false, true);\n",
       "            });\n",
       "\n",
       "            // Arrow key functionality\n",
       "            this.$el.keydown(function(e) {\n",
       "                switch(e.which) {\n",
       "                    case 74: // j\n",
       "                    that.switchCandidate(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 73: // i\n",
       "                    that.switchPage(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 76: // l\n",
       "                    that.switchCandidate(1);\n",
       "                    break;\n",
       "\n",
       "                    case 75: // k\n",
       "                    that.switchPage(1);\n",
       "                    break;\n",
       "\n",
       "                    case 84: // t\n",
       "                    that.labelCandidate(true, true);\n",
       "                    break;\n",
       "\n",
       "                    case 70: // f\n",
       "                    that.labelCandidate(false, true);\n",
       "                    break;\n",
       "                }\n",
       "            });\n",
       "\n",
       "            // Show the first page and highlight the first candidate\n",
       "            this.$el.find(\"#viewer-page-0\").show();\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Get candidate selector for currently selected candidate, escaping id properly\n",
       "        getCandidate: function() {\n",
       "            return this.$el.find(\".\"+this.cids[this.pid][this.cxid][this.cid]);\n",
       "        },  \n",
       "\n",
       "        // Color the candidate correctly according to registered label, as well as set highlighting\n",
       "        markCurrentCandidate: function(highlight) {\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var tags = this.$el.find(\".\"+cid);\n",
       "\n",
       "            // Clear color classes\n",
       "            tags.removeClass(\"candidate-h\");\n",
       "            tags.removeClass(\"true-candidate\");\n",
       "            tags.removeClass(\"true-candidate-h\");\n",
       "            tags.removeClass(\"false-candidate\");\n",
       "            tags.removeClass(\"false-candidate-h\");\n",
       "            tags.removeClass(\"highlighted\");\n",
       "\n",
       "            if (highlight) {\n",
       "                if (cid in this.labels) {\n",
       "                    tags.addClass(String(this.labels[cid]) + \"-candidate-h\");\n",
       "                } else {\n",
       "                    tags.addClass(\"candidate-h\");\n",
       "                }\n",
       "            \n",
       "            // If un-highlighting, leave with first non-null coloring\n",
       "            } else {\n",
       "                var that = this;\n",
       "                tags.each(function() {\n",
       "                    var cids = $(this).attr('class').split(/\\s+/).map(function(item) {\n",
       "                        return parseInt(item);\n",
       "                    });\n",
       "                    cids.sort();\n",
       "                    for (var i in cids) {\n",
       "                        if (cids[i] in that.labels) {\n",
       "                            var label = that.labels[cids[i]];\n",
       "                            $(this).addClass(String(label) + \"-candidate\");\n",
       "                            $(this).removeClass(String(!label) + \"-candidate\");\n",
       "                            break;\n",
       "                        }\n",
       "                    }\n",
       "                });\n",
       "            }\n",
       "\n",
       "            // Extra highlighting css\n",
       "            if (highlight) {\n",
       "                tags.addClass(\"highlighted\");\n",
       "            }\n",
       "\n",
       "            // Classes for showing direction of relation\n",
       "            if (highlight) {\n",
       "                this.$el.find(\".\"+cid+\"-0\").addClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").addClass(\"right-candidate\");\n",
       "            } else {\n",
       "                this.$el.find(\".\"+cid+\"-0\").removeClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").removeClass(\"right-candidate\");\n",
       "            }\n",
       "        },\n",
       "\n",
       "        // Cycle through candidates and highlight, by increment inc\n",
       "        switchCandidate: function(inc) {\n",
       "            var N = this.cids[this.pid].length\n",
       "            var M = this.cids[this.pid][this.cxid].length;\n",
       "            if (N == 0 || M == 0) { return false; }\n",
       "\n",
       "            // Clear highlighting from previous candidate\n",
       "            if (inc != 0) {\n",
       "                this.markCurrentCandidate(false);\n",
       "\n",
       "                // Increment the cid counter\n",
       "\n",
       "                // Move to next context\n",
       "                if (this.cid + inc >= M) {\n",
       "                    while (this.cid + inc >= M) {\n",
       "                        \n",
       "                        // At last context on page, halt\n",
       "                        if (this.cxid == N - 1) {\n",
       "                            this.cid = M - 1;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to next context\n",
       "                        } else {\n",
       "                            inc -= M - this.cid;\n",
       "                            this.cxid += 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = 0;\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                // Move to previous context\n",
       "                } else if (this.cid + inc < 0) {\n",
       "                    while (this.cid + inc < 0) {\n",
       "                        \n",
       "                        // At first context on page, halt\n",
       "                        if (this.cxid == 0) {\n",
       "                            this.cid = 0;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to previous context\n",
       "                        } else {\n",
       "                            inc += this.cid + 1;\n",
       "                            this.cxid -= 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = M - 1;\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "\n",
       "                // Move within current context\n",
       "                this.cid += inc;\n",
       "            }\n",
       "            this.markCurrentCandidate(true);\n",
       "\n",
       "            // Push this new cid to the model\n",
       "            this.model.set('_selected_cid', this.cids[this.pid][this.cxid][this.cid]);\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Switch through contexts\n",
       "        switchContext: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "\n",
       "            // Iterate context on this page\n",
       "            var M = this.cids[this.pid].length;\n",
       "            if (this.cxid + inc < 0) {\n",
       "                this.cxid = 0;\n",
       "            } else if (this.cxid + inc >= M) {\n",
       "                this.cxid = M - 1;\n",
       "            } else {\n",
       "                this.cxid += inc;\n",
       "            }\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Switch through pages\n",
       "        switchPage: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "            this.$el.find(\".viewer-page\").hide();\n",
       "            if (this.pid + inc < 0) {\n",
       "                this.pid = 0;\n",
       "            } else if (this.pid + inc > this.nPages - 1) {\n",
       "                this.pid = this.nPages - 1;\n",
       "            } else {\n",
       "                this.pid += inc;\n",
       "            }\n",
       "            this.$el.find(\"#viewer-page-\"+this.pid).show();\n",
       "\n",
       "            // Show pagination\n",
       "            this.$el.find(\"#page\").html(this.pid);\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.cxid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Label currently-selected candidate\n",
       "        labelCandidate: function(label, highlighted) {\n",
       "            var c    = this.getCandidate();\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var cl   = String(label) + \"-candidate\";\n",
       "            var clh  = String(label) + \"-candidate-h\";\n",
       "            var cln  = String(!label) + \"-candidate\";\n",
       "            var clnh = String(!label) + \"-candidate-h\";\n",
       "\n",
       "            // Toggle label highlighting\n",
       "            if (c.hasClass(cl) || c.hasClass(clh)) {\n",
       "                c.removeClass(cl);\n",
       "                c.removeClass(clh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(\"candidate-h\");\n",
       "                }\n",
       "                this.labels[cid] = null;\n",
       "                this.send({event: 'delete_label', cid: cid});\n",
       "            } else {\n",
       "                c.removeClass(cln);\n",
       "                c.removeClass(clnh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(clh);\n",
       "                } else {\n",
       "                    c.addClass(cl);\n",
       "                }\n",
       "                this.labels[cid] = label;\n",
       "                this.send({event: 'set_label', cid: cid, value: label});\n",
       "            }\n",
       "\n",
       "            // Set the label and pass back to the model\n",
       "            this.model.set('_labels_serialized', this.serializeDict(this.labels));\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Serialization of hash maps, because traitlets Dict doesn't seem to work...\n",
       "        serializeDict: function(d) {\n",
       "            var s = [];\n",
       "            for (var key in d) {\n",
       "                s.push(key+\"~~\"+d[key]);\n",
       "            }\n",
       "            return s.join();\n",
       "        },\n",
       "\n",
       "        // Deserialization of hash maps\n",
       "        deserializeDict: function(s) {\n",
       "            var d = {};\n",
       "            var entries = s.split(/,/);\n",
       "            var kv;\n",
       "            for (var i in entries) {\n",
       "                kv = entries[i].split(/~~/);\n",
       "                if (kv[1] == \"true\") {\n",
       "                    d[kv[0]] = true;\n",
       "                } else if (kv[1] == \"false\") {\n",
       "                    d[kv[0]] = false;\n",
       "                }\n",
       "            }\n",
       "            return d;\n",
       "        },\n",
       "    });\n",
       "\n",
       "    return {\n",
       "        ViewerView: ViewerView\n",
       "    };\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    sv = SentenceNgramViewer(tp, session, height=400)\n",
    "else:\n",
    "    sv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e324a9f3f494e02a8d696c24669419d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we should be getting an F1 score of around 0.6 to 0.7 on the development set, which is pretty good! However, we should be very careful in interpreting this. Since we developed our labeling functions using this development set as a guide, and our generative model is composed of these labeling functions, we expect it to score very well here!\n",
    "In fact, it is probably somewhat overfit to this set. However this is fine, since in the next tutorial, we'll train a more powerful end extraction model which will generalize beyond the development set, and which we will evaluate on a blind test set (i.e. one we never looked at during development)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Doing Some Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "At this point, we might want to look at some examples in one of the error buckets. For example, one of the false negatives that we did not correctly label as true mentions. To do this, we can again just use the Viewer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require.undef('viewer');\n",
       "\n",
       "// NOTE: all elements should be selected using this.$el.find to avoid collisions with other Viewers\n",
       "\n",
       "define('viewer', [\"jupyter-js-widgets\"], function(widgets) {\n",
       "    var ViewerView = widgets.DOMWidgetView.extend({\n",
       "        render: function() {\n",
       "            this.cids   = this.model.get('cids');\n",
       "            this.nPages = this.cids.length;\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Insert the html payload\n",
       "            this.$el.append(this.model.get('html'));\n",
       "\n",
       "            // Initialize all labels from previous sessions\n",
       "            this.labels = this.deserializeDict(this.model.get('_labels_serialized'));\n",
       "            for (var i=0; i < this.nPages; i++) {\n",
       "                this.pid = i;\n",
       "                for (var j=0; j < this.cids[i].length; j++) {\n",
       "                    this.cxid = j;\n",
       "                    for (var k=0; k < this.cids[i][j].length; k++) {\n",
       "                        this.cid = k;\n",
       "                        if (this.cids[i][j][k] in this.labels) {\n",
       "                            this.markCurrentCandidate(false);\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Enable button functionality for navigation\n",
       "            var that = this;\n",
       "            this.$el.find(\"#next-cand\").click(function() {\n",
       "                that.switchCandidate(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-cand\").click(function() {\n",
       "                that.switchCandidate(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-context\").click(function() {\n",
       "                that.switchContext(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-context\").click(function() {\n",
       "                that.switchContext(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-page\").click(function() {\n",
       "                that.switchPage(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-page\").click(function() {\n",
       "                that.switchPage(-1);\n",
       "            });\n",
       "            this.$el.find(\"#label-true\").click(function() {\n",
       "                that.labelCandidate(true, true);\n",
       "            });\n",
       "            this.$el.find(\"#label-false\").click(function() {\n",
       "                that.labelCandidate(false, true);\n",
       "            });\n",
       "\n",
       "            // Arrow key functionality\n",
       "            this.$el.keydown(function(e) {\n",
       "                switch(e.which) {\n",
       "                    case 74: // j\n",
       "                    that.switchCandidate(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 73: // i\n",
       "                    that.switchPage(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 76: // l\n",
       "                    that.switchCandidate(1);\n",
       "                    break;\n",
       "\n",
       "                    case 75: // k\n",
       "                    that.switchPage(1);\n",
       "                    break;\n",
       "\n",
       "                    case 84: // t\n",
       "                    that.labelCandidate(true, true);\n",
       "                    break;\n",
       "\n",
       "                    case 70: // f\n",
       "                    that.labelCandidate(false, true);\n",
       "                    break;\n",
       "                }\n",
       "            });\n",
       "\n",
       "            // Show the first page and highlight the first candidate\n",
       "            this.$el.find(\"#viewer-page-0\").show();\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Get candidate selector for currently selected candidate, escaping id properly\n",
       "        getCandidate: function() {\n",
       "            return this.$el.find(\".\"+this.cids[this.pid][this.cxid][this.cid]);\n",
       "        },  \n",
       "\n",
       "        // Color the candidate correctly according to registered label, as well as set highlighting\n",
       "        markCurrentCandidate: function(highlight) {\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var tags = this.$el.find(\".\"+cid);\n",
       "\n",
       "            // Clear color classes\n",
       "            tags.removeClass(\"candidate-h\");\n",
       "            tags.removeClass(\"true-candidate\");\n",
       "            tags.removeClass(\"true-candidate-h\");\n",
       "            tags.removeClass(\"false-candidate\");\n",
       "            tags.removeClass(\"false-candidate-h\");\n",
       "            tags.removeClass(\"highlighted\");\n",
       "\n",
       "            if (highlight) {\n",
       "                if (cid in this.labels) {\n",
       "                    tags.addClass(String(this.labels[cid]) + \"-candidate-h\");\n",
       "                } else {\n",
       "                    tags.addClass(\"candidate-h\");\n",
       "                }\n",
       "            \n",
       "            // If un-highlighting, leave with first non-null coloring\n",
       "            } else {\n",
       "                var that = this;\n",
       "                tags.each(function() {\n",
       "                    var cids = $(this).attr('class').split(/\\s+/).map(function(item) {\n",
       "                        return parseInt(item);\n",
       "                    });\n",
       "                    cids.sort();\n",
       "                    for (var i in cids) {\n",
       "                        if (cids[i] in that.labels) {\n",
       "                            var label = that.labels[cids[i]];\n",
       "                            $(this).addClass(String(label) + \"-candidate\");\n",
       "                            $(this).removeClass(String(!label) + \"-candidate\");\n",
       "                            break;\n",
       "                        }\n",
       "                    }\n",
       "                });\n",
       "            }\n",
       "\n",
       "            // Extra highlighting css\n",
       "            if (highlight) {\n",
       "                tags.addClass(\"highlighted\");\n",
       "            }\n",
       "\n",
       "            // Classes for showing direction of relation\n",
       "            if (highlight) {\n",
       "                this.$el.find(\".\"+cid+\"-0\").addClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").addClass(\"right-candidate\");\n",
       "            } else {\n",
       "                this.$el.find(\".\"+cid+\"-0\").removeClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").removeClass(\"right-candidate\");\n",
       "            }\n",
       "        },\n",
       "\n",
       "        // Cycle through candidates and highlight, by increment inc\n",
       "        switchCandidate: function(inc) {\n",
       "            var N = this.cids[this.pid].length\n",
       "            var M = this.cids[this.pid][this.cxid].length;\n",
       "            if (N == 0 || M == 0) { return false; }\n",
       "\n",
       "            // Clear highlighting from previous candidate\n",
       "            if (inc != 0) {\n",
       "                this.markCurrentCandidate(false);\n",
       "\n",
       "                // Increment the cid counter\n",
       "\n",
       "                // Move to next context\n",
       "                if (this.cid + inc >= M) {\n",
       "                    while (this.cid + inc >= M) {\n",
       "                        \n",
       "                        // At last context on page, halt\n",
       "                        if (this.cxid == N - 1) {\n",
       "                            this.cid = M - 1;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to next context\n",
       "                        } else {\n",
       "                            inc -= M - this.cid;\n",
       "                            this.cxid += 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = 0;\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                // Move to previous context\n",
       "                } else if (this.cid + inc < 0) {\n",
       "                    while (this.cid + inc < 0) {\n",
       "                        \n",
       "                        // At first context on page, halt\n",
       "                        if (this.cxid == 0) {\n",
       "                            this.cid = 0;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to previous context\n",
       "                        } else {\n",
       "                            inc += this.cid + 1;\n",
       "                            this.cxid -= 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = M - 1;\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "\n",
       "                // Move within current context\n",
       "                this.cid += inc;\n",
       "            }\n",
       "            this.markCurrentCandidate(true);\n",
       "\n",
       "            // Push this new cid to the model\n",
       "            this.model.set('_selected_cid', this.cids[this.pid][this.cxid][this.cid]);\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Switch through contexts\n",
       "        switchContext: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "\n",
       "            // Iterate context on this page\n",
       "            var M = this.cids[this.pid].length;\n",
       "            if (this.cxid + inc < 0) {\n",
       "                this.cxid = 0;\n",
       "            } else if (this.cxid + inc >= M) {\n",
       "                this.cxid = M - 1;\n",
       "            } else {\n",
       "                this.cxid += inc;\n",
       "            }\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Switch through pages\n",
       "        switchPage: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "            this.$el.find(\".viewer-page\").hide();\n",
       "            if (this.pid + inc < 0) {\n",
       "                this.pid = 0;\n",
       "            } else if (this.pid + inc > this.nPages - 1) {\n",
       "                this.pid = this.nPages - 1;\n",
       "            } else {\n",
       "                this.pid += inc;\n",
       "            }\n",
       "            this.$el.find(\"#viewer-page-\"+this.pid).show();\n",
       "\n",
       "            // Show pagination\n",
       "            this.$el.find(\"#page\").html(this.pid);\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.cxid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Label currently-selected candidate\n",
       "        labelCandidate: function(label, highlighted) {\n",
       "            var c    = this.getCandidate();\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var cl   = String(label) + \"-candidate\";\n",
       "            var clh  = String(label) + \"-candidate-h\";\n",
       "            var cln  = String(!label) + \"-candidate\";\n",
       "            var clnh = String(!label) + \"-candidate-h\";\n",
       "\n",
       "            // Toggle label highlighting\n",
       "            if (c.hasClass(cl) || c.hasClass(clh)) {\n",
       "                c.removeClass(cl);\n",
       "                c.removeClass(clh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(\"candidate-h\");\n",
       "                }\n",
       "                this.labels[cid] = null;\n",
       "                this.send({event: 'delete_label', cid: cid});\n",
       "            } else {\n",
       "                c.removeClass(cln);\n",
       "                c.removeClass(clnh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(clh);\n",
       "                } else {\n",
       "                    c.addClass(cl);\n",
       "                }\n",
       "                this.labels[cid] = label;\n",
       "                this.send({event: 'set_label', cid: cid, value: label});\n",
       "            }\n",
       "\n",
       "            // Set the label and pass back to the model\n",
       "            this.model.set('_labels_serialized', this.serializeDict(this.labels));\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Serialization of hash maps, because traitlets Dict doesn't seem to work...\n",
       "        serializeDict: function(d) {\n",
       "            var s = [];\n",
       "            for (var key in d) {\n",
       "                s.push(key+\"~~\"+d[key]);\n",
       "            }\n",
       "            return s.join();\n",
       "        },\n",
       "\n",
       "        // Deserialization of hash maps\n",
       "        deserializeDict: function(s) {\n",
       "            var d = {};\n",
       "            var entries = s.split(/,/);\n",
       "            var kv;\n",
       "            for (var i in entries) {\n",
       "                kv = entries[i].split(/~~/);\n",
       "                if (kv[1] == \"true\") {\n",
       "                    d[kv[0]] = true;\n",
       "                } else if (kv[1] == \"false\") {\n",
       "                    d[kv[0]] = false;\n",
       "                }\n",
       "            }\n",
       "            return d;\n",
       "        },\n",
       "    });\n",
       "\n",
       "    return {\n",
       "        ViewerView: ViewerView\n",
       "    };\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    sv = SentenceNgramViewer(fp, session, height=400)\n",
    "else:\n",
    "    sv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8af770152ed4037815608e83f98dd3b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens,\n",
    "    get_between_tokens,\n",
    "    get_right_tokens,\n",
    "    contains_token,\n",
    "    get_doc_candidate_spans,\n",
    "    get_sent_candidate_spans,\n",
    "    get_text_between,\n",
    "    get_text_splits,\n",
    "    get_tagged_text,\n",
    "    is_inverted,\n",
    "    get_tagged_text,\n",
    ")\n",
    "c = sv.get_selected() if sv else list(fp.union(fn))[0]\n",
    "print(c)\n",
    "print(\"\\n\")\n",
    "\n",
    "c.labels\n",
    "fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically Creating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator\n",
    "import multiprocessing\n",
    "featurizer = FeatureAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    }
   ],
   "source": [
    "%time F_train = featurizer.apply(split=3, parallelism=multiprocessing.cpu_count())\n",
    "F_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we apply the feature set we just got from the training set to the dev and test sets by using apply_existing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 2min 22s, sys: 6.58 s, total: 2min 29s\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "F_dev  = featurizer.apply_existing(split=4, parallelism=multiprocessing.cpu_count())\n",
    "F_test = featurizer.apply_existing(split=5, parallelism=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F_train = featurizer.load_matrix(session, split=3)\n",
    "F_dev   = featurizer.load_matrix(session, split=4)\n",
    "F_test  = featurizer.load_matrix(session, split=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Discriminative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the training marginals to train a discriminative model that classifies each Candidate as a true or false mention. We'll use a random hyperparameter search, evaluated on the development set labels, to find the best hyperparameters for our model. To run a hyperparameter search, we need labels for a development set. If they aren't already available, we can manually create labels using the Viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "disc_model = SparseLogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we set up and run the hyperparameter search, training our model with different hyperparamters and picking the best model configuration to keep. We'll set the random seed to maintain reproducibility.\n",
    "Note that we are fitting our model's parameters to the training set generated by our labeling functions, while we are picking hyperparamters with respect to score over the development set labels which we created by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RandomSearch search of size 20. Search space size = 125.\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning.utils import MentionScorer\n",
    "from snorkel.learning import RandomSearch, ListParameter, RangeParameter\n",
    "\n",
    "# Searching over learning rate\n",
    "rate_param = RangeParameter('lr', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l1_param  = RangeParameter('l1_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l2_param  = RangeParameter('l2_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "\n",
    "searcher = RandomSearch(session, disc_model, F_train, train_marginals, [rate_param, l1_param, l2_param], n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load in our dev set labels. We will pick the optimal result from the hyperparameter search by testing against these labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold_complex', split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the hyperparameter search / train the end extraction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[1] Testing lr = 1.00e-02, l1_penalty = 1.00e-03, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=0.001 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=25364  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (23.55s)\tAvg. loss=0.947219\tNNZ=335440\n",
      "[SparseLR] Epoch 25 (612.21s)\tAvg. loss=0.675827\tNNZ=335440\n",
      "[SparseLR] Epoch 49 (1177.96s)\tAvg. loss=0.666087\tNNZ=335440\n",
      "[SparseLR] Training done (1177.96s)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "512842",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e855854b8105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1701\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_gold_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrebalance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/snorkel/snorkel/learning/utils.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_validation, validation_labels, gold_candidate_set, b, set_unlabeled_as_neg, validation_kwargs, **model_hyperparams)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0mgold_candidate_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_unlabeled_as_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mvalidation_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             )\n\u001b[1;32m    358\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores_from_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/snorkel/snorkel/learning/disc_learning.pyc\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, session, X_test, test_labels, gold_candidate_set, b, set_unlabeled_as_neg, display, scorer, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtest_marginals\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarginals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         return s.score(test_marginals, None, b=b, display=display,\n\u001b[0;32m---> 40\u001b[0;31m                        set_unlabeled_as_neg=set_unlabeled_as_neg)\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/snorkel/snorkel/learning/utils.pyc\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, test_marginals, train_marginals, b, set_unlabeled_as_neg, set_at_thresh_as_neg, display)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mtest_label_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_row_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mtest_label\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_label_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/snorkel/snorkel/annotations.pyc\u001b[0m in \u001b[0;36mget_row_index\u001b[0;34m(self, candidate)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_row_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;34m\"\"\"Return the row index of the Candidate\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidate_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 512842"
     ]
    }
   ],
   "source": [
    "np.random.seed(1701)\n",
    "searcher.fit(F_dev, L_gold_dev, n_epochs=50, rebalance=0.5, print_freq=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that to train a model without tuning any hyperparameters (at your own risk) just use the train method of the discriminative model. For instance, to train with 20 epochs and a learning rate of 0.001, you could run:\n",
    "disc_model.train(F_train, train_marginals, n_epochs=20, lr=0.001)\n",
    "We can analyze the learned model by examining the weights. For example, we can print out the features with the highest weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, _ = disc_model.get_weights()\n",
    "largest_idxs = reversed(np.argsort(np.abs(w))[-5:])\n",
    "for i in largest_idxs:\n",
    "    print 'Feature: {0: <70}Weight: {1:.6f}'.format(F_train.get_key(session, i).name, w[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this last section of the tutorial, we'll get the score we've been after: the performance of the extraction model on the blind test set (split 2). First, we load the test set labels and gold candidates we made in Part III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold_complex', split=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we score using the discriminative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, _, _, _ = disc_model.score(session, F_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that if this is the final test set that you will be reporting final numbers on, to avoid biasing results you should not inspect results. However you can run the model on your development set and, as we did in the previous part with the generative labeling function model, inspect examples to do error analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thomas's Code\n",
    "## `DDLiteModel` objects and feature generation\n",
    "We'll then create a `DDLiteModel` object for our extracted gene candidates. This lets us iterate with our model and labeling functions. Since `Entities` object defines a feature generation method, features are automatically created when we initialize a `DDLiteModel` object. These are generic features from the NLP markup and dependency tree in the neighborhood of the mention. Alternatively, you can define a custom feature set as a NumPy array or a Scipy sparse matrix. Here, we pickle our feature matrix so we don't have to recompute it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = None\n",
    "\n",
    "pkl_f = 'gene_tag_example/gene_tag_feats_v1.pkl'\n",
    "try:\n",
    "    with open(pkl_f, 'rb') as f:\n",
    "        feats = cPickle.load(f)\n",
    "except:\n",
    "    %time E.extract_features()\n",
    "    with open(pkl_f, 'w+') as f:\n",
    "        cPickle.dump(E.feats, f)\n",
    "\n",
    "DDL = DDLiteModel(E, feats)\n",
    "print \"Extracted {} features for each of {} mentions\".format(DDL.num_feats(), DDL.num_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a blind, gold standard ground truth set to evaluate our predictions. We can add these by using the uids for the candidates we want, and align these with a value of 1 for positive or a value of -1 for negative. We'll load in a set now using `DDL.update_gt()` and set it as the holdout. We'll assign half of the holdout to a validation set for parameter tuning, and the other half to a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('gene_tag_example/gt/uids.pkl', 'rb') as f:\n",
    "    uids = cPickle.load(f)\n",
    "with open('gene_tag_example/gt/gt.pkl', 'rb') as f:\n",
    "    gt = cPickle.load(f)\n",
    "    \n",
    "DDL.update_gt(gt[:50], uids=uids[:50])\n",
    "DDL.set_holdout(validation_frac=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a ground truth set with MindTagger\n",
    "In order to evaluate our labeling functions and learning results, we'll create a small set of ground truth labels for some candidates using [Mindtagger](http://deepdive.stanford.edu/labeling). This will highlight each candidate in the sentence in which it appears. We set the response to yes if it is a mention of gene, and no otherwise. If you aren't sure, you can abstain from labeling. In a real application, we would likely want to tag more than 20 candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DDL.open_mindtagger(num_sample=100, width='100%', height=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll retrieve the tags and add them to our `DDLiteModel`. We can also use a previously defined Mindtagger label set, similar to how we added our gold labels (this time, we'll use indexes instead of uids). These labels are used for evaluating our labeling functions against the *development set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DDL.add_mindtagger_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DDL.update_gt(gt[50:], uids=uids[50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing labeling functions\n",
    "We'll use data programming to learn a logistic regression model which will predict the probability of a candidate entity being a true gene mention. Since our training data is not manually labeled, we'll generate many (potentially noisy) labels as a surrogate for precise, manual labels. Feature extraction and model learning are very simple in ddlite. Writing labeling functions is where the real artistry comes in. One of ddlite's goals is to enable rapid prototyping, debugging, and experimenting with labeling functions. These can be used either to create a simple standalone app, or to plug into DeepDive. Labeling functions, or LFs, are functions that take an `Candidate` object. They must return 1 (for a positive label), 0 (for abstaining), or -1 (for a negative example). For now, we'll write a few simple LFs to get started:\n",
    "\n",
    "* The first, second, and third LFs return a positive label if the lemma \"gene\", \"mutant\", or \"express\" appear in a window around the mention, respectively\n",
    "* The third LF returns a positive label if the dependency parent of any of the words in the mention is \"mutation\", and abstains otherwise\n",
    "* The fourth, fifth, and sixth LF return a negative label if the mention candidate contains \"DNA\", \"RNA\", or \"SNP\" respectively (these are common uppercase nouns which are not genes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LF_gene(m):\n",
    "    return 1 if ('gene' in m.post_window('lemmas')) or ('gene' in m.pre_window('lemmas')) else 0\n",
    "def LF_gene_dp(m):\n",
    "    return 1 if 'gene' in [m.lemmas[m.dep_parents[i] - 1] for i in m.idxs] else 0\n",
    "def LF_genotype_dp(m):\n",
    "    return 1 if 'genotype' in [m.lemmas[m.dep_parents[i] - 1] for i in m.idxs] else 0\n",
    "def LF_mutant(m):\n",
    "    return 1 if ('mutant' in m.post_window('lemmas')) or ('mutant' in m.pre_window('lemmas')) else 0\n",
    "def LF_variant(m):\n",
    "    return 1 if ('variant' in m.post_window('lemmas')) or ('variant' in m.pre_window('lemmas')) else 0\n",
    "def LF_express(m):\n",
    "    return 1 if ('express' in m.post_window('lemmas')) or ('express' in m.pre_window('lemmas')) else 0\n",
    "def LF_mutation(m):\n",
    "    return 1 if 'mutation' in [m.lemmas[m.dep_parents[i] - 1] for i in m.idxs] else 0\n",
    "def LF_JJ(m):\n",
    "    return 1 if 'JJ' in m.post_window('poses') else 0\n",
    "def LF_IN(m):\n",
    "    return 1 if 'IN' in m.pre_window('poses', 1) else 0\n",
    "\n",
    "def LF_dna(m):\n",
    "    return -1 if 'DNA' in m.mention('words') else 0\n",
    "def LF_rna(m):\n",
    "    return -1 if 'RNA' in m.mention('words') else 0\n",
    "def LF_snp(m):\n",
    "    return -1 if 'SNP' in m.mention('words') else 0\n",
    "def LF_protein(m):\n",
    "    return -1 if 'protein' in m.pre_window('lemmas') else 0\n",
    "def LF_LRB(m):\n",
    "    return -1 if '-LRB-' in m.post_window('poses', 1) else 0\n",
    "def LF_RRB(m):\n",
    "    return -1 if '-RRB-' in m.post_window('poses', 1) else 0\n",
    "def LF_dev_dp(m):\n",
    "    return -1 if 'development' in [m.lemmas[m.dep_parents[i] - 1] for i in m.idxs] else 0\n",
    "def LF_protein_dp(m):\n",
    "    return -1 if 'protein' in [m.lemmas[m.dep_parents[i] - 1] for i in m.idxs] else 0\n",
    "def LF_network_dp(m):\n",
    "    return -1 if 'network' in [m.lemmas[m.dep_parents[i] - 1] for i in m.idxs] else 0\n",
    "def LF_JJ_dp(m):\n",
    "    return -1 if 'JJ' in [m.poses[m.dep_parents[i] - 1] for i in m.idxs] else 0\n",
    "def LF_NNP(m):\n",
    "    return -1 if 'NNP' in m.mention('poses') else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our intuition is that the few simple LFs given above won't yield a good model. We'll see this more concretely when we evaluate the labeling functions. For now, let's leave them as is. After writing the LFs, we simply collect them and apply them to mentions. If we define more LFs later, we can apply them incrementally or overwrite all the LFs so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFs = [LF_JJ, LF_JJ_dp, LF_NNP, LF_RRB, LF_dev_dp, LF_dna, LF_express, LF_gene, LF_gene_dp,\n",
    "       LF_genotype_dp, LF_mutant, LF_mutation, LF_network_dp, LF_protein, LF_protein_dp,\n",
    "       LF_rna, LF_snp, LF_variant, LF_IN, LF_LRB]\n",
    "DDL.apply_lfs(LFs, clear=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use ddlite's LF assessment utilities to debug and analyze our LFs before running inference. First, we'll generate summary plots which show the following:\n",
    "\n",
    "* Do the LFs cover the data well or do we have many candidates for which all of the LFs abstained?\n",
    "* Is there overlap between the LFs? Do many return positive or negative labels for the same candidate?\n",
    "* Do the LFs conflict with each other? Do candidates tend to have many of one type of label, or a mix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DDL.print_lf_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DDL.plot_lf_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we may expect, the few LFs do not cover the data very well. We can switch to an LF view of these measures and show tables for the LFs with the most conflict, lowest coverage, and lowest empirical accuracy as compared to the development set labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DDL.top_conflict_lfs(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DDL.lowest_coverage_lfs(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DDL.lowest_empirical_accuracy_lfs(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DDL.lf_summary_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our development set is small, we can't evaluate LF performance particularly well. If we observe LFs with very low accuracy, poor generalization, and a sufficiently large sample size, this could be an indication that the LF is buggy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "To learn weights for the features and LFs, we use a simple, regularized logistic regression model. Again, the results won't be meaningful without more LFs. We'll tell ddlite to log the results and print out a summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (12,4)\n",
    "mu_seq = np.ravel([1e-9, 1e-5, 1e-3, 1e-1])\n",
    "lf_opts = {'sample': False, 'verbose': True}\n",
    "model_opts = {'sample': False, 'n_iter': 3000, 'alpha': 0.5, 'mu': mu_seq, 'bias': False, 'verbose': True}\n",
    "%time DDL.train_model(method='lr', lf_opts=lf_opts, model_opts=model_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use [DeepDive-style calibration plots](http://deepdive.stanford.edu/calibration) to evaluate the quality of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DDL.plot_calibration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used pipeline learning, we can also see the gain in accuracy we got from using features and not just LFs as a collection of rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idxs, gt = DDL.get_labeled_ground_truth(subset=DDL.holdout())\n",
    "acc_feats = np.mean(DDL.get_predicted(subset=DDL.holdout()) == gt)\n",
    "acc_lfs = np.mean(DDL.get_lf_predicted(subset=DDL.holdout()) == gt)\n",
    "print \"LF accuracy: {:.3f}\\nFull model accuracy: {:.3f}\".format(acc_lfs, acc_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating with labeling functions\n",
    "After analyzing our LFs and obtaining model results, we can revise our LF set. We can see all of the results thus far, and reopen MindTagger to see some mentions that aren't currently covered by LFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DDL.show_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DDL.open_mindtagger(width='100%', height=1200, abstain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use LSTM to learn weights, we just just call the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lf_opts = {'sample': False, 'verbose': True}\n",
    "model_opts = {'n_iter': 300, 'verbose': True, 'contain_mention': True, 'word_window_length': 0, 'ignore_case': False}\n",
    "%time DDL.train_model(method='lstm', lf_opts=lf_opts, model_opts=model_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idxs, gt = DDL.get_labeled_ground_truth(subset=DDL.holdout())\n",
    "acc_feats = np.mean(DDL.get_predicted(subset=DDL.holdout()) == gt)\n",
    "print \"Full model accuracy: {:.3f}\".format(acc_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
