{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging genes with ddlite: learning and labeling function iteration\n",
    "\n",
    "## Introduction\n",
    "In this example **ddlite** app, we'll build a gene tagger from scratch. Domain-specific tagging systems take months or years to develop. They use hand-crafted model circuitry and accurate, hand-labeled training data. We'll start to build a pretty good one in a few minutes with none of those things. The generalized extraction and learning utilities provided by ddlite will allow us to turn a sampling of article abstracts and some basic domain knowledge into an automated tagging system. Specifically, we want an accurate tagger for genes in academic articles. We have comprehensive dictionaries of genes, but applying a simple matching rule might yield a lot of false positives. For example, \"p53\" might get tagged as a gene if it refers to a page number. Our goal is to use distant supervision to improve precision.\n",
    "\n",
    "Here's the pipeline we'll follow:\n",
    "\n",
    "1. Obtain and parse input data (relevant article abstracts from PubMed)\n",
    "2. Extract candidates for tagging\n",
    "3. Generate features\n",
    "4. Create a test set\n",
    "5. Write labeling functions\n",
    "6. Learn the tagging model\n",
    "7. Iterate on labeling functions\n",
    "\n",
    "Parts 3 through 7 are covered in this notebook. It requires candidates extracted from `GeneTaggerExample_Extraction.ipynb`, which covers parts 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# print(os.environ['SNORKELDB'])\n",
    "# Use production DB\n",
    "from set_env import set_env\n",
    "set_env() \n",
    "sys.path.insert(1, '../snorkel')\n",
    "\n",
    "# Must set SNORKELDB before importing SnorkelSession\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import TextDocPreprocessor\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence, candidate_subclass\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "session = SnorkelSession()\n",
    "\n",
    "#np.random.seed(seed=1701)\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (18,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading candidate extractions\n",
    "First, we'll load in the candidates that we created in the last notebook. We can construct an docs object with the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PhenoPair = candidate_subclass('ComplexPhenotypes', ['descriptor', 'entity'])\n",
    "\n",
    "docs = session.query(PhenoPair).filter(PhenoPair.split == 3).all()  #should edit split to be 3.  \n",
    "\n",
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Sentences:\", session.query(Sentence).count()\n",
    "\n",
    "##Once we get all the labels, for loop through all docs and split into train, dev, test. \n",
    "\n",
    "print 'Document set:\\t{0} candidates'.format(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_docs = session.query(PhenoPair).filter(PhenoPair.split == 4).all()  #should edit split to be 1. \n",
    "test_docs = session.query(PhenoPair).filter(PhenoPair.split == 5).all()  #should edit split to be 1.\n",
    "print 'Dev set:\\t{0} candidates'.format(len(dev_docs))\n",
    "print 'Test set:\\t{0} candidates'.format(len(test_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models.context import TemporaryContext\n",
    "import re\n",
    "\n",
    "print docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens,\n",
    "    get_between_tokens,\n",
    "    get_right_tokens,\n",
    "    contains_token,\n",
    "    get_text_between,\n",
    "    get_text_splits,\n",
    "    get_tagged_text,\n",
    "    is_inverted,\n",
    "    get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "def LF_mutant(c):\n",
    "    return 1 if ('mutant' in get_right_tokens(c, attrib='lemmas')) or ('mutant' in get_left_tokens(c, attrib='lemmas')) else 0\n",
    "def LF_variant(c):\n",
    "    return 1 if ('variant' in get_right_tokens(c, attrib='lemmas')) or ('variant' in get_left_tokens(c, attrib='lemmas')) else 0\n",
    "def LF_express(c):\n",
    "    return 1 if ('express' in get_right_tokens(c, attrib='lemmas')) or ('express' in get_left_tokens(c, attrib='lemmas')) else 0  \n",
    "def LF_JJ(c):\n",
    "    return 1 if 'JJ' in get_right_tokens(c, attrib='pos_tags') else 0\n",
    "def LF_IN(c):\n",
    "    return 1 if 'IN' in get_right_tokens(c, window=1, attrib='pos_tags') else 0\n",
    "def LF_dna(c):\n",
    "    return -1 if contains_token(c, 'DNA', attrib='words') else 0\n",
    "def LF_rna(c):\n",
    "    return -1 if contains_token(c, 'RNA', attrib='words') else 0\n",
    "def LF_snp(c):\n",
    "    return -1 if contains_token(c, 'SNP', attrib='words') else 0\n",
    "def LF_protein(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c, attrib='lemmas') else 0\n",
    "def LF_LRB(c):\n",
    "    return -1 if '-LRB-' in get_right_tokens(c, window=1, attrib='pos_tags') else 0\n",
    "def LF_RRB(c):\n",
    "    return -1 if '-RRB-' in get_right_tokens(c, window=1, attrib='pos_tags') else 0    \n",
    "def LF_NNP(c):\n",
    "    return -1 if contains_token(c, 'NNP', attrib='pos_tags') else 0\n",
    "def lfdistBtw0(c):\n",
    "    return 1 if len(get_between_tokens(c, attrib=\"words\")) == 0 else 0\n",
    "def lfdistBtw(c):\n",
    "    return 1 if len(get_between_tokens(c, attrib=\"words\")) < 3 else 0\n",
    "def lfdistBtwNeg(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib=\"words\")) > 4 else 0\n",
    "def lfdistBtwNeg2(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib=\"words\")) > 8 else 0\n",
    "\n",
    "action_link_words = set(['affect', 'lead', 'led', 'show', 'display', 'exhibit', 'cause', 'result in'])\n",
    "mutant_words = set(['mutant', 'mutation', 'plant', 'line', 'phenotype', 'seedlings', 'variant'])\n",
    "helper_vbs = set(['is', 'was', 'are', 'were', 'become', 'became'])\n",
    "tester_words = set(['sequence', 'published', 'diagram', 'hypothesis', 'hypothesize', 'aim', 'goal', 'understand', 'examine', 'we', 'our', 'experiment', 'test', 'study', 'design', 'analyze', 'analysis', 'results', 'research'])\n",
    "neg_words = set(['strategy', 'public', 'examine', 'measure', 'subject', 'statistic', 'instance'])\n",
    "\n",
    "def lfnegWords(c):\n",
    "    for word in neg_words:\n",
    "        if contains_token(c, word, attrib='lemmas'): return -1\n",
    "    return 0    \n",
    "\n",
    "def resultsAlone(c):\n",
    "    return -1 if (len(c[0].get_attrib_tokens('words')) == 1 and contains_token(c[0], 'result', attrib='lemmas')) or (len(c[1].get_attrib_tokens('lemmas')) == 1 and contains_token(c[1], 'result', attrib='lemmas')) else 0\n",
    "\n",
    "def lfLenCand(c):\n",
    "    return -1 if len(c[0].get_attrib_tokens('words')) == 1 or len(c[1].get_attrib_tokens('words')) == 1 else 0\n",
    "\n",
    "def lf1(c):\n",
    "    return 1 if 'in' in get_between_tokens(c, attrib='words') else 0\n",
    "\n",
    "def lf2(c):\n",
    "    return 1 if len(action_link_words.intersection(set(get_between_tokens(c, attrib='lemmas')))) > 0 else 0\n",
    "\n",
    "def lf2a(c):\n",
    "    for aw in action_link_words:\n",
    "        if contains_token(c[1], aw, attrib='lemmas'): return 1\n",
    "    return 0\n",
    "\n",
    "def lf3(c):\n",
    "    return 1 if contains_token(c[0], 'JJR', attrib='pos_tags') else 0\n",
    "\n",
    "def lf4(c):\n",
    "    return 1 if contains_token(c, r'fold') or contains_token(c, r'\\d+(\\.\\d+_)?%') or contains_token(c, 'percent') else 0\n",
    "\n",
    "def lf5(c):\n",
    "    return 1 if len(mutant_words.intersection(set(get_left_tokens(c, attrib='lemmas')))) > 0 or len(mutant_words.intersection(set(get_right_tokens(c, attrib='lemmas')))) > 0 else 0\n",
    "\n",
    "def lf6(c):\n",
    "    return 1 if len(helper_vbs.intersection(set(get_between_tokens(c, attrib='lemmas', n_max=3)))) > 0 else 0\n",
    "\n",
    "def lf7(c):\n",
    "    return -1 if 'not' in get_between_tokens(c) else 0\n",
    "\n",
    "def lf8(c):\n",
    "    return -1 if 'not' in get_left_tokens(c[0]) or 'not' in get_left_tokens(c[1]) else 0\n",
    "\n",
    "def lf9(c):\n",
    "    return -1 if 'level' in get_left_tokens(c[0], attrib='lemmas', n_max=2) or 'level' in get_right_tokens(c[0], attrib='lemmas', n_max=2) else 0\n",
    "\n",
    "def lf10(c):\n",
    "    return -1 if 'transcript' in get_left_tokens(c[0], attrib='lemmas', n_max=3) or 'transcript' in get_right_tokens(c[0], attrib='lemmas', n_max=2) else 0\n",
    "\n",
    "def lf11(c):\n",
    "    return -1 if not contains_token(c, 'JJR', attrib='pos_tags') and not contains_token(c, 'JJ', attrib='pos_tags') and not contains_token(c[1], 'VBN', attrib='pos_tags') else 0\n",
    "\n",
    "def inverted(c):\n",
    "    return 1 if is_inverted(c) else 0\n",
    "\n",
    "def lf12(c):\n",
    "    return 1 if inverted(c) and lf1(c) else 0\n",
    "\n",
    "def lf13(c):\n",
    "    return 1 if inverted(c) and 'IN' in get_between_tokens(c, attrib='pos_tags', n_max=4) else 0\n",
    "\n",
    "def lf14(c):\n",
    "    return 1 if contains_token(c, 'phenotype', attrib='lemmas') else 0\n",
    "\n",
    "def lf15(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[0], attrib='lemmas') or 'protein' in get_right_tokens(c[0], attrib='lemmas') else 0\n",
    "\n",
    "def lf16(c):\n",
    "    return -1 if 'activity' in get_left_tokens(c[0], attrib='lemmas', n_max=2) or 'level' in get_right_tokens(c[0], attrib='lemmas', n_max=1) else 0\n",
    "\n",
    "def lf17(c):\n",
    "    return -1 if len(tester_words.intersection(set(get_tagged_text(c).split()))) > 0 else 0\n",
    "\n",
    "# def LF_gene_dp(c):\n",
    "#     return 1 if 'gene' in get_left_tokens(c[0], window=2, attrib='lemmas') else 0\n",
    "# def LF_genotype_dp(c):\n",
    "#     return 1 if 'genotype' in get_left_tokens(c[0], window=2, attrib='lemmas') else 0\n",
    "def LF_phenotype_dp(c):\n",
    "    return 1 if 'phenotype' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "def LF_mutation(c):\n",
    "    return 1 if ('mutation' in get_right_tokens(c[0], window=2, attrib='lemmas') or 'mutant' in get_right_tokens(c[0], window=2, attrib='lemmas') or 'mutations' in get_right_tokens(c[0], window=2, attrib='lemmas')) else 0\n",
    "\n",
    "def LF_pheno(c):\n",
    "    return 1 if contains_token(c, 'phenotype', attrib='words') else 0\n",
    "\n",
    "def LF_dev_dp(c):\n",
    "    return -1 if 'development' in get_right_tokens(c[1], window=2, attrib='lemmas')  else 0\n",
    "def LF_protein_dp(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[1], window=2, attrib='lemmas') or 'protein' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "def LF_network_dp(c):\n",
    "    return -1 if 'network' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "def LF_JJ_dp(c):\n",
    "    return -1 if 'JJ' in get_right_tokens(c[1], window=2, attrib='pos_tags') else 0\n",
    "\n",
    "def lf_helpers(c):\n",
    "    return 1 if any(word in get_left_tokens(c, window=2, attrib='words') for word in ['had', 'has', 'was', 'have', 'showed', 'were', 'is', 'are', 'results']) else 0\n",
    "\n",
    "adj_words = set(['increase', 'lower', 'reduce', 'higher', 'less', 'more', 'elevate', 'decrease', 'insensitive', 'absence', 'inhibit', 'double',])\n",
    "def lf_adjwords(c):\n",
    "    for aw in adj_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return 1\n",
    "    return 0\n",
    "    \n",
    "stats_words = set(['statistically', 'significant', 'quantitative', 'real-time', 'generated', 'exposed', 'stratified'])\n",
    "def lf_statswords(c):\n",
    "    for aw in stats_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return -1\n",
    "    return 0\n",
    "   \n",
    "def lf_proteinIn(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[1], attrib='lemmas') or 'protein' in get_right_tokens(c[1], attrib='lemmas') or contains_token(c[1], 'protein', attrib='lemmas') else 0\n",
    "\n",
    "def lf18(c):\n",
    "    return 1 if 'mutant' in get_tagged_text(c).split() or 'mutation' in get_text_splits(c) else 0\n",
    "\n",
    "# def lf19(c):\n",
    "#     lemmas = c[0].get_attrib_tokens('lemmas')\n",
    "#     poses = c[0].get_attrib_tokens('pos_tags')\n",
    "#     for i, w in enumerate(poses):\n",
    "#         if w in ['NN', 'NNS', 'NNP', 'NNPS'] and not re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "#             return -1\n",
    "#     return 0\n",
    "\n",
    "def lf20(c):\n",
    "    lemmas = c[0].get_attrib_tokens('lemmas')\n",
    "    poses = c[0].get_attrib_tokens('pos_tags')\n",
    "    result = 0\n",
    "    for i, w in enumerate(lemmas):\n",
    "        if w in ['NN', 'NNS', 'NNP', 'NNPS'] and not re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "            result = 0\n",
    "        elif re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "            result = -1\n",
    "    return result\n",
    "\n",
    "def lf21(c):\n",
    "    return rule_regex_search_btw_BA(c, '.* in .*', 1)\n",
    "\n",
    "def lf22(c):\n",
    "    return -1 if 'expression' in get_right_tokens(c[0], attrib='lemmas', window=2) or 'expression' in get_left_tokens(c[0], attrib='lemmas', window=2) else 0\n",
    "    \n",
    "def lf23(c):\n",
    "    return -1 if not inverted(c) and len(helper_vbs.intersection(set(get_right_tokens(c[0], window = 1, attrib='lemmas')))) > 0 and ('VBN' == c[0].get_attrib_tokens('pos_tags')[0] or ('VBN' == c[0].get_attrib_tokens('pos_tags')[1] and 'RB' == c[0].get_attrib_tokens('pos_tags')[0])) else 0\n",
    "\n",
    "def lf24(c):\n",
    "    return -1 if not contains_token(c[0], 'VB', attrib='pos_tags') and not contains_token(c[0], 'VBZ', attrib='pos_tags') and not contains_token(c[0], 'VBD', attrib='pos_tags') else 0\n",
    "\n",
    "def lf25(c):\n",
    "    return -1 if 'IN' == c[0].get_attrib_tokens('pos_tags')[0] or 'TO' == c[0].get_attrib_tokens('pos_tags')[0] else 0\n",
    "\n",
    "def lf26(c):\n",
    "    if len(c[0].get_attrib_tokens('pos_tags')) < 2:\n",
    "        return 0\n",
    "    return -1 if 'JJR' == c[0].get_attrib_tokens('pos_tags')[0] and len(set(['NN', 'NNS', 'NNP', 'NNPS']).intersection(set(c[0].get_attrib_tokens('pos_tags')[1]))) == 0 else 0\n",
    "\n",
    "def lf27(c):\n",
    "    hasNoNoun = len(set(['NN', 'NNS', 'NNP', 'NNPS']).intersection(set(c[0].get_attrib_tokens('pos_tags')[0]))) == 0\n",
    "    return -1 if (len(c[0]) < 3 and hasNoNoun) else 0\n",
    "                  \n",
    "def lf28(c):\n",
    "    if len(c[0].get_attrib_tokens('pos_tags')) == 0:\n",
    "        return 0\n",
    "    if len(c[0].get_attrib_tokens('lemmas')) < 2:\n",
    "        return 0\n",
    "    lastWordAdj = True if c[0].get_attrib_tokens('pos_tags')[-1] in set(['JJ', 'JJR']) else False\n",
    "    nextLastVrb = True if c[0].get_attrib_tokens('lemmas')[-2] in helper_vbs else False\n",
    "    return -1 if not nextLastVrb and lastWordAdj else 0              \n",
    "    \n",
    "def lf29(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[0].get_attrib_tokens('pos_tags')[-1] == 'VBG' else 0\n",
    "\n",
    "def lf30(c):                #if ends in prep, its bad\n",
    "    return -1 if c[0].get_attrib_tokens('pos_tags')[-1] == 'IN' else 0\n",
    "\n",
    "def lf29b(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] == 'VBG' else 0\n",
    "\n",
    "def lf30b(c):                #if ends in prep, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] == 'IN' else 0\n",
    "\n",
    "def lf30c(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] in ['JJ', 'JJR', 'JJS'] else 0\n",
    "\n",
    "\n",
    "\n",
    "def lf31(c):\n",
    "    return 1 if len(get_between_tokens(c, attrib='words')) <= 2 else 0\n",
    "\n",
    "def lf32(c):\n",
    "     return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['is', 'are']]) else 0\n",
    "        \n",
    "def lf33(c):\n",
    "    return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['results', 'affected']]) else 0\n",
    "\n",
    "def lf34(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib='words')) >= 5 else 0\n",
    "\n",
    "def lf35(c):\n",
    "    return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['showed', 'were',]]) else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HELPERS\n",
    "def inverted(c):\n",
    "    return 1 if is_inverted(c) else 0\n",
    "\n",
    "#DISTANCE RULES\n",
    "def lfdistBtw0(c):\n",
    "    return 1 if len(get_between_tokens(c, attrib=\"words\")) == 0 else 0\n",
    "def lfdistBtwMax2(c):\n",
    "    return 1 if len(get_between_tokens(c, attrib=\"words\")) < 3 else 0\n",
    "def lfdistBtwMin5(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib=\"words\")) > 4 else 0\n",
    "def lfdistBtwMin8(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib=\"words\")) > 8 else 0\n",
    "def lfdistBtwMax1(c):\n",
    "    return 1 if len(get_between_tokens(c, attrib='words')) < 2 else 0\n",
    "def lfdistBtwMin12(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib='words')) > 11 else 0\n",
    "def lfdistBtwMin14(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib='words')) > 14 else 0\n",
    "\n",
    "def lfLenCand(c):\n",
    "    return -1 if len(c[0].get_attrib_tokens('words')) == 1 else 0\n",
    "\n",
    "#WORD BASED RULES\n",
    "cause_words = set(['affect', 'lead', 'led', 'show', 'display', 'exhibit', 'cause', 'result in'])\n",
    "mutant_words = set(['mutant', 'mutation', 'plant', 'line', 'phenotype', 'seedling', 'variant'])\n",
    "helper_vbs = set(['is', 'was', 'are', 'were', 'become', 'became', 'has', 'had'])\n",
    "tester_words = set(['sequence', 'published', 'diagram', 'hypothesis', 'hypothesize', 'aim', 'goal', 'understand', 'examine', 'we', 'our', 'experiment', 'test', 'study', 'design', 'analyze', 'analysis', 'research'])\n",
    "neg_words = set(['strategy', 'public', 'examine', 'measure', 'subject', 'statistic', 'instance'])\n",
    "adj_words = set(['increase', 'low', 'reduce', 'high', 'less', 'more', 'elevate', 'decrease', 'insensitive', 'absence', 'inhibit', 'double'])   \n",
    "stats_words = set(['statistically', 'quantitative', 'qualitative', 'real-time', 'generate', 'expose', 'stratify'])\n",
    "\n",
    "#WORDS IN CAND RULES\n",
    "def LF_dna(c):\n",
    "    return -1 if contains_token(c, 'DNA', attrib='words') else 0\n",
    "def LF_rna(c):\n",
    "    return -1 if contains_token(c, 'RNA', attrib='words') else 0\n",
    "def LF_snp(c):\n",
    "    return -1 if contains_token(c, 'SNP', attrib='words') else 0\n",
    "\n",
    "def lfwordis_result(c):\n",
    "    return -1 if (len(c[0].get_attrib_tokens('words')) == 1 and contains_token(c[0], 'result', attrib='lemmas')) or (len(c[1].get_attrib_tokens('lemmas')) == 1 and contains_token(c[1], 'result', attrib='lemmas')) else 0\n",
    "\n",
    "def lfwordsin_percent(c):\n",
    "    return 1 if contains_token(c, r'fold') or contains_token(c, r'\\d+(\\.\\d+)?%') or contains_token(c, 'percent') else 0\n",
    "\n",
    "def lfwordsin_phenotype(c):\n",
    "    return 1 if contains_token(c, 'phenotype', attrib='lemmas') else 0\n",
    "\n",
    "def lfwordsin_testerwords(c):\n",
    "    #return -1 if len(tester_words.intersection(set(get_tagged_text(c).split()))) > 0 else 0\n",
    "    return -1 len(tester_words.intersection(set(c.get_parent()._asdict()['text'].split()))) > 0 else 0\n",
    "\n",
    "def lfwordsin_statistically(c):\n",
    "    return 1 if 'statistically' in c.get_parent()._asdict()['text'].split() else 0\n",
    "\n",
    "def lfwordsin_negwords(c):\n",
    "    for word in neg_words:\n",
    "        if contains_token(c, word, attrib='lemmas'): return -1\n",
    "    return 0 \n",
    "def lfwordsin_causewords(c):\n",
    "    for aw in cause_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return 1\n",
    "    return 0\n",
    "def lfwordsin_adjwords(c):\n",
    "    for aw in adj_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return 1\n",
    "    return 0\n",
    "def lfwordsin_statswords(c):\n",
    "    for aw in stats_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return -1\n",
    "    return 0\n",
    "\n",
    "#WORDS IN CONTEXT\n",
    "def lfwordscontext_mutant(c):\n",
    "    return 1 if len(mutant_words.intersection(set(get_left_tokens(c[0], attrib='lemmas')))) > 0 or len(mutant_words.intersection(set(get_right_tokens(c[0], attrib='lemmas')))) > 0 or len(mutant_words.intersection(set(get_left_tokens(c[1], attrib='lemmas')))) > 0 or len(mutant_words.intersection(set(get_right_tokens(c[1], attrib='lemmas')))) > 0 else 0\n",
    "def lfwordsbtwn_mutant(c):\n",
    "    return 1 if len(mutant_words.intersection(set(get_between_tokens(c, attrib='lemmas', n_max=4)))) > 0\n",
    "\n",
    "def LF_variant(c):\n",
    "    return 1 if ('variant' in get_right_tokens(c, attrib='lemmas')) or ('variant' in get_left_tokens(c, attrib='lemmas')) else 0\n",
    "def LF_express(c):\n",
    "    return 1 if ('express' in get_right_tokens(c, attrib='lemmas')) or ('express' in get_left_tokens(c, attrib='lemmas')) else 0  \n",
    "def lfLenCand(c):\n",
    "    return -1 if len(c[0].get_attrib_tokens('words')) == 1 or len(c[1].get_attrib_tokens('words')) == 1 else 0\n",
    "\n",
    "\n",
    "def lfwordscontext_protein_desc(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[0], attrib='lemmas') or 'protein' in get_right_tokens(c[0], attrib='lemmas') else 0\n",
    "def lfwordscontext_protein_ent(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[1], window=2, attrib='lemmas') or 'protein' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "def lfwordsin_protein(c):\n",
    "    return -1 if contains_token(c[1], 'protein', attrib='lemmas') or contains_token(c[0], 'protein', attrib='lemmas') else 0\n",
    "\n",
    "\n",
    "#def lf1(c):\n",
    "    return 1 if 'in' in get_between_tokens(c, attrib='words') else 0\n",
    "#def lf21(c):\n",
    "    return rule_regex_search_btw_BA(c, '.* in .*', 1)\n",
    "\n",
    "def lf2(c):\n",
    "    return 1 if len(action_link_words.intersection(set(get_between_tokens(c, attrib='lemmas')))) > 0 else 0\n",
    "\n",
    "\n",
    "def lf6(c):\n",
    "    return 1 if len(helper_vbs.intersection(set(get_between_tokens(c, attrib='lemmas', n_max=3)))) > 0 else 0\n",
    "\n",
    "#def lf7(c):\n",
    "#    return -1 if 'not' in get_between_tokens(c) else 0\n",
    "\n",
    "#def lf8(c):\n",
    "#    return -1 if 'not' in get_left_tokens(c[0]) or 'not' in get_left_tokens(c[1]) else 0\n",
    "\n",
    "#def lf9(c):\n",
    "#    return -1 if 'level' in get_left_tokens(c[0], attrib='lemmas', n_max=2) or 'level' in get_right_tokens(c[0], attrib='lemmas', n_max=2) else 0\n",
    "\n",
    "#def lf10(c):\n",
    "#    return -1 if 'transcript' in get_left_tokens(c[0], attrib='lemmas', n_max=3) or 'transcript' in get_right_tokens(c[0], attrib='lemmas', n_max=2) else 0\n",
    "\n",
    "def lf12(c):\n",
    "    return 1 if inverted(c) and lf1(c) else 0\n",
    "\n",
    "\n",
    "\n",
    "#def lf16(c):\n",
    "#    return -1 if 'activity' in get_left_tokens(c[0], attrib='lemmas', n_max=2) or 'level' in get_right_tokens(c[0], attrib='lemmas', n_max=1) else 0\n",
    "\n",
    "\n",
    "def LF_phenotype_dp(c):\n",
    "    return 1 if 'phenotype' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "\n",
    "#def LF_dev_dp(c):\n",
    "#    return -1 if 'development' in get_right_tokens(c[1], window=2, attrib='lemmas')  else 0\n",
    "def LF_network_dp(c):\n",
    "    return -1 if 'network' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "\n",
    "def lf_helpers(c):\n",
    "    return 1 if any(word in get_left_tokens(c, window=2, attrib='words') for word in ['had', 'has', 'was', 'have', 'showed', 'were', 'is', 'are', 'results']) else 0\n",
    "\n",
    "def lf22(c):\n",
    "    return -1 if 'expression' in get_right_tokens(c[0], attrib='lemmas', window=2) or 'expression' in get_left_tokens(c[0], attrib='lemmas', window=2) else 0\n",
    "    \n",
    "def lf23(c):\n",
    "    return -1 if not inverted(c) and len(helper_vbs.intersection(set(get_right_tokens(c[0], window = 1, attrib='lemmas')))) > 0 and ('VBN' == c[0].get_attrib_tokens('pos_tags')[0] or ('VBN' == c[0].get_attrib_tokens('pos_tags')[1] and 'RB' == c[0].get_attrib_tokens('pos_tags')[0])) else 0\n",
    "\n",
    "def lf32(c):\n",
    "     return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['is', 'are']]) else 0\n",
    "        \n",
    "def lf33(c):\n",
    "    return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['results', 'affected']]) else 0\n",
    "\n",
    "def lf35(c):\n",
    "    return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['showed', 'were', 'was']]) else 0\n",
    "\n",
    "#POS\n",
    "\n",
    "def LF_LRB_Context(c):\n",
    "    return -1 if '-RRB-' in get_right_tokens(c[0], window=1, attrib='pos_tags') or '-RRB-' in get_right_tokens(c[1], window=1, attrib='pos_tags')else 0\n",
    "def LF_LRB_Contains(c):\n",
    "    return -1 if '-LRB-' == c[0].get_attrib_tokens['pos_tags'][0] or '-LRB-' == c[1].get_attrib_tokens['pos_tags'][0] else 0\n",
    "def LF_RRB(c):\n",
    "    return -1 if '-LRB-' in get_right_tokens(c[0], window=1, attrib='pos_tags') or '-LRB-' in get_right_tokens(c[1], window=1, attrib='pos_tags') else 0\n",
    "def LF_JJR(c)\n",
    "    return 1 if contains_token(c, 'JJR', attrib='pos_tags') else 0\n",
    "\n",
    "def LF_ModPhrase(c):\n",
    "    if is_inverted(c):\n",
    "        if c[1].get_attrib_tokens['lemmas'][0] in helper_vbs and c[1].get_attrib_tokens['pos_tags'][1] in ['JJR', 'VBN', 'JJ', 'RBR', 'RB']:\n",
    "            return 1\n",
    "    else 0\n",
    "\n",
    "def LF_JJ(c):\n",
    "    return 1 if 'JJ' in get_right_tokens(c, attrib='pos_tags') else 0\n",
    "def LF_IN(c):\n",
    "    return 1 if 'IN' in get_right_tokens(c, window=1, attrib='pos_tags') else 0\n",
    "   \n",
    "def LF_NNP(c):\n",
    "    return -1 if contains_token(c, 'NNP', attrib='pos_tags') else 0\n",
    "\n",
    "\n",
    "def lf13(c):\n",
    "    return 1 if inverted(c) and 'IN' in get_between_tokens(c, attrib='pos_tags', n_max=4) else 0\n",
    "\n",
    "def LF_JJ_dp(c):\n",
    "    return -1 if 'JJ' in get_right_tokens(c[1], window=2, attrib='pos_tags') else 0\n",
    "\n",
    "def lf20(c):\n",
    "    lemmas = c[0].get_attrib_tokens('lemmas')\n",
    "    poses = c[0].get_attrib_tokens('pos_tags')\n",
    "    result = 0\n",
    "    for i, w in enumerate(lemmas):\n",
    "        if w in ['NN', 'NNS', 'NNP', 'NNPS'] and not re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "            result = 0\n",
    "        elif re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "            result = -1\n",
    "    return result\n",
    "\n",
    "\n",
    "def lf24(c):\n",
    "    return -1 if not contains_token(c[0], 'VB', attrib='pos_tags') and not contains_token(c[0], 'VBZ', attrib='pos_tags') and not contains_token(c[0], 'VBD', attrib='pos_tags') else 0\n",
    "\n",
    "def lf25(c):\n",
    "    return -1 if 'IN' == c[0].get_attrib_tokens('pos_tags')[0] or 'TO' == c[0].get_attrib_tokens('pos_tags')[0] else 0\n",
    "\n",
    "def lf26(c):\n",
    "    if len(c[0].get_attrib_tokens('pos_tags')) < 2:\n",
    "        return 0\n",
    "    return -1 if 'JJR' == c[0].get_attrib_tokens('pos_tags')[0] and len(set(['NN', 'NNS', 'NNP', 'NNPS']).intersection(set(c[0].get_attrib_tokens('pos_tags')[1]))) == 0 else 0\n",
    "\n",
    "def lf27(c):\n",
    "    hasNoNoun = len(set(['NN', 'NNS', 'NNP', 'NNPS']).intersection(set(c[0].get_attrib_tokens('pos_tags')[0]))) == 0\n",
    "    return -1 if (len(c[0]) < 3 and hasNoNoun) else 0\n",
    "                  \n",
    "def lf28(c):\n",
    "    if len(c[0].get_attrib_tokens('pos_tags')) == 0:\n",
    "        return 0\n",
    "    if len(c[0].get_attrib_tokens('lemmas')) < 2:\n",
    "        return 0\n",
    "    lastWordAdj = True if c[0].get_attrib_tokens('pos_tags')[-1] in set(['JJ', 'JJR']) else False\n",
    "    nextLastVrb = True if c[0].get_attrib_tokens('lemmas')[-2] in helper_vbs else False\n",
    "    return -1 if not nextLastVrb and lastWordAdj else 0              \n",
    "    \n",
    "def lf29(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[0].get_attrib_tokens('pos_tags')[-1] == 'VBG' else 0\n",
    "\n",
    "def lf30(c):                #if ends in prep, its bad\n",
    "    return -1 if c[0].get_attrib_tokens('pos_tags')[-1] == 'IN' else 0\n",
    "\n",
    "def lf29b(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] == 'VBG' else 0\n",
    "\n",
    "def lf30b(c):                #if ends in prep, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] == 'IN' else 0\n",
    "\n",
    "def lf30c(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] in ['JJ', 'JJR', 'JJS'] else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test how to query candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models.context import TemporaryContext\n",
    "import re\n",
    "\n",
    "print docs[15]\n",
    "sent = docs[15].get_parent()\n",
    "print sent\n",
    "text = sent._asdict()['text']\n",
    "splt = text.split()\n",
    "print splt\n",
    "print splt[4:5]\n",
    "print \"\\n\"\n",
    "print \"REGEX VERSION: \"\n",
    "\n",
    "resplit = re.split(' ',text)\n",
    "print resplit\n",
    "print resplit[4:5]\n",
    "print \"\\n\"\n",
    "\n",
    "print docs[15].get_contexts()\n",
    "print (docs[15][0]).get_attrib_tokens('words')\n",
    "print len((docs[15][0]).get_attrib_tokens('words'))\n",
    "# print (docs[15][0]).get_attrib_tokens('dep_parents')\n",
    "\n",
    "#print LF_DP(docs[0])\n",
    "print get_text_splits(docs[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LFs on the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFs = [\n",
    "    LF_mutant,\n",
    "    LF_variant,\n",
    "    LF_express,\n",
    "    LF_JJ,\n",
    "    LF_IN,\n",
    "    LF_dna,\n",
    "    LF_rna,\n",
    "    LF_snp,\n",
    "    LF_protein,\n",
    "    LF_LRB,\n",
    "    LF_RRB,\n",
    "    LF_NNP,\n",
    "    lfdistBtw0,\n",
    "    lfdistBtw,\n",
    "    lfdistBtwNeg,\n",
    "    lfdistBtwNeg2,\n",
    "    lf1,\n",
    "    lf2,\n",
    "    lf3,\n",
    "    lf4,\n",
    "    lf5,\n",
    "    lf6,\n",
    "    lf7,\n",
    "    lf8,\n",
    "    lf9,\n",
    "    lf10,\n",
    "    lf11,\n",
    "    lf2a,\n",
    "    lf12,\n",
    "    lf13,\n",
    "    lf14,\n",
    "    lf15,\n",
    "    lf16,\n",
    "    lf17,\n",
    "    lf18,\n",
    "    lf20,\n",
    "    lf21,\n",
    "    lf22,\n",
    "    lf23,\n",
    "    lf24,\n",
    "    lf25,\n",
    "    lf26,\n",
    "    lf27,\n",
    "    lf29,\n",
    "    lf30,\n",
    "#     lf29b,\n",
    "#     lf30b,\n",
    "#     lf30c,\n",
    "    lf31,\n",
    "    lf32,\n",
    "    lf33,\n",
    "    lf34,\n",
    "    lf35,\n",
    "    LF_phenotype_dp,\n",
    "    LF_mutation,\n",
    "    LF_pheno,\n",
    "    LF_dev_dp,\n",
    "    LF_protein_dp,\n",
    "    LF_network_dp,\n",
    "    LF_JJ_dp,\n",
    "    lf_helpers,\n",
    "    lf_adjwords,\n",
    "    lf_statswords,\n",
    "    lf_proteinIn,\n",
    "    lfnegWords,\n",
    "    resultsAlone,\n",
    "    lfLenCand,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens,\n",
    "    get_between_tokens,\n",
    "    get_right_tokens,\n",
    "    contains_token,\n",
    "    get_text_between,\n",
    "    get_text_splits,\n",
    "    get_tagged_text,\n",
    "    is_inverted,\n",
    "    get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,    \n",
    ")\n",
    "\n",
    "def distance_btwn(c):\n",
    "    span0 = c[0]\n",
    "    span1 = c[1]\n",
    "    indices0 = set(np.arange(span0.get_word_start(), span0.get_word_end() + 1))\n",
    "    indices1 = set(np.arange(span1.get_word_start(), span1.get_word_end() + 1))\n",
    "    if len(indices0.intersection(indices1)) > 0: return 0\n",
    "    if span0.get_word_start() < span1.get_word_start():\n",
    "        return span1.get_word_start() - span0.get_word_end() - 1\n",
    "    else:\n",
    "        left_span = span1\n",
    "        return span0.get_word_start() - span1.get_word_end() - 1\n",
    "    \n",
    "def overlap(c):\n",
    "    span0 = c[0]\n",
    "    span1 = c[1]\n",
    "    indices0 = set(np.arange(span0.get_word_start(), span0.get_word_end() + 1))\n",
    "    indices1 = set(np.arange(span1.get_word_start(), span1.get_word_end() + 1))\n",
    "    if len(indices0.intersection(indices1)) > 0: return 1\n",
    "    return 0\n",
    "\n",
    "#DISTANCE RULES\n",
    "def LF(c):\n",
    "    return 1 if distance_btwn(c) <2 or overlap(c) else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFs = [LF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "import multiprocessing\n",
    "labeler = LabelAnnotator(f=LFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time L_train = labeler.apply(split=3, parallelism=multiprocessing.cpu_count())\n",
    "L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>Coverage</b> is the fraction of candidates that the labeling function emits a non-zero label for.\n",
    "* <b>Overlap</b> is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a non-zero label for.\n",
    "* <b>Conflict</b> is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a conflicting non-zero label for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning.structure import DependencySelector\n",
    "ds = DependencySelector()\n",
    "deps = ds.select(L_train, threshold=0.1)\n",
    "len(deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps # (lf, lf, relationship_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel(lf_propensity=True)\n",
    "gen_model.train(\n",
    "    L_train, deps=deps, epochs=2, decay=0.95, step_size=0.1/L_train.shape[0],\n",
    "    init_acc=2.0, reg_param=0.0,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the generative model to the training candidates to get the noise-aware training label set. We'll refer to these as the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.weights.lf_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import save_marginals\n",
    "save_marginals(session, L_train, train_marginals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START RUNNING HERE TO GET SAVED MARGINALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "train_marginals = load_marginals(session, split=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TARA- DON'T RUN THESE CELLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model to Iterate on Labeling Functions\n",
    "Now that we have learned the generative model, we can stop here and use this to potentially debug and/or improve our labeling function set. First, we apply the LFs to our development set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_dev = labeler.apply_existing(split=4, parallelism=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold_complex', split=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = gen_model.score(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fpsv = SentenceNgramViewer(fp, session, height=400)\n",
    "else:\n",
    "    fpsv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fnsv = SentenceNgramViewer(fn, session, height=400)\n",
    "else:\n",
    "    fnsv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tpsv = SentenceNgramViewer(tp, session, height=400)\n",
    "else:\n",
    "    tpsv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tnsv = SentenceNgramViewer(tn, session, height=400)\n",
    "else:\n",
    "    tnsv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we should be getting an F1 score of around 0.6 to 0.7 on the development set, which is pretty good! However, we should be very careful in interpreting this. Since we developed our labeling functions using this development set as a guide, and our generative model is composed of these labeling functions, we expect it to score very well here!\n",
    "In fact, it is probably somewhat overfit to this set. However this is fine, since in the next tutorial, we'll train a more powerful end extraction model which will generalize beyond the development set, and which we will evaluate on a blind test set (i.e. one we never looked at during development)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Doing Some Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At this point, we might want to look at some examples in one of the error buckets. For example, one of the false negatives that we did not correctly label as true mentions. To do this, we can again just use the Viewer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    sv = SentenceNgramViewer(fp, session, height=400)\n",
    "else:\n",
    "    sv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens,\n",
    "    get_between_tokens,\n",
    "    get_right_tokens,\n",
    "    contains_token,\n",
    "    get_doc_candidate_spans,\n",
    "    get_sent_candidate_spans,\n",
    "    get_text_between,\n",
    "    get_text_splits,\n",
    "    get_tagged_text,\n",
    "    is_inverted,\n",
    "    get_tagged_text,\n",
    ")\n",
    "c = sv.get_selected() if sv else list(fp.union(fn))[0]\n",
    "print(c)\n",
    "print(\"\\n\")\n",
    "\n",
    "c.labels\n",
    "fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically Creating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator\n",
    "import multiprocessing\n",
    "featurizer = FeatureAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time F_train = featurizer.apply(split=3, parallelism=multiprocessing.cpu_count())\n",
    "F_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we apply the feature set we just got from the training set to the dev and test sets by using apply_existing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "F_dev  = featurizer.apply_existing(split=4, parallelism=multiprocessing.cpu_count())\n",
    "F_test = featurizer.apply_existing(split=5, parallelism=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F_train = featurizer.load_matrix(session, split=3)\n",
    "F_dev   = featurizer.load_matrix(session, split=4)\n",
    "F_test  = featurizer.load_matrix(session, split=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Discriminative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the training marginals to train a discriminative model that classifies each Candidate as a true or false mention. We'll use a random hyperparameter search, evaluated on the development set labels, to find the best hyperparameters for our model. To run a hyperparameter search, we need labels for a development set. If they aren't already available, we can manually create labels using the Viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "disc_model = SparseLogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we set up and run the hyperparameter search, training our model with different hyperparamters and picking the best model configuration to keep. We'll set the random seed to maintain reproducibility.\n",
    "Note that we are fitting our model's parameters to the training set generated by our labeling functions, while we are picking hyperparamters with respect to score over the development set labels which we created by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning.utils import MentionScorer\n",
    "from snorkel.learning import RandomSearch, ListParameter, RangeParameter\n",
    "\n",
    "# Searching over learning rate\n",
    "rate_param = RangeParameter('lr', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l1_param  = RangeParameter('l1_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l2_param  = RangeParameter('l2_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "\n",
    "searcher = RandomSearch(session, disc_model, F_train, train_marginals, [rate_param, l1_param, l2_param], n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load in our dev set labels. We will pick the optimal result from the hyperparameter search by testing against these labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold_complex', split=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the hyperparameter search / train the end extraction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1701)\n",
    "searcher.fit(F_dev, L_gold_dev, n_epochs=50, rebalance=0.5, print_freq=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that to train a model without tuning any hyperparameters (at your own risk) just use the train method of the discriminative model. For instance, to train with 20 epochs and a learning rate of 0.001, you could run:\n",
    "disc_model.train(F_train, train_marginals, n_epochs=20, lr=0.001)\n",
    "We can analyze the learned model by examining the weights. For example, we can print out the features with the highest weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, _ = disc_model.get_weights()\n",
    "largest_idxs = reversed(np.argsort(np.abs(w))[-5:])\n",
    "for i in largest_idxs:\n",
    "    print 'Feature: {0: <70}Weight: {1:.6f}'.format(F_train.get_key(session, i).name, w[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this last section of the tutorial, we'll get the score we've been after: the performance of the extraction model on the blind test set (split 2). First, we load the test set labels and gold candidates we made in Part III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold_complex', split=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we score using the discriminative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _ = disc_model.score(session, F_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that if this is the final test set that you will be reporting final numbers on, to avoid biasing results you should not inspect results. However you can run the model on your development set and, as we did in the previous part with the generative labeling function model, inspect examples to do error analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an LSTM extraction model\n",
    "\n",
    "In the intro tutorial, we automatically featurized the candidates and trained a linear model over these features. Here, we'll train a more complicated model for relation extraction: an LSTM network. You can read more about LSTMs here or here. An LSTM is a type of recurrent neural network and automatically generates a numerical representation for the candidate based on the sentence text, so no need for featurizing explicitly as in the intro tutorial. LSTMs take longer to train, and Snorkel doesn't currently support hyperparameter searches for them. We'll train a single model here, but feel free to try out other parameter sets. Just make sure to use the development set - and not the test set - for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_labels = (np.ravel(L_gold_dev.todense()) + 1) / 2\n",
    "print dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.contrib.rnn import reRNN\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':         0.01,\n",
    "    'dim':        100,\n",
    "    'n_epochs':   50,\n",
    "    'dropout':    0.5,\n",
    "    'rebalance':  0.25,\n",
    "    'print_freq': 5\n",
    "}\n",
    "\n",
    "lstm = reRNN(seed=1701, n_threads=None)\n",
    "lstm.train(docs, train_marginals, dev_candidates=dev_docs, dev_labels=dev_labels, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.rnn import reRNN\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':         0.01,\n",
    "    'dim':        100,\n",
    "    'n_epochs':   50,\n",
    "    'dropout':    0.5,\n",
    "    'rebalance':  0.25,\n",
    "    'print_freq': 5\n",
    "}\n",
    "\n",
    "lstm = reRNN(seed=1701, n_threads=None)\n",
    "lstm.train(docs, train_marginals, dev_candidates=dev_docs, dev_labels=dev_labels, **train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to test....but it doesn't really work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold_complex', split=5)\n",
    "\n",
    "test_labels = (np.ravel(L_gold_test.todense()) + 1) / 2\n",
    "print test_labels\n",
    "_, _, _, _ = lstm.score(session, test_docs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
