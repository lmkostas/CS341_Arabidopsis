{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phenotype Candidate Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the first three cells once after restarting the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "# Must set SNORKELDB before importing SnorkelSession\n",
    "from set_env import set_env\n",
    "set_env()\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import TextDocPreprocessor\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For small-data environment, should see 400 documents and 95656 sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Sentences:\", session.query(Sentence).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "PhenoPairComplex = candidate_subclass('ComplexPhenotypes',['descriptor', 'entity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN THIS CELL TO GET ALL DOCS LABELED BY TANYA \n",
    "### we also need to think about how to split for dev and test - not sure if we should do this here or somewhere else, if it is done here we need to update brat import to accomodate for dif splits because right now it only looks for split=0, but the good thing is the application of the labels is agnostic to the split so we basically just have to make sure we do the same process for each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "\n",
    "#load small set of 400 documents\n",
    "with open('small_data/pmcids_400.pkl', 'rb') as f:\n",
    "    sent_dicts = cPickle.load(f)\n",
    "    \n",
    "train_ids, dev_ids, test_ids = set(sent_dicts['train']), set(sent_dicts['dev']), set(sent_dicts['test'])\n",
    "all_ids = train_ids.union(dev_ids).union(test_ids)\n",
    "all_sents = set()\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "#get PMCIDs for BRAT labeled documents\n",
    "tair = []\n",
    "with open('small_data/tair_labels.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        tair.append(line.strip())\n",
    "tair = set(tair)\n",
    "\n",
    "doc_splits = {}\n",
    "for doc_num, doc in enumerate(docs):\n",
    "    name = doc.name.split('-')[0]\n",
    "    if name in tair:  \n",
    "        doc_splits[name] = set()\n",
    "        for s in doc.sentences:\n",
    "            all_sents.add(s)\n",
    "            doc_splits[name].add(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print 'Total Sentences:', len(all_sents)\n",
    "print 'Showing number of sentences per document...'\n",
    "for key in doc_splits.keys():\n",
    "    print key, len(doc_splits[key])\n",
    "#print all_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phenotype extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pheno_candidates import PATO, OBO\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "\n",
    "pheno_ngrams = Ngrams(n_max=15)\n",
    "\n",
    "pheno_extractor_complex = CandidateExtractor(PhenoPairComplex, \n",
    "                                    [pheno_ngrams, pheno_ngrams], [PATO, OBO],\n",
    "                                    nested_relations=True, self_relations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We expect 53846 candidates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"Extracting Candidates...\"\n",
    "#clear dev and test splits\n",
    "pheno_extractor_complex.clear(session, split=4)\n",
    "pheno_extractor_complex.clear(session, split=5)\n",
    "\n",
    "#extract all cands in a single split\n",
    "pheno_extractor_complex.apply(all_sents, split=3)\n",
    "pheno_cands_complex = session.query(PhenoPairComplex).filter(PhenoPairComplex.split==3).all()\n",
    "print \"Number of dev candidates:\", len(pheno_cands_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "svp = SentenceNgramViewer(pheno_cands_complex, session, annotator_name = 'gold_complex', height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
