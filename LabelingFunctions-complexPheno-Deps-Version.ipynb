{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging genes with ddlite: learning and labeling function iteration\n",
    "\n",
    "## Introduction\n",
    "In this example **ddlite** app, we'll build a gene tagger from scratch. Domain-specific tagging systems take months or years to develop. They use hand-crafted model circuitry and accurate, hand-labeled training data. We'll start to build a pretty good one in a few minutes with none of those things. The generalized extraction and learning utilities provided by ddlite will allow us to turn a sampling of article abstracts and some basic domain knowledge into an automated tagging system. Specifically, we want an accurate tagger for genes in academic articles. We have comprehensive dictionaries of genes, but applying a simple matching rule might yield a lot of false positives. For example, \"p53\" might get tagged as a gene if it refers to a page number. Our goal is to use distant supervision to improve precision.\n",
    "\n",
    "Here's the pipeline we'll follow:\n",
    "\n",
    "1. Obtain and parse input data (relevant article abstracts from PubMed)\n",
    "2. Extract candidates for tagging\n",
    "3. Generate features\n",
    "4. Create a test set\n",
    "5. Write labeling functions\n",
    "6. Learn the tagging model\n",
    "7. Iterate on labeling functions\n",
    "\n",
    "Parts 3 through 7 are covered in this notebook. It requires candidates extracted from `GeneTaggerExample_Extraction.ipynb`, which covers parts 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# print(os.environ['SNORKELDB'])\n",
    "# Use production DB\n",
    "from set_env import set_env\n",
    "set_env() \n",
    "sys.path.insert(1, '../snorkel')\n",
    "\n",
    "# Must set SNORKELDB before importing SnorkelSession\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import TextDocPreprocessor\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence, candidate_subclass\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "session = SnorkelSession()\n",
    "\n",
    "#np.random.seed(seed=1701)\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (18,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading candidate extractions\n",
    "First, we'll load in the candidates that we created in the last notebook. We can construct an docs object with the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PhenoPair = candidate_subclass('ComplexPhenotypes', ['descriptor', 'entity'])\n",
    "\n",
    "docs = session.query(PhenoPair).filter(PhenoPair.split == 3).all()  #should edit split to be 1. \n",
    "\n",
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Sentences:\", session.query(Sentence).count()\n",
    "\n",
    "##Once we get all the labels, for loop through all docs and split into train, dev, test. \n",
    "\n",
    "print 'Document set:\\t{0} candidates'.format(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models.context import TemporaryContext\n",
    "import re\n",
    "\n",
    "# print docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @Jeff do not run these next 3 cells until, below you will see two more notes about where to stop and start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens,\n",
    "    get_between_tokens,\n",
    "    get_right_tokens,\n",
    "    contains_token,\n",
    "    get_text_between,\n",
    "    get_text_splits,\n",
    "    get_tagged_text,\n",
    "    is_inverted,\n",
    "    get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "def LF_mutant(c):\n",
    "    return 1 if ('mutant' in get_right_tokens(c, attrib='lemmas')) or ('mutant' in get_left_tokens(c, attrib='lemmas')) else 0\n",
    "def LF_variant(c):\n",
    "    return 1 if ('variant' in get_right_tokens(c, attrib='lemmas')) or ('variant' in get_left_tokens(c, attrib='lemmas')) else 0\n",
    "def LF_express(c):\n",
    "    return 1 if ('express' in get_right_tokens(c, attrib='lemmas')) or ('express' in get_left_tokens(c, attrib='lemmas')) else 0  \n",
    "def LF_JJ(c):\n",
    "    return 1 if 'JJ' in get_right_tokens(c, attrib='pos_tags') else 0\n",
    "def LF_IN(c):\n",
    "    return 1 if 'IN' in get_right_tokens(c, window=1, attrib='pos_tags') else 0\n",
    "def LF_dna(c):\n",
    "    return -1 if contains_token(c, 'DNA', attrib='words') else 0\n",
    "def LF_rna(c):\n",
    "    return -1 if contains_token(c, 'RNA', attrib='words') else 0\n",
    "def LF_snp(c):\n",
    "    return -1 if contains_token(c, 'SNP', attrib='words') else 0\n",
    "def LF_protein(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c, attrib='lemmas') else 0\n",
    "def LF_LRB(c):\n",
    "    return -1 if '-LRB-' in get_right_tokens(c, window=1, attrib='pos_tags') else 0\n",
    "def LF_RRB(c):\n",
    "    return -1 if '-RRB-' in get_right_tokens(c, window=1, attrib='pos_tags') else 0    \n",
    "def LF_NNP(c):\n",
    "    return -1 if contains_token(c, 'NNP', attrib='pos_tags') else 0\n",
    "def lfdistBtw0(c):\n",
    "    return 1 if len(get_between_tokens(c, attrib=\"words\")) == 0 else 0\n",
    "def lfdistBtw(c):\n",
    "    return 1 if len(get_between_tokens(c, attrib=\"words\")) < 3 else 0\n",
    "def lfdistBtwNeg(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib=\"words\")) > 4 else 0\n",
    "def lfdistBtwNeg2(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib=\"words\")) > 8 else 0\n",
    "\n",
    "action_link_words = set(['affect', 'lead', 'led', 'show', 'display', 'exhibit', 'cause', 'result in'])\n",
    "mutant_words = set(['mutant', 'mutation', 'plant', 'line', 'phenotype', 'seedlings', 'variant'])\n",
    "helper_vbs = set(['is', 'was', 'are', 'were', 'become', 'became'])\n",
    "tester_words = set(['sequence', 'published', 'diagram', 'hypothesis', 'hypothesize', 'aim', 'goal', 'understand', 'examine', 'we', 'our', 'experiment', 'test', 'study', 'design', 'analyze', 'analysis', 'results', 'research'])\n",
    "neg_words = set(['strategy', 'public', 'examine', 'measure', 'subject', 'statistic', 'instance'])\n",
    "\n",
    "def lfnegWords(c):\n",
    "    for word in neg_words:\n",
    "        if contains_token(c, word, attrib='lemmas'): return -1\n",
    "    return 0    \n",
    "\n",
    "def resultsAlone(c):\n",
    "    return -1 if (len(c[0].get_attrib_tokens('words')) == 1 and contains_token(c[0], 'result', attrib='lemmas')) or (len(c[1].get_attrib_tokens('lemmas')) == 1 and contains_token(c[1], 'result', attrib='lemmas')) else 0\n",
    "\n",
    "def lfLenCand(c):\n",
    "    return -1 if len(c[0].get_attrib_tokens('words')) == 1 or len(c[1].get_attrib_tokens('words')) == 1 else 0\n",
    "\n",
    "def lf1(c):\n",
    "    return 1 if 'in' in get_between_tokens(c, attrib='words') else 0\n",
    "\n",
    "def lf2(c):\n",
    "    return 1 if len(action_link_words.intersection(set(get_between_tokens(c, attrib='lemmas')))) > 0 else 0\n",
    "\n",
    "def lf2a(c):\n",
    "    for aw in action_link_words:\n",
    "        if contains_token(c[1], aw, attrib='lemmas'): return 1\n",
    "    return 0\n",
    "\n",
    "def lf3(c):\n",
    "    return 1 if contains_token(c[0], 'JJR', attrib='pos_tags') else 0\n",
    "\n",
    "def lf4(c):\n",
    "    return 1 if contains_token(c, r'fold') or contains_token(c, r'\\d+(\\.\\d+_)?%') or contains_token(c, 'percent') else 0\n",
    "\n",
    "def lf5(c):\n",
    "    return 1 if len(mutant_words.intersection(set(get_left_tokens(c, attrib='lemmas')))) > 0 or len(mutant_words.intersection(set(get_right_tokens(c, attrib='lemmas')))) > 0 else 0\n",
    "\n",
    "def lf6(c):\n",
    "    return 1 if len(helper_vbs.intersection(set(get_between_tokens(c, attrib='lemmas', n_max=3)))) > 0 else 0\n",
    "\n",
    "def lf7(c):\n",
    "    return -1 if 'not' in get_between_tokens(c) else 0\n",
    "\n",
    "def lf8(c):\n",
    "    return -1 if 'not' in get_left_tokens(c[0]) or 'not' in get_left_tokens(c[1]) else 0\n",
    "\n",
    "def lf9(c):\n",
    "    return -1 if 'level' in get_left_tokens(c[0], attrib='lemmas', n_max=2) or 'level' in get_right_tokens(c[0], attrib='lemmas', n_max=2) else 0\n",
    "\n",
    "def lf10(c):\n",
    "    return -1 if 'transcript' in get_left_tokens(c[0], attrib='lemmas', n_max=3) or 'transcript' in get_right_tokens(c[0], attrib='lemmas', n_max=2) else 0\n",
    "\n",
    "def lf11(c):\n",
    "    return -1 if not contains_token(c, 'JJR', attrib='pos_tags') and not contains_token(c, 'JJ', attrib='pos_tags') and not contains_token(c[1], 'VBN', attrib='pos_tags') else 0\n",
    "\n",
    "def inverted(c):\n",
    "    return 1 if is_inverted(c) else 0\n",
    "\n",
    "def lf12(c):\n",
    "    return 1 if inverted(c) and lf1(c) else 0\n",
    "\n",
    "def lf13(c):\n",
    "    return 1 if inverted(c) and 'IN' in get_between_tokens(c, attrib='pos_tags', n_max=4) else 0\n",
    "\n",
    "def lf14(c):\n",
    "    return 1 if contains_token(c, 'phenotype', attrib='lemmas') else 0\n",
    "\n",
    "def lf15(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[0], attrib='lemmas') or 'protein' in get_right_tokens(c[0], attrib='lemmas') else 0\n",
    "\n",
    "def lf16(c):\n",
    "    return -1 if 'activity' in get_left_tokens(c[0], attrib='lemmas', n_max=2) or 'level' in get_right_tokens(c[0], attrib='lemmas', n_max=1) else 0\n",
    "\n",
    "def lf17(c):\n",
    "    return -1 if len(tester_words.intersection(set(get_tagged_text(c).split()))) > 0 else 0\n",
    "\n",
    "# def LF_gene_dp(c):\n",
    "#     return 1 if 'gene' in get_left_tokens(c[0], window=2, attrib='lemmas') else 0\n",
    "# def LF_genotype_dp(c):\n",
    "#     return 1 if 'genotype' in get_left_tokens(c[0], window=2, attrib='lemmas') else 0\n",
    "def LF_phenotype_dp(c):\n",
    "    return 1 if 'phenotype' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "def LF_mutation(c):\n",
    "    return 1 if ('mutation' in get_right_tokens(c[0], window=2, attrib='lemmas') or 'mutant' in get_right_tokens(c[0], window=2, attrib='lemmas') or 'mutations' in get_right_tokens(c[0], window=2, attrib='lemmas')) else 0\n",
    "\n",
    "def LF_pheno(c):\n",
    "    return 1 if contains_token(c, 'phenotype', attrib='words') else 0\n",
    "\n",
    "def LF_dev_dp(c):\n",
    "    return -1 if 'development' in get_right_tokens(c[1], window=2, attrib='lemmas')  else 0\n",
    "def LF_protein_dp(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[1], window=2, attrib='lemmas') or 'protein' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "def LF_network_dp(c):\n",
    "    return -1 if 'network' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "def LF_JJ_dp(c):\n",
    "    return -1 if 'JJ' in get_right_tokens(c[1], window=2, attrib='pos_tags') else 0\n",
    "\n",
    "def lf_helpers(c):\n",
    "    return 1 if any(word in get_left_tokens(c, window=2, attrib='words') for word in ['had', 'has', 'was', 'have', 'showed', 'were', 'is', 'are', 'results']) else 0\n",
    "\n",
    "adj_words = set(['increase', 'lower', 'reduce', 'higher', 'less', 'more', 'elevate', 'decrease', 'insensitive', 'absence', 'inhibit', 'double',])\n",
    "def lf_adjwords(c):\n",
    "    for aw in adj_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return 1\n",
    "    return 0\n",
    "    \n",
    "stats_words = set(['statistically', 'significant', 'quantitative', 'real-time', 'generated', 'exposed', 'stratified'])\n",
    "def lf_statswords(c):\n",
    "    for aw in stats_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return -1\n",
    "    return 0\n",
    "   \n",
    "def lf_proteinIn(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[1], attrib='lemmas') or 'protein' in get_right_tokens(c[1], attrib='lemmas') or contains_token(c[1], 'protein', attrib='lemmas') else 0\n",
    "\n",
    "def lf18(c):\n",
    "    return 1 if 'mutant' in get_tagged_text(c).split() or 'mutation' in get_text_splits(c) else 0\n",
    "\n",
    "# def lf19(c):\n",
    "#     lemmas = c[0].get_attrib_tokens('lemmas')\n",
    "#     poses = c[0].get_attrib_tokens('pos_tags')\n",
    "#     for i, w in enumerate(poses):\n",
    "#         if w in ['NN', 'NNS', 'NNP', 'NNPS'] and not re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "#             return -1\n",
    "#     return 0\n",
    "\n",
    "def lf20(c):\n",
    "    lemmas = c[0].get_attrib_tokens('lemmas')\n",
    "    poses = c[0].get_attrib_tokens('pos_tags')\n",
    "    result = 0\n",
    "    for i, w in enumerate(lemmas):\n",
    "        if w in ['NN', 'NNS', 'NNP', 'NNPS'] and not re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "            result = 0\n",
    "        elif re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "            result = -1\n",
    "    return result\n",
    "\n",
    "def lf21(c):\n",
    "    return rule_regex_search_btw_BA(c, '.* in .*', 1)\n",
    "\n",
    "def lf22(c):\n",
    "    return -1 if 'expression' in get_right_tokens(c[0], attrib='lemmas', window=2) or 'expression' in get_left_tokens(c[0], attrib='lemmas', window=2) else 0\n",
    "    \n",
    "def lf23(c):\n",
    "    return -1 if not inverted(c) and len(helper_vbs.intersection(set(get_right_tokens(c[0], window = 1, attrib='lemmas')))) > 0 and ('VBN' == c[0].get_attrib_tokens('pos_tags')[0] or ('VBN' == c[0].get_attrib_tokens('pos_tags')[1] and 'RB' == c[0].get_attrib_tokens('pos_tags')[0])) else 0\n",
    "\n",
    "def lf24(c):\n",
    "    return -1 if not contains_token(c[0], 'VB', attrib='pos_tags') and not contains_token(c[0], 'VBZ', attrib='pos_tags') and not contains_token(c[0], 'VBD', attrib='pos_tags') else 0\n",
    "\n",
    "def lf25(c):\n",
    "    return -1 if 'IN' == c[0].get_attrib_tokens('pos_tags')[0] or 'TO' == c[0].get_attrib_tokens('pos_tags')[0] else 0\n",
    "\n",
    "def lf26(c):\n",
    "    if len(c[0].get_attrib_tokens('pos_tags')) < 2:\n",
    "        return 0\n",
    "    return -1 if 'JJR' == c[0].get_attrib_tokens('pos_tags')[0] and len(set(['NN', 'NNS', 'NNP', 'NNPS']).intersection(set(c[0].get_attrib_tokens('pos_tags')[1]))) == 0 else 0\n",
    "\n",
    "def lf27(c):\n",
    "    hasNoNoun = len(set(['NN', 'NNS', 'NNP', 'NNPS']).intersection(set(c[0].get_attrib_tokens('pos_tags')[0]))) == 0\n",
    "    return -1 if (len(c[0]) < 3 and hasNoNoun) else 0\n",
    "                  \n",
    "def lf28(c):\n",
    "    if len(c[0].get_attrib_tokens('pos_tags')) == 0:\n",
    "        return 0\n",
    "    if len(c[0].get_attrib_tokens('lemmas')) < 2:\n",
    "        return 0\n",
    "    lastWordAdj = True if c[0].get_attrib_tokens('pos_tags')[-1] in set(['JJ', 'JJR']) else False\n",
    "    nextLastVrb = True if c[0].get_attrib_tokens('lemmas')[-2] in helper_vbs else False\n",
    "    return -1 if not nextLastVrb and lastWordAdj else 0              \n",
    "    \n",
    "def lf29(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[0].get_attrib_tokens('pos_tags')[-1] == 'VBG' else 0\n",
    "\n",
    "def lf30(c):                #if ends in prep, its bad\n",
    "    return -1 if c[0].get_attrib_tokens('pos_tags')[-1] == 'IN' else 0\n",
    "\n",
    "def lf29b(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] == 'VBG' else 0\n",
    "\n",
    "def lf30b(c):                #if ends in prep, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] == 'IN' else 0\n",
    "\n",
    "def lf30c(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] in ['JJ', 'JJR', 'JJS'] else 0\n",
    "\n",
    "\n",
    "\n",
    "def lf31(c):\n",
    "    return 1 if len(get_between_tokens(c, attrib='words')) <= 2 else 0\n",
    "\n",
    "def lf32(c):\n",
    "     return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['is', 'are']]) else 0\n",
    "        \n",
    "def lf33(c):\n",
    "    return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['results', 'affected']]) else 0\n",
    "\n",
    "def lf34(c):\n",
    "    return -1 if len(get_between_tokens(c, attrib='words')) >= 5 else 0\n",
    "\n",
    "def lf35(c):\n",
    "    return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['showed', 'were',]]) else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import util\n",
    "import re\n",
    "from obo_parser import parseGOOBO\n",
    "if 'CI' not in os.environ:\n",
    "    try:\n",
    "        from nltk.stem.porter import PorterStemmer\n",
    "    except ImportError:\n",
    "        warnings.warn(\"nltk not installed- some default functionality may be absent.\")\n",
    "        \n",
    "        \n",
    "def stem(w):\n",
    "        \"\"\"Apply stemmer, handling encoding errors\"\"\"\n",
    "        stemmer = PorterStemmer()\n",
    "        try:\n",
    "            return stemmer.stem(w)\n",
    "        except UnicodeDecodeError:\n",
    "            return w\n",
    "\n",
    "BLACKLIST = \"dicts/blacklist_words.txt\"\n",
    "\n",
    "PHENO_LIST = \"dicts/list_phenotypes_arabidopsis_filtered.txt\"\n",
    "PHENO_EQ_LIST = \"dicts/phenotypes_all_eq_dict.txt\"\n",
    "PHENO_MANUAL = \"dicts/phenotypes_manual.txt\"\n",
    "ONTOLOGIES = [\"dicts/po.obo\", \"dicts/chebi.obo\", \"dicts/go-basic.obo\"]\n",
    "PATO_ONTOLOGY = \"dicts/pato.obo\"\n",
    "\n",
    "\n",
    "def read_blacklist():\n",
    "    \"\"\"\n",
    "    Read the blacklist words from disk.\n",
    "    \"\"\"\n",
    "    with open(BLACKLIST) as blacklist:\n",
    "        # Blacklist words are in the second column, starting on the second row.\n",
    "        return [line.rstrip().split('\\t')[1].lower() for line in blacklist][1:]\n",
    "\n",
    "def load_pheno_list():\n",
    "    \"\"\"\n",
    "    Loads a list of phenotypes from multiple files.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    result.extend(util.read_file_lines(PHENO_LIST))\n",
    "    split = []\n",
    "    for r in result:\n",
    "        split.extend(r.split(r'[\\.,;]'))\n",
    "    result.extend(split)\n",
    "    result.extend(util.read_tsv_flat(PHENO_EQ_LIST, delimiter=\";\"))\n",
    "    result.extend(util.read_file_lines(PHENO_MANUAL))\n",
    "\n",
    "    # Filter by blacklist\n",
    "    blacklist = read_blacklist()\n",
    "    #blacklist = [stem_word(b) for b in blacklist]\n",
    "\n",
    "    return [pheno.lower() for pheno in result if pheno.lower() not in blacklist and len(pheno)>1]\n",
    "\n",
    "def load_pheno_ontology():\n",
    "    \"\"\"\n",
    "    Load chebi, pato, and go ontologies.\n",
    "    \"\"\"\n",
    "    ontology_terms = []\n",
    "    for ontology_file in ONTOLOGIES:\n",
    "        ontology_terms.extend(parse_ontology(ontology_file))\n",
    "\n",
    "    #blacklist = read_blacklist()\n",
    "    #return [pheno.lower() for pheno in ontology_terms if pheno.lower() not in blacklist and len(pheno)>1]\n",
    "\n",
    "    return ontology_terms\n",
    "\n",
    "\n",
    "def parse_ontology(ontology_file):\n",
    "    terms = []\n",
    "    for elt in parseGOOBO(ontology_file):\n",
    "        terms.append(elt[\"name\"])\n",
    "        if 'synonym' in elt:\n",
    "            if isinstance(elt['synonym'], list):\n",
    "                for syn in elt['synonym']:\n",
    "                    try:\n",
    "                        terms.append(syn.split('\"')[1])\n",
    "                    except:\n",
    "                        print 'error parsing ontology synonym'\n",
    "            else:\n",
    "                try:\n",
    "                    terms.append(elt['synonym'].split('\"')[1])\n",
    "                except:\n",
    "                    print 'error parsing ontology synonym non list'\n",
    "    blacklist = read_blacklist()\n",
    "    #blacklist = [stem_word(b) for b in blacklist]\n",
    "\n",
    "    return [stem(pheno.lower()) else pheno.lower() if len(pheno.split()) == 1 for pheno in terms if pheno.lower() not in blacklist and len(pheno)>1]\n",
    "\n",
    "    return terms\n",
    "\n",
    "def parse_pato(ontology_file):\n",
    "    terms = []\n",
    "    for elt in parseGOOBO(ontology_file):\n",
    "        terms.append(elt[\"name\"])\n",
    "        if 'synonym' in elt:\n",
    "            if isinstance(elt['synonym'], list):\n",
    "                for syn in elt['synonym']:\n",
    "                    try:\n",
    "                        terms.append(syn.split('\"')[1])\n",
    "                    except:\n",
    "                        print 'error parsing ontology synonym'\n",
    "            else:\n",
    "                try:\n",
    "                    terms.append(elt['synonym'].split('\"')[1])\n",
    "                except:\n",
    "                    print 'error parsing ontology synonym non list'\n",
    "\n",
    "    blacklist = read_blacklist()\n",
    "    blacklist.extend([stem(b) for b in blacklist])\n",
    "    #blacklist = [stem_word(b) for b in blacklist]\n",
    "    return [stem(pheno.lower()) else pheno.lower() if len(pheno.split()) == 1 for pheno in terms if pheno.lower() not in blacklist and len(pheno)>1]\n",
    "\n",
    "    return terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_phenos = laod_pheno_list()\n",
    "blacklist = read_blacklist()\n",
    "blacklist.extend([stem(b) for b in blacklist])\n",
    "obos = load_pheno_ontology() + ['stem', 'leaves', 'phenotype', 'carpel', 'tip', 'type' ]\n",
    "patos = parse_pato(PATO_ONTOLOGY) + ['alter', 'growth', 'develop', 'affect', 'display', 'twice', 'inhibit', 'type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @Jeff - run this cell and the next to ge the LFs - do not run any other cell until the cell with the header \"Start Gen Model Here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens,\n",
    "    get_between_tokens,\n",
    "    get_right_tokens,\n",
    "    contains_token,\n",
    "    get_text_between,\n",
    "    get_text_splits,\n",
    "    get_tagged_text,\n",
    "    is_inverted,\n",
    "    get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,    \n",
    ")\n",
    "\n",
    "#DICTIONARIES\n",
    "cause_words = set(['affect', 'lead', 'led', 'show', 'display', 'exhibit', 'cause', 'result in'])\n",
    "mutant_words = set(['mutant', 'mutation', 'plant', 'line', 'phenotype', 'seedling', 'variant'])\n",
    "helper_vbs = set(['is', 'was', 'are', 'were', 'become', 'became', 'has', 'had'])\n",
    "tester_words = set(['sequence', 'published', 'diagram', 'hypothesis', 'hypothesize', 'aim', 'goal', 'understand', 'examine', 'we', 'our', 'experiment', 'test', 'study', 'design', 'analyze', 'analysis', 'research'])\n",
    "neg_words = set(['strategy', 'public', 'examine', 'measure', 'subject', 'statistic', 'instance'])\n",
    "adj_words = set(['increase', 'low', 'reduce', 'high', 'less', 'more', 'elevate', 'decrease', 'insensitive', 'absence', 'inhibit', 'double'])   \n",
    "stats_words = set(['statistically', 'quantitative', 'qualitative', 'real-time', 'generate', 'expose', 'stratify'])\n",
    "cc_words = set(['while', 'but', 'however', 'whereas'])\n",
    "comp_words = set(['compare', 'relative', 'than', 'same', 'different', 'relatively', 'contrast', 'similar'])\n",
    "\n",
    "#HELPERS\n",
    "def inverted(c):\n",
    "    return 1 if is_inverted(c) else 0\n",
    "\n",
    "def distance_btwn(c):\n",
    "    span0 = c[0]\n",
    "    span1 = c[1]\n",
    "    indices0 = set(np.arange(span0.get_word_start(), span0.get_word_end() + 1))\n",
    "    indices1 = set(np.arange(span1.get_word_start(), span1.get_word_end() + 1))\n",
    "    if len(indices0.intersection(indices1)) > 0: return 0\n",
    "    if span0.get_word_start() < span1.get_word_start():\n",
    "        return span1.get_word_start() - span0.get_word_end() - 1\n",
    "    else:\n",
    "        left_span = span1\n",
    "        return span0.get_word_start() - span1.get_word_end() - 1\n",
    "    \n",
    "def overlap(c):\n",
    "    span0 = c[0]\n",
    "    span1 = c[1]\n",
    "    indices0 = set(np.arange(span0.get_word_start(), span0.get_word_end() + 1))\n",
    "    indices1 = set(np.arange(span1.get_word_start(), span1.get_word_end() + 1))\n",
    "    if len(indices0.intersection(indices1)) > 0: return 1\n",
    "    return 0\n",
    "\n",
    "def ends_in(ci, val, attrib):\n",
    "    return val == ci.get_attrib_tokens(attrib)[-1]\n",
    "    \n",
    "def starts_with(ci, val, attrib):\n",
    "    return val == ci.get_attrib_tokens(attrib)[0]\n",
    "\n",
    "\n",
    "#DISTANCE RULES\n",
    "def lfdistBtw0(c):\n",
    "    return 1 if distance_btwn(c) == 0 else 0\n",
    "def lfdistBtwMax1(c):\n",
    "    return 1 if distance_btwn(c) < 2 else 0\n",
    "def lfdistBtwMax2(c):\n",
    "    return 1 if distance_btwn(c) < 3 else 0\n",
    "def lfdistBtwnOverlap(c):\n",
    "    return overlap(c)\n",
    "\n",
    "def lfdistBtwMin5(c):\n",
    "    return -1 if distance_btwn(c) > 4 else 0\n",
    "def lfdistBtwMin8(c):\n",
    "    return -1 if distance_btwn(c) > 8 else 0\n",
    "def lfdistBtwMin12(c):\n",
    "    return -1 if distance_btwn(c) > 11 else 0\n",
    "def lfdistBtwMin14(c):\n",
    "    return -1 if distance_btwn(c) > 14 else 0\n",
    "\n",
    "#LENGTH RULES\n",
    "def lfLenCand(c):\n",
    "    return -1 if len(c[0].get_attrib_tokens('words')) == 1 else 0\n",
    "\n",
    "#PAIRWISE RULES\n",
    "def lfend_prep(c):\n",
    "    if not overlap(c):\n",
    "        left = c[0] if c[0].get_word_start() < c[1].get_word_start() else c[1]\n",
    "        right = c[1] if c[0].get_word_start() < c[1].get_word_start() else c[0]\n",
    "        if left.get_attrib_tokens('pos_tags')[-1] == 'IN' and len(set(right.get_attrib_tokens('pos_tags')[:2]).intersection(set(['NN', 'NNS', 'NNP', 'NNPS', 'DT'])))==0:\n",
    "            return -1 \n",
    "    return 0\n",
    "    \n",
    "def lfend_det(c):\n",
    "    if not overlap(c):\n",
    "        left = c[0] if c[0].get_word_start() < c[1].get_word_start() else c[1]\n",
    "        right = c[1] if c[0].get_word_start() < c[1].get_word_start() else c[0]\n",
    "        if left.get_attrib_tokens('pos_tags')[-1] == 'DT' and right.get_attrib_tokens('pos_tags')[0] not in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            return -1\n",
    "    return 0\n",
    "\n",
    "  \n",
    "def lfend_adj(c):\n",
    "    left = c[0] if c[0].get_word_start() < c[1].get_word_start() else c[1]\n",
    "    right = c[1] if c[0].get_word_start() < c[1].get_word_start() else c[0]\n",
    "    if left.get_attrib_tokens('pos_tags')[-1] in ['JJ', 'JJR', 'VBN'] and len(set(get_right_tokens(left, attrib='pos_tags', n_max=2)).intersection(set(['NN', 'NNS', 'NNP', 'NNPS'])))==0:\n",
    "        return -1\n",
    "    return 0\n",
    "        \n",
    "#CONTAINS RULES\n",
    "def lfin_fig(c):\n",
    "    return -1 if contains_token(c[0], 'Fig', attrib='words') \\\n",
    "    or contains_token(c[0], 'FIG', attrib='words') \\\n",
    "    or contains_token(c[0], 'fig', attrib='words') \\\n",
    "    or contains_token(c[0], 'Fig', attrib='words') \\\n",
    "    or contains_token(c[0], 'FIG.', attrib='words') \\\n",
    "    or contains_token(c[0], 'fig.', attrib='words') \\\n",
    "    or contains_token(c[0], 'Fig.', attrib='words') else 0\n",
    "\n",
    "def lfin_num(c):\n",
    "    return 1 if contains_token(c[0], 'CD', attrib='pos_tags') or contains_token(c[1], 'CD', attrib='pos_tags') else 0\n",
    "\n",
    "def lfin_equals(c):\n",
    "    return -1 if contains_token(c[0], '=', attrib='words') or contains_token(c[1], '=', attrib='words') else 0\n",
    "\n",
    "def lfin_comp_adj(c):\n",
    "    return 1 if contains_token(c[0], 'JJR', attrib='pos_tags') or contains_token(c[1], 'JJR', attrib='pos_tags') else 0\n",
    "    \n",
    "def lfin_comp_advb(c):\n",
    "    return 1 if contains_token(c[0], 'RBR', attrib='pos_tags') or contains_token(c[1], 'RBR', attrib='pos_tags') else 0\n",
    "\n",
    "#BETWEEN RULES\n",
    "def lfbtwn_is(c):\n",
    "    return 1 if len(helper_vbs.intersection(set(get_between_tokens(c, attrib='lemmas', n_max=4)))) > 0 else 0\n",
    "    return 1 if 1 == distance_btwn(c) and len(helper_vbs.intersection(set(get_between_tokens(c, attrib='lemmas')))) > 0 else 0\n",
    "                                             \n",
    "def lfbtwm_comma(c):\n",
    "    return -1 if -1 == rule_regex_search_btw_BA(c, '*[,;]*', -1) or -1 == rule_regex_search_btw_AB(c, '*[,;]*', -1) else 0\n",
    "                                             \n",
    "def lfbtwn_parenthesis(c):\n",
    "    return -1 if re.search(r'\\([^\\)]*{{A}}.*\\).*{{B}}', get_tagged_text(c), flags=re.I) \\\n",
    "    or re.search(r'\\([^\\)]*{{B}}.*\\).*{{A}}', get_tagged_text(c), flags=re.I) \\\n",
    "    or re.search(r'\\{{A}}.*\\([^\\)]*{{B}}.*\\)', get_tagged_text(c), flags=re.I) \\\n",
    "    or re.search(r'\\{{B}}.*\\([^\\)]*{{A}}.*\\)', get_tagged_text(c), flags=re.I) else 0\n",
    "    \n",
    "                                             \n",
    "#WORD BASED RULES\n",
    "\n",
    "\n",
    "#WORDS IN CAND RULES\n",
    "def LF_dna(c):\n",
    "    return -1 if contains_token(c, 'DNA', attrib='words') else 0\n",
    "def LF_rna(c):\n",
    "    return -1 if contains_token(c, 'RNA', attrib='words') else 0\n",
    "def LF_snp(c):\n",
    "    return -1 if contains_token(c, 'SNP', attrib='words') else 0\n",
    "\n",
    "def lfwordis_result(c):\n",
    "    return -1 if (len(c[0].get_attrib_tokens('words')) == 1 and contains_token(c[0], 'result', attrib='lemmas')) or (len(c[1].get_attrib_tokens('lemmas')) == 1 and contains_token(c[1], 'result', attrib='lemmas')) else 0\n",
    "\n",
    "def lfwordsin_percent(c):\n",
    "    return 1 if contains_token(c, r'fold') or contains_token(c, r'\\d+(\\.\\d+)?%') or contains_token(c, 'percent') else 0\n",
    "\n",
    "def lfwordsin_phenotype(c):\n",
    "    return 1 if contains_token(c, 'phenotype', attrib='lemmas') else 0\n",
    "\n",
    "def lfwordsin_testerwords(c):\n",
    "    #return -1 if len(tester_words.intersection(set(get_tagged_text(c).split()))) > 0 else 0\n",
    "    return -1 if len(tester_words.intersection(set(c.get_parent()._asdict()['text'].split()))) > 0 else 0\n",
    "\n",
    "#def lfwordsin_statistically(c):\n",
    "#    return 1 if 'statistically' in c.get_parent()._asdict()['text'].split() else 0\n",
    "\n",
    "def lfwordsin_compwords(c):\n",
    "    for word in comp_words:\n",
    "        if contains_token(c, word, attrib='lemmas'): return 1\n",
    "    return 0 \n",
    "\n",
    "def lfwordsin_negwords(c):\n",
    "    for word in neg_words:\n",
    "        if contains_token(c, word, attrib='lemmas'): return -1\n",
    "    return 0 \n",
    "def lfwordsin_causewords(c):\n",
    "    for aw in cause_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return 1\n",
    "    return 0\n",
    "def lfwordsin_adjwords(c):\n",
    "    for aw in adj_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return 1\n",
    "    return 0\n",
    "def lfwordsin_statswords(c):\n",
    "    for aw in stats_words:\n",
    "        if contains_token(c, aw, attrib='lemmas'): return -1\n",
    "    return 0\n",
    "\n",
    "#WORDS IN CONTEXT\n",
    "def lfwordscontext_mutant(c):\n",
    "    return 1 if len(mutant_words.intersection(set(get_left_tokens(c[0], attrib='lemmas')))) > 0 or len(mutant_words.intersection(set(get_right_tokens(c[0], attrib='lemmas')))) > 0 or len(mutant_words.intersection(set(get_left_tokens(c[1], attrib='lemmas')))) > 0 or len(mutant_words.intersection(set(get_right_tokens(c[1], attrib='lemmas')))) > 0 else 0\n",
    "def lfwordsbtwn_mutant(c):\n",
    "    return 1 if len(mutant_words.intersection(set(get_between_tokens(c, attrib='lemmas', n_max=4)))) > 0 else 0\n",
    "\n",
    "def LF_variant(c):\n",
    "    return 1 if ('variant' in get_right_tokens(c, attrib='lemmas')) or ('variant' in get_left_tokens(c, attrib='lemmas')) else 0\n",
    "def LF_express(c):\n",
    "    return 1 if ('express' in get_right_tokens(c, attrib='lemmas')) or ('express' in get_left_tokens(c, attrib='lemmas')) else 0  \n",
    "#def lfLenCand(c):\n",
    "#    return -1 if len(c[0].get_attrib_tokens('words')) == 1 or len(c[1].get_attrib_tokens('words')) == 1 else 0\n",
    "\n",
    "\n",
    "def lfwordscontext_protein_desc(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[0], attrib='lemmas') or 'protein' in get_right_tokens(c[0], attrib='lemmas') else 0\n",
    "def lfwordscontext_protein_ent(c):\n",
    "    return -1 if 'protein' in get_left_tokens(c[1], window=2, attrib='lemmas') or 'protein' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "def lfwordsin_protein(c):\n",
    "    return -1 if contains_token(c[1], 'protein', attrib='lemmas') or contains_token(c[0], 'protein', attrib='lemmas') else 0\n",
    "\n",
    "\n",
    "#def lf1(c):\n",
    "    #return 1 if 'in' in get_between_tokens(c, attrib='words') else 0\n",
    "#def lf21(c):\n",
    "    #return rule_regex_search_btw_BA(c, '.* in .*', 1)\n",
    "\n",
    "def lf2(c):\n",
    "    return 1 if len(cause_words.intersection(set(get_between_tokens(c, attrib='lemmas')))) > 0 else 0\n",
    "\n",
    "\n",
    "def lf6(c):\n",
    "    return 1 if len(helper_vbs.intersection(set(get_between_tokens(c, attrib='lemmas', n_max=3)))) > 0 else 0\n",
    "\n",
    "#def lf7(c):\n",
    "#    return -1 if 'not' in get_between_tokens(c) else 0\n",
    "\n",
    "#def lf8(c):\n",
    "#    return -1 if 'not' in get_left_tokens(c[0]) or 'not' in get_left_tokens(c[1]) else 0\n",
    "\n",
    "#def lf9(c):\n",
    "#    return -1 if 'level' in get_left_tokens(c[0], attrib='lemmas', n_max=2) or 'level' in get_right_tokens(c[0], attrib='lemmas', n_max=2) else 0\n",
    "\n",
    "#def lf10(c):\n",
    "#    return -1 if 'transcript' in get_left_tokens(c[0], attrib='lemmas', n_max=3) or 'transcript' in get_right_tokens(c[0], attrib='lemmas', n_max=2) else 0\n",
    "\n",
    "#def lf12(c):\n",
    "#    return 1 if inverted(c) and lf1(c) else 0\n",
    "\n",
    "\n",
    "\n",
    "#def lf16(c):\n",
    "#    return -1 if 'activity' in get_left_tokens(c[0], attrib='lemmas', n_max=2) or 'level' in get_right_tokens(c[0], attrib='lemmas', n_max=1) else 0\n",
    "\n",
    "\n",
    "def LF_phenotype_dp(c):\n",
    "    return 1 if 'phenotype' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "\n",
    "#def LF_dev_dp(c):\n",
    "#    return -1 if 'development' in get_right_tokens(c[1], window=2, attrib='lemmas')  else 0\n",
    "#def LF_network_dp(c):\n",
    "#    return -1 if 'network' in get_right_tokens(c[1], window=2, attrib='lemmas') else 0\n",
    "\n",
    "def lf_helpers(c):\n",
    "    return 1 if any(word in get_left_tokens(c, window=2, attrib='words') for word in ['had', 'has', 'was', 'have', 'showed', 'were', 'is', 'are', 'results']) else 0\n",
    "\n",
    "#def lf22(c):\n",
    "#    return -1 if 'expression' in get_right_tokens(c[0], attrib='lemmas', window=2) or 'expression' in get_left_tokens(c[0], attrib='lemmas', window=2) else 0\n",
    "    \n",
    "def lf23(c):\n",
    "    return -1 if not inverted(c) and len(helper_vbs.intersection(set(get_right_tokens(c[0], window = 1, attrib='lemmas')))) > 0 and ('VBN' == c[0].get_attrib_tokens('pos_tags')[0] or ('VBN' == c[0].get_attrib_tokens('pos_tags')[1] and 'RB' == c[0].get_attrib_tokens('pos_tags')[0])) else 0\n",
    "\n",
    "def lf32(c):\n",
    "     return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['is', 'are']]) else 0\n",
    "        \n",
    "def lf33(c):\n",
    "    return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['results', 'affected']]) else 0\n",
    "\n",
    "def lf35(c):\n",
    "    return 1 if any([word in get_left_tokens(c, window=4, attrib='words') for word in ['showed', 'were', 'was']]) else 0\n",
    "\n",
    "#POS\n",
    "\n",
    "def LF_LRB_Context(c):\n",
    "    return -1 if '-RRB-' in get_right_tokens(c[0], window=1, attrib='pos_tags') or '-RRB-' in get_right_tokens(c[1], window=1, attrib='pos_tags')else 0\n",
    "def LF_LRB_Contains(c):\n",
    "    return -1 if '-LRB-' == c[0].get_attrib_tokens('pos_tags')[0] or '-LRB-' == c[1].get_attrib_tokens('pos_tags')[0] else 0\n",
    "def LF_RRB(c):\n",
    "    return -1 if '-LRB-' in get_right_tokens(c[0], window=1, attrib='pos_tags') or '-LRB-' in get_right_tokens(c[1], window=1, attrib='pos_tags') else 0\n",
    "def LF_JJR(c):\n",
    "    return 1 if contains_token(c, 'JJR', attrib='pos_tags') else 0\n",
    "\n",
    "def LF_ModPhrase(c):\n",
    "    if is_inverted(c):\n",
    "        if c[1].get_attrib_tokens('lemmas')[0] in helper_vbs and c[1].get_attrib_tokens('pos_tags')[1] in ['JJR', 'VBN', 'JJ', 'RBR', 'RB']:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def LF_JJ(c):\n",
    "    return 1 if 'JJ' in get_right_tokens(c, attrib='pos_tags') else 0\n",
    "def LF_IN(c):\n",
    "    return 1 if 'IN' in get_right_tokens(c, window=1, attrib='pos_tags') else 0\n",
    "   \n",
    "def LF_NNP(c):\n",
    "    return -1 if contains_token(c, 'NNP', attrib='pos_tags') else 0\n",
    "\n",
    "\n",
    "def lf13(c):\n",
    "    return 1 if inverted(c) and 'IN' in get_between_tokens(c, attrib='pos_tags', n_max=4) else 0\n",
    "\n",
    "def LF_JJ_dp(c):\n",
    "    return -1 if 'JJ' in get_right_tokens(c[1], window=2, attrib='pos_tags') else 0\n",
    "\n",
    "# def lf20(c):\n",
    "#     lemmas = c[0].get_attrib_tokens('lemmas')\n",
    "#     poses = c[0].get_attrib_tokens('pos_tags')\n",
    "#     result = 0\n",
    "#     for i, w in enumerate(lemmas):\n",
    "#         if w in ['NN', 'NNS', 'NNP', 'NNPS'] and not re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "#             result = 0\n",
    "#         elif re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "#             result = -1\n",
    "#     return result\n",
    "\n",
    "def lf20(c):\n",
    "    lemmas = c[0].get_attrib_tokens('lemmas')\n",
    "    poses = c[0].get_attrib_tokens('pos_tags')\n",
    "    result = 0\n",
    "    for i, w in enumerate(lemmas):\n",
    "        if re.match(r'\\w+(ion|ment|vity)', lemmas[i]):\n",
    "            return 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def lf24(c):\n",
    "    return -1 if not contains_token(c[0], 'VB', attrib='pos_tags') and not contains_token(c[0], 'VBZ', attrib='pos_tags') and not contains_token(c[0], 'VBD', attrib='pos_tags') else 0\n",
    "\n",
    "def lf25(c):\n",
    "    return -1 if 'IN' == c[0].get_attrib_tokens('pos_tags')[0] or 'TO' == c[0].get_attrib_tokens('pos_tags')[0] else 0\n",
    "\n",
    "def lf26(c):\n",
    "    if len(c[0].get_attrib_tokens('pos_tags')) < 2:\n",
    "        return 0\n",
    "    return -1 if 'JJR' == c[0].get_attrib_tokens('pos_tags')[0] and len(set(['NN', 'NNS', 'NNP', 'NNPS']).intersection(set(c[0].get_attrib_tokens('pos_tags')[1]))) == 0 else 0\n",
    "\n",
    "def lfnonoun(c):\n",
    "    return -1 if len(set(['NN', 'NNS', 'NNP', 'NNPS']).intersection(set(c[0].get_attrib_tokens('pos_tags')+c[1].get_attrib_tokens('pos_tags')))) == 0 else 0\n",
    "    return -1 if (len(c[0]) < 3 and hasNoNoun) else 0\n",
    "                  \n",
    "def lf28(c):\n",
    "    if len(c[0].get_attrib_tokens('pos_tags')) == 0:\n",
    "        return 0\n",
    "    if len(c[0].get_attrib_tokens('lemmas')) < 2:\n",
    "        return 0\n",
    "    lastWordAdj = True if c[0].get_attrib_tokens('pos_tags')[-1] in set(['JJ', 'JJR']) else False\n",
    "    nextLastVrb = True if c[0].get_attrib_tokens('lemmas')[-2] in helper_vbs else False\n",
    "    return -1 if not nextLastVrb and lastWordAdj else 0              \n",
    "    \n",
    "def lf29(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[0].get_attrib_tokens('pos_tags')[-1] == 'VBG' else 0\n",
    "\n",
    "def lf30(c):                #if ends in prep, its bad\n",
    "    return -1 if c[0].get_attrib_tokens('pos_tags')[-1] == 'IN' else 0\n",
    "\n",
    "def lf29b(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] == 'VBG' else 0\n",
    "\n",
    "def lf30b(c):                #if ends in prep, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] == 'IN' else 0\n",
    "\n",
    "def lf30c(c):                  #if pheno ends in VBG, its bad\n",
    "    return -1 if c[1].get_attrib_tokens('pos_tags')[-1] in ['JJ', 'JJR', 'JJS'] else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFs = [\n",
    "    lfdistBtw0,\n",
    "    lfdistBtwMax1,\n",
    "    lfdistBtwMax2,\n",
    "    lfdistBtwnOverlap,\n",
    "    lfdistBtwMin5,\n",
    "    lfdistBtwMin8,\n",
    "    lfdistBtwMin12,\n",
    "    lfdistBtwMin14,\n",
    "    lfLenCand,\n",
    "    lfend_prep,\n",
    "    lfend_det,\n",
    "    lfend_adj,\n",
    "    lfin_fig,\n",
    "    lfin_num,\n",
    "    lfin_equals,\n",
    "    lfin_comp_adj,\n",
    "    lfbtwn_is,\n",
    "    lfbtwm_comma,\n",
    "    lfbtwn_parenthesis,\n",
    "    LF_dna,\n",
    "    LF_rna,\n",
    "    LF_snp,\n",
    "    lfwordis_result,\n",
    "    lfwordsin_percent,\n",
    "    lfwordsin_phenotype,\n",
    "    lfwordsin_testerwords,\n",
    "    lfwordsin_compwords,\n",
    "    lfwordsin_negwords,\n",
    "    lfwordsin_causewords,\n",
    "    lfwordsin_adjwords,\n",
    "    lfwordsin_statswords,\n",
    "    lfnonoun,\n",
    "#     lfwordscontext_mutant,\n",
    "#     lfwordsbtwn_mutant,\n",
    "#     LF_variant,\n",
    "#     LF_express,\n",
    "#     lfLenCand,\n",
    "#     lfwordscontext_protein_desc,\n",
    "#     lfwordscontext_protein_ent,\n",
    "#     lfwordsin_protein,\n",
    "     lf2,\n",
    "     lf6,\n",
    "     LF_phenotype_dp,\n",
    "     lf_helpers,\n",
    "#     lf23,\n",
    "     lf32,\n",
    "     lf33,\n",
    "     lf35,\n",
    "# #     LF_LRB_Context,\n",
    "# #     LF_LRB_Contains,\n",
    "# #     LF_RRB,\n",
    "#     LF_JJR,\n",
    "#     LF_ModPhrase,\n",
    "#     LF_JJ,\n",
    "#     LF_IN,\n",
    "#     LF_NNP,\n",
    "# #    lf13,\n",
    "# #    LF_JJ_dp,\n",
    "     lf20,\n",
    "     lf24,\n",
    "#     lf25,\n",
    "     lf26\n",
    "# #    lf27,\n",
    "#     lf28,\n",
    "#     lf29,\n",
    "#     lf30\n",
    "# #     lf29b,\n",
    "# #     lf30b,\n",
    "# #     lf30c\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test how to query candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models.context import TemporaryContext\n",
    "import re\n",
    "\n",
    "print docs[15]\n",
    "sent = docs[15].get_parent()\n",
    "print sent\n",
    "text = sent._asdict()['text']\n",
    "splt = text.split()\n",
    "print splt\n",
    "print splt[4:5]\n",
    "print \"\\n\"\n",
    "print \"REGEX VERSION: \"\n",
    "\n",
    "resplit = re.split(' ',text)\n",
    "print resplit\n",
    "print resplit[4:5]\n",
    "print \"\\n\"\n",
    "\n",
    "print docs[15].get_contexts()\n",
    "print (docs[15][0]).get_attrib_tokens('words')\n",
    "print len((docs[15][0]).get_attrib_tokens('words'))\n",
    "# print (docs[15][0]).get_attrib_tokens('dep_parents')\n",
    "\n",
    "#print LF_DP(docs[0])\n",
    "print get_text_splits(docs[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LFs on the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFs = [\n",
    "    LF_mutant,\n",
    "    LF_variant,\n",
    "    LF_express,\n",
    "    LF_JJ,\n",
    "    LF_IN,\n",
    "    LF_dna,\n",
    "    LF_rna,\n",
    "    LF_snp,\n",
    "    LF_protein,\n",
    "    LF_LRB,\n",
    "    LF_RRB,\n",
    "    LF_NNP,\n",
    "    lfdistBtw0,\n",
    "    lfdistBtw,\n",
    "    lfdistBtwNeg,\n",
    "    lfdistBtwNeg2,\n",
    "    lf1,\n",
    "    lf2,\n",
    "    lf3,\n",
    "    lf4,\n",
    "    lf5,\n",
    "    lf6,\n",
    "    lf7,\n",
    "    lf8,\n",
    "    lf9,\n",
    "    lf10,\n",
    "    lf11,\n",
    "    lf2a,\n",
    "    lf12,\n",
    "    lf13,\n",
    "    lf14,\n",
    "    lf15,\n",
    "    lf16,\n",
    "    lf17,\n",
    "    lf18,\n",
    "    lf20,\n",
    "    lf21,\n",
    "    lf22,\n",
    "    lf23,\n",
    "    lf24,\n",
    "    lf25,\n",
    "    lf26,\n",
    "    lf27,\n",
    "    lf29,\n",
    "    lf30,\n",
    "#     lf29b,\n",
    "#     lf30b,\n",
    "#     lf30c,\n",
    "    lf31,\n",
    "    lf32,\n",
    "    lf33,\n",
    "    lf34,\n",
    "    lf35,\n",
    "    LF_phenotype_dp,\n",
    "    LF_mutation,\n",
    "    LF_pheno,\n",
    "    LF_dev_dp,\n",
    "    LF_protein_dp,\n",
    "    LF_network_dp,\n",
    "    LF_JJ_dp,\n",
    "    lf_helpers,\n",
    "    lf_adjwords,\n",
    "    lf_statswords,\n",
    "    lf_proteinIn,\n",
    "    lfnegWords,\n",
    "    resultsAlone,\n",
    "    lfLenCand,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @Jeff Start Gen Model Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "import multiprocessing\n",
    "labeler = LabelAnnotator(f=LFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time L_train = labeler.apply(split=3, parallelism=multiprocessing.cpu_count())\n",
    "L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF1(c):\n",
    "    return 1 if distance_btwn(c)<2 or overlap(c) else -1\n",
    "LFs = [LF1]\n",
    "from snorkel.annotations import LabelAnnotator\n",
    "import multiprocessing\n",
    "labeler = LabelAnnotator(f=LFs)\n",
    "%time L_train = labeler.apply(split=3, parallelism=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_dev = labeler.apply_existing(split=4, parallelism=multiprocessing.cpu_count())\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold_complex', split=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "tlabs = (L_dev+L_gold_dev)/2\n",
    "tlabs = csr_matrix.toarray(tlabs).reshape(tlabs.shape[0])\n",
    "flabs = (L_dev-L_gold_dev)/2\n",
    "flabs = csr_matrix.toarray(flabs).reshape(flabs.shape[0])\n",
    "print tlabs\n",
    "print flabs\n",
    "tp = tlabs[tlabs==1].shape[0]\n",
    "tn = tlabs[tlabs==-1].shape[0]\n",
    "fp = flabs[flabs==1].shape[0]\n",
    "fn = flabs[flabs==-1].shape[0]\n",
    "print tp, fp, tn, fn\n",
    "prec = float(tp) / (tp+fp)\n",
    "rec = float(tp) / (tp+fn)\n",
    "f1 = (2.0*prec*rec)/(prec+rec)\n",
    "print prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>Coverage</b> is the fraction of candidates that the labeling function emits a non-zero label for.\n",
    "* <b>Overlap</b> is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a non-zero label for.\n",
    "* <b>Conflict</b> is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a conflicting non-zero label for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning.structure import DependencySelector\n",
    "ds = DependencySelector()\n",
    "deps = ds.select(L_train, threshold=0.1)\n",
    "len(deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deps # (lf, lf, relationship_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel(lf_propensity=True)\n",
    "gen_model.train(\n",
    "    L_train, deps=deps, epochs=2, decay=0.975, step_size=0.1/L_train.shape[0],\n",
    "    init_acc=2.0, reg_param=0.0, burn_in = 10,\n",
    "    verbose=True\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the generative model to the training candidates to get the noise-aware training label set. We'll refer to these as the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.weights.lf_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import save_marginals\n",
    "save_marginals(session, L_train, train_marginals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "train_marginals = load_marginals(session, split=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model to Iterate on Labeling Functions\n",
    "Now that we have learned the generative model, we can stop here and use this to potentially debug and/or improve our labeling function set. First, we apply the LFs to our development set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L_test = labeler.apply_existing(split=5, parallelism=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold_complex', split=4)\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold_complex', split=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_gold_train = load_gold_labels(session, annotator_name='gold_complex', split=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority Vote Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed = np.clip(np.sum(L_dev, axis=1), -1, 1)\n",
    "eqs = (L_gold_dev == summed)\n",
    "eqs_1 = (eqs == summed)\n",
    "print eqs_1.shape\n",
    "# print (summed[summed == 0].shape)\n",
    "# print (summed[summed == 1].shape)\n",
    "print L_gold_dev[L_gold_dev == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L_gold_dev.shape)\n",
    "print(summed.shape)\n",
    "\n",
    "print eqs[eqs == True].shape\n",
    "from snorkel.learning.utils import MentionScorer\n",
    "dev_candidates = [L_dev.get_candidate(session, i) for i in xrange(L_dev.shape[0])]\n",
    "s = MentionScorer(dev_candidates, L_gold_dev)\n",
    "s.score(summed, b=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = gen_model.score(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = gen_model.score(session, L_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning.utils import MentionScorer\n",
    "from snorkel.learning import ListParameter, RangeParameter\n",
    "from snorkel.learning import GenerativeModel, scores_from_counts\n",
    "gen_model = GenerativeModel(lf_propensity=True)\n",
    "# Searching over learning rate\n",
    "epoch_param = RangeParameter('epochs', 2, 5, step=5)\n",
    "decay_param  = RangeParameter('decay', .85, .975, step=0.025)\n",
    "step_param  = RangeParameter('step_size', 0., .3, step=.05) #0.25/L_train.shape[0], 1.5/L_train.shape[0], step=.000005)\n",
    "burn_param  = RangeParameter('burn_in', 0, 12, step=2)\n",
    "#reg_param  = RangeParameter('reg_type', 2,2, step=1)\n",
    "print  step_param.get_all_values()\n",
    "#searcher = GenRandomSearch(session, gen_model, L_train, deps, [epoch_param, decay_param, burn_param], n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(432)\n",
    "params = []\n",
    "#for epoch in epoch_param.get_all_values():\n",
    "for decay in decay_param.get_all_values():\n",
    "    for burn in burn_param.get_all_values():\n",
    "        for step in step_param.get_all_values():\n",
    "             params.append((burn, decay, step/L_train.shape[0]))\n",
    "param_idxs = np.random.choice(np.arange(len(params)), min(20,len(params)), replace=False)\n",
    "grid_search = {}\n",
    "for p in param_idxs:\n",
    "    burn, decay, step = params[p]\n",
    "    print 'burn:', burn, 'decay:', decay, 'step_size', step*L_train.shape[0], step\n",
    "    gen_model.train(L_train, epochs=2, decay=decay, burn_in=burn, step_size=step,\\\n",
    "                    init_acc=2.0, reg_param=5.0, reg_type = 2)\n",
    "    tp, fp, tn, fn = gen_model.score(session, L_dev, L_gold_dev)\n",
    "    prec, rec, f1 = scores_from_counts(tp, fp, tn, fn)\n",
    "    grid_search[(burn, decay, step*L_train.shape[0])] = f1\n",
    "    \n",
    "grid_search = sorted(grid_search.items(), key=lambda x: x[1], reverse=True)\n",
    "for result in grid_search:\n",
    "    print result[0], 'f1:', result[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fpsv = SentenceNgramViewer(fp, session, height=400)\n",
    "else:\n",
    "    fpsv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fnsv = SentenceNgramViewer(fn, session, height=400)\n",
    "else:\n",
    "    fnsv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tpsv = SentenceNgramViewer(tp, session, height=400)\n",
    "else:\n",
    "    tpsv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tnsv = SentenceNgramViewer(tn, session, height=400)\n",
    "else:\n",
    "    tnsv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we should be getting an F1 score of around 0.6 to 0.7 on the development set, which is pretty good! However, we should be very careful in interpreting this. Since we developed our labeling functions using this development set as a guide, and our generative model is composed of these labeling functions, we expect it to score very well here!\n",
    "In fact, it is probably somewhat overfit to this set. However this is fine, since in the next tutorial, we'll train a more powerful end extraction model which will generalize beyond the development set, and which we will evaluate on a blind test set (i.e. one we never looked at during development)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Generative Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = gen_model.score(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Doing Some Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "At this point, we might want to look at some examples in one of the error buckets. For example, one of the false negatives that we did not correctly label as true mentions. To do this, we can again just use the Viewer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    sv = SentenceNgramViewer(fp, session, height=400)\n",
    "else:\n",
    "    sv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens,\n",
    "    get_between_tokens,\n",
    "    get_right_tokens,\n",
    "    contains_token,\n",
    "    get_doc_candidate_spans,\n",
    "    get_sent_candidate_spans,\n",
    "    get_text_between,\n",
    "    get_text_splits,\n",
    "    get_tagged_text,\n",
    "    is_inverted,\n",
    "    get_tagged_text,\n",
    ")\n",
    "c = sv.get_selected() if sv else list(fp.union(fn))[0]\n",
    "print(c)\n",
    "print(\"\\n\")\n",
    "\n",
    "c.labels\n",
    "fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically Creating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator\n",
    "import multiprocessing\n",
    "featurizer = FeatureAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time F_train = featurizer.apply(split=3, parallelism=multiprocessing.cpu_count())\n",
    "F_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we apply the feature set we just got from the training set to the dev and test sets by using apply_existing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "F_dev  = featurizer.apply_existing(split=4, parallelism=multiprocessing.cpu_count())\n",
    "F_test = featurizer.apply_existing(split=5, parallelism=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F_train = featurizer.load_matrix(session, split=3)\n",
    "F_dev   = featurizer.load_matrix(session, split=4)\n",
    "F_test  = featurizer.load_matrix(session, split=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Discriminative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the training marginals to train a discriminative model that classifies each Candidate as a true or false mention. We'll use a random hyperparameter search, evaluated on the development set labels, to find the best hyperparameters for our model. To run a hyperparameter search, we need labels for a development set. If they aren't already available, we can manually create labels using the Viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "disc_model = SparseLogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we set up and run the hyperparameter search, training our model with different hyperparamters and picking the best model configuration to keep. We'll set the random seed to maintain reproducibility.\n",
    "Note that we are fitting our model's parameters to the training set generated by our labeling functions, while we are picking hyperparamters with respect to score over the development set labels which we created by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning.utils import MentionScorer\n",
    "from snorkel.learning import RandomSearch, ListParameter, RangeParameter\n",
    "\n",
    "# Searching over learning rate\n",
    "rate_param = RangeParameter('lr', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l1_param  = RangeParameter('l1_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l2_param  = RangeParameter('l2_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "\n",
    "searcher = RandomSearch(session, disc_model, F_train, train_marginals, [rate_param, l1_param, l2_param], n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load in our dev set labels. We will pick the optimal result from the hyperparameter search by testing against these labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold_complex', split=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the hyperparameter search / train the end extraction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(432)\n",
    "params = []\n",
    "for lr in rate_param.get_all_values():\n",
    "    for l1_penalty in l1_param.get_all_values():\n",
    "        for l2_penalty in l2_param.get_all_values():\n",
    "             params.append((lr, l1_penalty, l2_penalty))\n",
    "param_idxs = np.random.choice(np.arange(len(params)), min(20,len(params)), replace=False)\n",
    "grid_search = {}\n",
    "for p in param_idxs:\n",
    "    lr, l1_penalty, l2_penalty = params[p]\n",
    "    print 'lr:', lr, 'l1_penalty:', l1_penalty, 'l2_penalty', l2_penalty\n",
    "    disc_model.train(F_train, train_marginals, n_epochs=50, lr=lr, l1_penalty=l1_penalty, l2_penalty=l2_penalty,\\\n",
    "                     print_freq=25, rebalance=0.25)\n",
    "    tp, fp, tn, fn = disc_model.score(session, F_dev, L_gold_dev)\n",
    "    prec, rec, f1 = scores_from_counts(tp, fp, tn, fn)\n",
    "    grid_search[(lr, l1_penalty,l2_penalty)] = f1\n",
    "    \n",
    "grid_search = sorted(grid_search.items(), key=lambda x: x[1], reverse=True)\n",
    "for result in grid_search:\n",
    "    print result[0], 'f1:', result[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1701)\n",
    "searcher.fit(F_dev, L_gold_dev, n_epochs=50, rebalance=0.5, print_freq=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that to train a model without tuning any hyperparameters (at your own risk) just use the train method of the discriminative model. For instance, to train with 20 epochs and a learning rate of 0.001, you could run:\n",
    "disc_model.train(F_train, train_marginals, n_epochs=20, lr=0.001)\n",
    "We can analyze the learned model by examining the weights. For example, we can print out the features with the highest weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_model.train(F_train, train_marginals, n_epochs=50, lr=0.0001, batch_size=100, \\\n",
    "                 l1_penalty=0.000001, l2_penalty=0.000001, print_freq=25,\\\n",
    "                 rebalance=0.25, seed=432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, _ = disc_model.get_weights()\n",
    "largest_idxs = reversed(np.argsort(np.abs(w))[-5:])\n",
    "for i in largest_idxs:\n",
    "    print 'Feature: {0: <70}Weight: {1:.6f}'.format(F_train.get_key(session, i).name, w[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this last section of the tutorial, we'll get the score we've been after: the performance of the extraction model on the blind test set (split 2). First, we load the test set labels and gold candidates we made in Part III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold_complex', split=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we score using the discriminative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_lr, fp_lr, tn_lr, fn_lr = disc_model.score(session, F_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_lr_test, fp_lr_test, tn_lr_test, fn_lr_test = disc_model.score(session, F_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that if this is the final test set that you will be reporting final numbers on, to avoid biasing results you should not inspect results. However you can run the model on your development set and, as we did in the previous part with the generative labeling function model, inspect examples to do error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_labels = (np.ravel(L_gold_dev.todense()) + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = session.query(PhenoPair).filter(PhenoPair.split == 3).all()\n",
    "dev = session.query(PhenoPair).filter(PhenoPair.split == 4).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.contrib.rnn import reRNN\n",
    "\n",
    "np.random.seed(432)\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':   0.01,\n",
    "#     'lr_end':     0.001,\n",
    "    'dim':        50,\n",
    "    'n_epochs':   50,\n",
    "    'dropout':    0.5,\n",
    "    'rebalance':  0.25,\n",
    "    'print_freq': 1\n",
    "}\n",
    "\n",
    "lstm = reRNN(seed=1701, n_threads=None)\n",
    "lstm.train(train, train_marginals, dev_candidates=dev, dev_labels=dev_labels, **train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Viewers for error analysis for dependency models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dep-gen model - f1 .584 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fpsv_gen = SentenceNgramViewer(fp, session, height=400)\n",
    "else:\n",
    "    fpsv_gen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpsv_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fnsv_gen = SentenceNgramViewer(fn, session, height=400)\n",
    "else:\n",
    "    fnsv_gen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnsv_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tpsv_gen = SentenceNgramViewer(tp, session, height=400)\n",
    "else:\n",
    "    tpsv_gen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpsv_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tnsv_gen = SentenceNgramViewer(tn, session, height=400)\n",
    "else:\n",
    "    tnsv_gen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnsv_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dep-lr dev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fpsv_lr = SentenceNgramViewer(fp_lr, session, height=400)\n",
    "else:\n",
    "    fpsv_lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpsv_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fnsv_lr = SentenceNgramViewer(fn_lr, session, height=400)\n",
    "else:\n",
    "    fnsv_lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnsv_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tpsv_lr = SentenceNgramViewer(tp_lr, session, height=400)\n",
    "else:\n",
    "    tpsv_lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpsv_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tnsv_lr = SentenceNgramViewer(tn_lr, session, height=400)\n",
    "else:\n",
    "    tnsv_lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnsv_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dep lr test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fpsv_lr_test = SentenceNgramViewer(fp_lr_test, session, height=400)\n",
    "else:\n",
    "    fpsv_lr_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpsv_lr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    fnsv_lr_test = SentenceNgramViewer(fn_lr_test, session, height=400)\n",
    "else:\n",
    "    fnsv_lr_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnsv_lr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tpsv_lr_test = SentenceNgramViewer(tp_lr_test, session, height=400)\n",
    "else:\n",
    "    tpsv_lr_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpsv_lr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    tnsv_lr_test = SentenceNgramViewer(tn_lr_test, session, height=400)\n",
    "else:\n",
    "    tnsv_lr_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnsv_lr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
